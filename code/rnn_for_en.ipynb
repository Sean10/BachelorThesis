{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = {line.strip() for line in open(filepath, \"r\", encoding='utf-8').readlines()}\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def cleanSentences(string):\n",
    "    return re.sub(\"[0-9]\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！<>《》，。？、~@#￥%……&*（）]+\", \"\",string)\n",
    "\n",
    "\n",
    "program = os.path.basename(\"deep learning\")\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "# positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "# negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "\n",
    "# files = positiveFiles + negativeFiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [\"corpus.txt\"]\n",
    "\n",
    "# vec = []\n",
    "\n",
    "\n",
    "# # logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "# i = 0\n",
    "# for file in files:\n",
    "#     with open(file, \"r\",encoding='utf-8') as f:\n",
    "#         if os.path.exists(\"corpus_segment.txt\"):\n",
    "#             continue\n",
    "        \n",
    "#         fp = codecs.open(\"corpus_segment.txt\", \"a+\", encoding=\"utf-8\")\n",
    "        \n",
    "#         for line in f:\n",
    "#             line = cleanSentences(line)\n",
    "#             if line.strip() is \"\n",
    "#                 continue\n",
    "            \n",
    "#             sentence_seged = jieba.cut(line.strip())\n",
    "# #             print(type(sentence_seged))\n",
    "# #             tmp = []\n",
    "# #             for word in sentence_seged:\n",
    "# #                 if word.strip() not in stoplist:\n",
    "# #                     tmp.append(word)\n",
    "# #             vec.append(tmp)\n",
    "# #             tmp.append(word)\n",
    "#             fp.write(\" \".join(sentence_seged))\n",
    "            \n",
    "#             i += 1\n",
    "#             if i % 1000 == 0:\n",
    "#                 logger.info(\"Saved \"+ str(i) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(i) + \" articles\")\n",
    "# # print(vec)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-09 19:49:05,936: INFO: 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "似乎不需要去除停用词\n",
    "'''\n",
    "# stoplist = stopwordslist(\"stopwords.txt\")\n",
    "# fout = \"corpus_seg_without_stop.txt\"\n",
    "# cnt = 0\n",
    "# with codecs.open(\"corpus_segment.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         tmp = []\n",
    "#         for word in line.split():\n",
    "#             if word.strip() in stoplist:\n",
    "#                continue\n",
    "#             tmp.append(word)\n",
    "#         fout.write(\" \".join(tmp))\n",
    "#         cnt += 1\n",
    "#         if cnt % 1000 == 0:\n",
    "#             logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# input = \"corpus_segment.txt\"\n",
    "# model = Word2Vec(LineSentence(input), size=400, window=5,min_count=5,workers=multiprocessing.cpu_count())\n",
    "# model.save('model_word2vec')\n",
    "# model.wv.save_word2vec_format(\"model_saved_format_version\", binary=False)  \n",
    "\n",
    "\n",
    "# model = Word2Vec.load(\"model_word2vec\")\n",
    "# for mo in model.wv.vocab:\n",
    "#     print(mo, model.wv[mo])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# w2v_list = np.array([model.wv[word] for word in model.wv.vocab])\n",
    "# word_list = np.array([word for word in model.wv.vocab]).tolist()\n",
    "# print(w2v_np)\n",
    "\n",
    "w2v_list = np.load(\"training_data/wordVectors.npy\")\n",
    "word_list = np.load(\"training_data/wordsList.npy\").tolist()\n",
    "word_list = [word.decode('UTF-8') for word in word_list]\n",
    "# print(model[u'n'])\n",
    "# print(model.wv[u'机会'])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n",
      "of\n"
     ]
    }
   ],
   "source": [
    "print(w2v_list.shape)\n",
    "print(word_list[3])\n",
    "# print(word_list.shape)\n",
    "# print(model.wv.most_similar('爱情'))\n",
    "# print(model.wv.similarity('爱情','宾馆'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'them', 'what', 'him', 'united', 'during', 'before', 'may', 'since', 'many', 'while', 'where', 'states', 'because', 'now', 'city', 'made', 'like', 'between', 'did', 'just', 'national', 'day', 'country', 'under', 'such', 'second', 'then', 'company', 'group', 'any', 'through', 'china', 'four', 'being', 'down', 'war', 'back', 'off', 'south', 'american', 'minister', 'police', 'well', 'including', 'team', 'international', 'week', 'officials', 'still', 'both', 'even', 'high', 'part', 'told', 'those', 'end', 'former', 'these', 'make', 'billion', 'work', 'our', 'home', 'school', 'party', 'house', 'old', 'later', 'get', 'another', 'tuesday', 'news', 'long', 'five', 'called', '1', 'wednesday', 'military', 'way', 'used', 'much', 'next', 'monday', 'thursday', 'friday', 'game', 'here', '?', 'should', 'take', 'very', 'my', 'north', 'security', 'season', 'york', 'how', 'public', 'early', 'according', 'several', 'court', 'say', 'around', 'foreign', '10', 'until', 'set', 'political', 'says', 'market', 'however', 'family', 'life', 'same', 'general', '–', 'left', 'good', 'top', 'university', 'going', 'number', 'major', 'known', 'points', 'won', 'six', 'month', 'dollars', 'bank', '2', 'iraq', 'use', 'members', 'each', 'area', 'found', 'official', 'sunday', 'place', 'go', 'based', 'among', 'third', 'times', 'took', 'right', 'days', 'local', 'economic', 'countries', 'see', 'best', 'report', 'killed', 'held', 'business', 'west', 'does', 'own', '%', 'came', 'law', 'months', 'women', \"'re\", 'power', 'think', 'service', 'children', 'bush', 'show', '/', 'help', 'chief', 'saturday', 'system', 'john', 'support', 'series', 'play', 'office', 'following', 'me', 'meeting', 'expected', 'late', 'washington', 'games', 'european', 'league', 'reported', 'final', 'added', 'without', 'british', 'white', 'history', 'man', 'men', 'became', 'want', 'march', 'case', 'few', 'run', 'money', 'began', 'open', 'name', 'trade', 'center', '3', 'israel', 'oil', 'too', 'al', 'film', 'win', 'led', 'east', 'central', '20', 'air', 'come', 'chinese', 'town', 'leader', 'army', 'line', 'never', 'little', 'played', 'prime', 'death', 'companies', 'least', 'put', 'forces', 'past', 'de', 'half', 'june', 'saying', 'know', 'federal', 'french', 'peace', 'earlier', 'capital', 'force', 'great', 'union', 'near', 'released', 'small', 'department', 'every', 'health', 'japan', 'head', 'ago', 'night', 'big', 'cup', 'election', 'region', 'director', 'talks', 'program', 'far', 'today', 'statement', 'july', 'although', 'district', 'again', 'born', 'development', 'leaders', 'council', 'close', 'record', 'along', 'county', 'france', 'went', 'point', 'must', 'spokesman', 'your', 'member', 'plan', 'financial', 'april', 'recent', 'campaign', 'become', 'troops', 'whether', 'lost', 'music', '15', 'got', 'israeli', '30', 'need', '4', 'lead', 'already', 'russia', 'though', 'might', 'free', 'hit', 'rights', '11', 'information', 'away', '12', '5', 'others', 'control', 'within', 'large', 'economy', 'press', 'agency', 'water', 'died', 'career', 'making', '...', 'deal', 'attack', 'side', 'seven', 'better', 'less', 'september', 'once', 'clinton', 'main', 'due', 'committee', 'building', 'conference', 'club', 'january', 'decision', 'stock', 'america', 'given', 'give', 'often', 'announced', 'television', 'industry', 'order', 'young', \"'ve\", 'palestinian', 'age', 'start', 'administration', 'russian', 'prices', 'round', 'december', 'nations', \"'m\", 'human', 'india', 'defense', 'asked', 'total', 'october', 'players', 'bill', 'important', 'southern', 'move', 'fire', 'population', 'rose', 'november', 'include', 'further', 'nuclear', 'street', 'taken', 'media', 'different', 'issue', 'received', 'secretary', 'return', 'college', 'working', 'community', 'eight', 'groups', 'despite', 'level', 'largest', 'whose', 'attacks', 'germany', 'august', 'change', 'church', 'nation', 'german', 'station', 'london', 'weeks', 'having', '18', 'research', 'black', 'services', 'story', '6', 'europe', 'sales', 'policy', 'visit', 'northern', 'lot', 'across', 'per', 'current', 'board', 'football', 'ministry', 'workers', 'vote', 'book', 'fell', 'seen', 'role', 'students', 'shares', 'iran', 'process', 'agreement', 'quarter', 'full', 'match', 'started', 'growth', 'yet', 'moved', 'possible', 'western', 'special', '100', 'plans', 'interest', 'behind', 'strong', 'england', 'named', 'food', 'period', 'real', 'authorities', 'car', 'term', 'rate', 'race', 'nearly', 'korea', 'enough', 'site', 'opposition', 'keep', '25', 'call', 'future', 'taking', 'island', '2008', '2006', 'road', 'outside', 'really', 'century', 'democratic', 'almost', 'single', 'share', 'leading', 'trying', 'find', 'album', 'senior', 'minutes', 'together', 'congress', 'index', 'australia', 'results', 'hard', 'hours', 'land', 'action', 'higher', 'field', 'cut', 'coach', 'elections', 'san', 'issues', 'executive', 'february', 'production', 'areas', 'river', 'face', 'using', 'japanese', 'province', 'park', 'price', 'commission', 'california', 'father', 'son', 'education', '7', 'village', 'energy', 'shot', 'short', 'africa', 'key', 'red', 'association', 'average', 'pay', 'exchange', 'eu', 'something', 'gave', 'likely', 'player', 'george', '2007', 'victory', '8', 'low', 'things', '2010', 'pakistan', '14', 'post', 'social', 'continue', 'ever', 'look', 'chairman', 'job', '2000', 'soldiers', 'able', 'parliament', 'front', 'himself', 'problems', 'private', 'lower', 'list', 'built', '13', 'efforts', 'dollar', 'miles', 'included', 'radio', 'live', 'form', 'david', 'african', 'increase', 'reports', 'sent', 'fourth', 'always', 'king', '50', 'tax', 'taiwan', 'britain', '16', 'playing', 'title', 'middle', 'meet', 'global', 'wife', '2009', 'position', 'located', 'clear', 'ahead', '2004', '2005', 'iraqi', 'english', 'result', 'release', 'violence', 'goal', 'project', 'closed', 'border', 'body', 'soon', 'crisis', 'division', '&amp;', 'served', 'tour', 'hospital', 'kong', 'test', 'hong', 'u.n.', 'inc.', 'technology', 'believe', 'organization', 'published', 'weapons', 'agreed', 'why', 'nine', 'summer', 'wanted', 'republican', 'act', 'recently', 'texas', 'course', 'problem', 'senate', 'medical', 'un', 'done', 'reached', 'star', 'continued', 'investors', 'living', 'care', 'signed', '17', 'art', 'provide', 'worked', 'presidential', 'gold', 'obama', 'morning', 'dead', 'opened', \"'ll\", 'event', 'previous', 'cost', 'instead', 'canada', 'band', 'teams', 'daily', '2001', 'available', 'drug', 'coming', '2003', 'investment', '’s', 'michael', 'civil', 'woman', 'training', 'appeared', '9', 'involved', 'indian', 'similar', 'situation', '24', 'los', 'running', 'fighting', 'mark']\n"
     ]
    }
   ],
   "source": [
    "# del model\n",
    "print(word_list[100:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwordslist(\"stopwords.en.txt\")\n",
    "\n",
    "# positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "# negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "\n",
    "# seg_positiveFiles = ['seged_positiveReviews/' + f for f in listdir('seged_positiveReviews/') if isfile(join('seged_positiveReviews/', f))]\n",
    "# seg_negativeFiles = ['seged_negativeReviews/' + f for f in listdir('seged_negativeReviews/') if isfile(join('seged_negativeReviews/', f))]\n",
    "\n",
    "# positiveFiles = ['training_data/positiveReviews/' + f for f in listdir('training_data/positiveReviews/' if isfile(join('training_data/positiveReviews/', f)))]\n",
    "# negativeFiles = ['training_data/negativeReviews/' + f for f in listdir('training_data/negativeReviews/' if isfile(join('training_data/negativeReviews/', f)))]\n",
    "# seg_positiveFiles = ['seged_data_new/B/Pos-train.txt','seged_data_new/B/Pos-test.txt']\n",
    "# seg_negativeFiles = ['seged_data_new/B/Neg-train.txt','seged_data_new/B/Neg-test.txt']\n",
    "\n",
    "# pos_test_files = []\n",
    "# neg_test_files = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "# if os.path.exists(\"seged_data_new\"):\n",
    "#     if os.path.exists(\"B\"):\n",
    "#         pass\n",
    "#     else:\n",
    "#         os.mkdir(\"B\")\n",
    "# else:\n",
    "#     os.mkdir(\"seged_data_new\")\n",
    "    \n",
    "# for pf in positiveFiles:\n",
    "# #     positive_seged = \"seged_\"+pf\n",
    "    \n",
    "# #     if os.path.exists(positive_seged):\n",
    "# #         continue\n",
    "        \n",
    "#     with open(pf, \"r\") as f:\n",
    "# #         file = []\n",
    "#         for line in f:\n",
    "#             line = cleanSentences(line).strip()\n",
    "# #             sentence_seged = jieba.cut(line)\n",
    "            \n",
    "#             temp = []\n",
    "#             for word in line:\n",
    "#                 if word in stoplist:\n",
    "#                     continue;\n",
    "#                 temp.append(word)\n",
    "# #             file.append(\" \".join(temp))\n",
    "#             cnt += 1\n",
    "#             if cnt % 1000 == 0:\n",
    "#                 logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "# #             with open(positive_seged, \"w\",encoding='utf-8') as of:\n",
    "# #                 of.write(\" \".join(temp))\n",
    "# #                 os.write(\"\\n\")\n",
    "            \n",
    "\n",
    "# for pf in negativeFiles:\n",
    "# #     negative_seged = \"seged_\"+pf\n",
    "    \n",
    "# #     if os.path.exists(negative_seged):\n",
    "# #         continue\n",
    "    \n",
    "#     with open(pf, \"r\") as f:\n",
    "# #         file = []\n",
    "#         for line in f:\n",
    "#             line = cleanSentences(line).strip()\n",
    "# #             sentence_seged = jieba.cut(line)\n",
    "            \n",
    "#             temp = []\n",
    "#             for word in line:\n",
    "#                 if word in stoplist:\n",
    "#                     continue;\n",
    "#                 temp.append(word)\n",
    "# #             file.append(\" \".join(temp))\n",
    "#             cnt += 1\n",
    "#             if cnt % 1000 == 0:\n",
    "#                 logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "# #             with open(negative_seged, \"w\",encoding='utf-8') as of:\n",
    "# #                 of.write(\" \".join(temp))\n",
    "# #                 os.write(\"\\n\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(cnt) + \" articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "# numWords = []\n",
    "# for pf in seg_positiveFiles:\n",
    "#     with open(pf, \"r\", encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "# #         line=f.readline()\n",
    "#             counter = len(\"\".join(line.split()))\n",
    "# #         print(line.split(), len(line.split()))\n",
    "#             numWords.append(counter)       \n",
    "# print('Positive files finished')\n",
    "\n",
    "# for nf in seg_negativeFiles:\n",
    "#     with open(nf, \"r\", encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#         #         line=f.readline()\n",
    "#             counter = len(line.split())\n",
    "#             numWords.append(counter)  \n",
    "# print('Negative files finished')\n",
    "\n",
    "# numFiles = len(numWords)\n",
    "# print('The total number of files is', numFiles)\n",
    "# print('The total number of words in the files is', sum(numWords))\n",
    "# print('The average number of words in the files is', sum(numWords)/len(numWords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.hist(numWords, 50)\n",
    "# plt.xlabel('Sequence Length')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.axis([0, 200, 0, 10000])\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "maxSeqLength = 300\n",
    "numDimensions = 400\n",
    "# firstSentense = np.zeros((maxSeqLength), dtype='int32')\n",
    "# firstSentense[0] = word_list.index('宾馆')\n",
    "# firstSentense[1] = word_list.index('喜欢')\n",
    "# firstSentense[2] = word_list.index('爱')\n",
    "# print(firstSentense.shape)\n",
    "# print(firstSentense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "# fileCounter = 0\n",
    "# steps = []\n",
    "\n",
    "# for pf in seg_positiveFiles:\n",
    "#     with open(pf, \"r\") as f:\n",
    "#         for line in f:\n",
    "#         #         line=f.readline()\n",
    "# #         print(line.split())\n",
    "#             indexCounter = 0\n",
    "            \n",
    "#             for word in line.split():\n",
    "#                 try:\n",
    "# #                 print(word, word_list.index(word))\n",
    "#                     ids[fileCounter][indexCounter] = word_list.index(word)\n",
    "#                 except ValueError:\n",
    "# #                 print(word)\n",
    "#                     ids[fileCounter][indexCounter] = 572296 #Vector for unkown words\n",
    "#                 indexCounter = indexCounter + 1\n",
    "#                 if indexCounter >= maxSeqLength:\n",
    "#                     break\n",
    "#             steps.append(indexCounter)\n",
    "#             fileCounter = fileCounter + 1\n",
    "\n",
    "# for nf in seg_negativeFiles:\n",
    "#     with open(nf, \"r\") as f:\n",
    "        \n",
    "#         for line in f:\n",
    "#         #         line=f.readline()\n",
    "#             indexCounter = 0\n",
    "#             for word in line.split():\n",
    "#                 try:\n",
    "# #                 print(word, word_list.index(word))\n",
    "#                     ids[fileCounter][indexCounter] = word_list.index(word)\n",
    "#                 except ValueError:\n",
    "#                     ids[fileCounter][indexCounter] = 572296 #Vector for unkown words\n",
    "#                 indexCounter = indexCounter + 1\n",
    "#                 if indexCounter >= maxSeqLength:\n",
    "#                     break\n",
    "#             steps.append(indexCounter)\n",
    "#             fileCounter = fileCounter + 1 \n",
    "\n",
    "# #Pass into embedding function and see if it evaluates. \n",
    "\n",
    "# np.save('idsMatrix', ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ids[6])\n",
    "from random import randint, shuffle\n",
    "\n",
    "\n",
    "def getWords(filePath):\n",
    "    files = [filePath + f for f in listdir(filePath) if isfile(join(filePath, f))]\n",
    "#     print(files)\n",
    "    lineList = []\n",
    "    for nf in files:\n",
    "        \n",
    "        with open(nf, \"r\") as f:\n",
    "            indexCounter = 0\n",
    "#             print(f)\n",
    "            line=f.readline()\n",
    "#             print(line)\n",
    "            wordList = []\n",
    "            line = re.sub('[^a-zA-Z]',' ',line)\n",
    "#             line = cleanSentences(line)\n",
    "            for word in line.lower().split():\n",
    "#                 if word in stoplist:\n",
    "#                     continue\n",
    "                wordList.append(word)\n",
    "            \n",
    "#         print(wordList)\n",
    "        lineList.append(wordList)\n",
    "    return lineList\n",
    "#     content = []\n",
    "#     cnt = 0\n",
    "#     with codecs.open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             line = cleanSentences(line).strip()\n",
    "# #             sentence_seged = jieba.cut(line)\n",
    "            \n",
    "#             temp = []\n",
    "#             for word in line:\n",
    "#                 if word in stoplist:\n",
    "#                     continue;\n",
    "#                 temp.append(word)\n",
    "#             content.append(\" \".join(temp))\n",
    "        \n",
    "#             cnt += 1\n",
    "#             if cnt % 1000 == 0:\n",
    "#                 logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "#     logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "# #     if os.path.exists(\"seged_\"+file):\n",
    "# #         return content\n",
    "    \n",
    "# #     with open(\"seged_\"+file, \"w\",encoding='utf-8') as of:\n",
    "# #         print(\"seged_\"+file)\n",
    "# #         of.write(\"\\n\".join(content))\n",
    "#     return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2Array(lineList):\n",
    "    numFiles = len(lineList)\n",
    "    linesArray=np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "    \n",
    "    steps = []\n",
    "    cnt = 0\n",
    "    for line in lineList:\n",
    "        p = 0\n",
    "#         print(line)\n",
    "        for word in line:\n",
    "#                 wordsArray.append(model.wv.word_vec(line[i]))\n",
    "            try:\n",
    "                linesArray[cnt][p] = word_list.index(word)\n",
    "            except ValueError:\n",
    "                linesArray[cnt][p] = 399999\n",
    "            p = p + 1\n",
    "            if p >= maxSeqLength:\n",
    "                break\n",
    "        steps.append(p)\n",
    "#         linesArray.append(wordsArray)\n",
    "        cnt += 1\n",
    "    #             cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            logger.info(\"process \"+ str(cnt) + \" articles\")\n",
    "#     linesArray = np.array(linesArray)\n",
    "#     steps = np.array(steps)\n",
    "    print(\"words2Array succeed\")\n",
    "    return linesArray, steps\n",
    "\n",
    "def convert2Data(posArray, negArray, posStep, negStep):\n",
    "    randIt = []\n",
    "    data = []\n",
    "    steps = []\n",
    "    labels = []\n",
    "    for i in range(len(posArray)):\n",
    "        randIt.append([posArray[i], posStep[i], [1,0]])\n",
    "    for i in range(len(negArray)):\n",
    "        randIt.append([negArray[i], negStep[i], [0,1]])\n",
    "    shuffle(randIt)\n",
    "    for i in range(len(randIt)):\n",
    "        data.append(randIt[i][0])\n",
    "        steps.append(randIt[i][1])\n",
    "        labels.append(randIt[i][2])\n",
    "#     print(len(data))\n",
    "#     data = np.array(data)\n",
    "#     steps = np.array(steps)\n",
    "    return data, steps, labels\n",
    "\n",
    "def makeData(posPath,negPath):\n",
    "    #获取词汇，返回类型为[[word1,word2...],[word1,word2...],...]\n",
    "    pos = getWords(posPath)\n",
    "    print(\"The positive data's length is :\",len(pos))\n",
    "    neg = getWords(negPath)\n",
    "    print(\"The negative data's length is :\",len(neg))\n",
    "    #将评价数据转换为矩阵，返回类型为array\n",
    "    posArray, posSteps = words2Array(pos)\n",
    "    negArray, negSteps = words2Array(neg)\n",
    "    #将积极数据和消极数据混合在一起打乱，制作数据集\n",
    "#     Data, Steps, Labels = convert2Data(posArray, negArray, posSteps, negSteps)\n",
    "    return posArray, negArray, posSteps, negSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeed to load numpy data\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "if os.path.exists(\"training_data/data_saved\"):\n",
    "    trainData = np.load(\"training_data/data_saved/trainData.npy\")\n",
    "    trainSteps = np.load(\"training_data/data_saved/trainSteps.npy\")\n",
    "    trainLabels = np.load(\"training_data/data_saved/trainLabels.npy\")\n",
    "    testData = np.load(\"training_data/data_saved/testData.npy\")\n",
    "    testSteps = np.load(\"training_data/data_saved/testSteps.npy\")\n",
    "    testLabels = np.load(\"training_data/data_saved/testLabels.npy\")\n",
    "    print(\"Succeed to load numpy data\")\n",
    "    \n",
    "    \n",
    "#     print(tempData.shape)\n",
    "\n",
    "   \n",
    "    \n",
    "else: \n",
    "\n",
    "    train_posArray, train_negArray, train_posSteps, train_negSteps = makeData('training_data/positiveReviews/', 'training_data/negativeReviews/')\n",
    "\n",
    "#     print(\"In test data:\")\n",
    "#     test_posArray, test_negArray, test_posSteps, test_negSteps = makeData('data_new/B/Pos-test.txt', 'data_new/B/Neg-test.txt')\n",
    "\n",
    "# del model\n",
    "#     tempData_pos = np.concatenate((train_posArray, test_posArray), axis=0)\n",
    "    train_posLabels = np.array([[1,0] for i in range(len(train_posArray))])\n",
    "    \n",
    "    train_posArray, test_posArray, train_posLabels, test_posLabels, train_posSteps, test_posSteps = train_test_split(train_posArray, train_posLabels, train_posSteps, test_size=0.1)\n",
    "\n",
    "    train_negLabels = np.array([[0,1] for i in range(len(train_negArray))])\n",
    "    \n",
    "    train_negArray, test_negArray, train_negLabels, test_negLabels, train_negSteps, test_negSteps = train_test_split(train_negArray, train_negLabels, train_negSteps, test_size=0.1)\n",
    "    \n",
    "    trainData = np.concatenate((train_posArray, train_negArray),axis=0)\n",
    "    trainLabels = np.concatenate((train_posLabels, train_negLabels), axis=0)\n",
    "    testData = np.concatenate((test_posArray, test_negArray),axis=0)\n",
    "    testLabels = np.concatenate((test_posLabels, test_negLabels),axis=0)\n",
    "    trainSteps = np.concatenate((train_posSteps, train_negSteps), axis=0)\n",
    "    testSteps = np.concatenate((test_posSteps, test_negSteps), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    trainSteps = np.ndarray.flatten(trainSteps)\n",
    "    trainSteps = trainSteps[:, np.newaxis]\n",
    "    train_temp = np.hstack((trainData, trainLabels, trainSteps))\n",
    "    \n",
    "# print(train_temp[1])\n",
    "    np.random.shuffle(train_temp)\n",
    "\n",
    "# trainData = train_temp[:,:-1]\n",
    "# trainLabels = train_temp[:,:,-1]\n",
    "# trainSteps = \n",
    "    trainData, trainLabels, trainSteps = train_temp[:,0:50], train_temp[:,50:52], train_temp[:,52:]\n",
    "    trainSteps = np.ndarray.flatten(trainSteps)\n",
    "    \n",
    "#     trainData, testData, trainLabels, testLabels = train_test_split(tempData, tempLabel, test_size=0.1, random_state=0)\n",
    "\n",
    "#     trainData, trainSteps, trainLabels = convert2Data(train_posArray, train_negArray, train_posSteps, train_negSteps)\n",
    "\n",
    "#     testData, testSteps, testLabels = convert2Data(test_posArray, test_negArray, test_posSteps, test_negSteps)\n",
    "# print(\"In train data:\")\n",
    "# trainData, trainSteps, trainLabels = makeData('data_new/B/Pos-train.txt',\n",
    "#                                               'data_new/B/Neg-train.txt')\n",
    "# print(\"In test data:\")\n",
    "# testData, testSteps, testLabels = makeData('data_new/B/Pos-test.txt',\n",
    "#                                            'data_new/B/Neg-test.txt')\n",
    "# trainLabels = np.array(trainLabels)\n",
    "\n",
    "\n",
    "# del model\n",
    "\n",
    "# print(\"-\"*30)\n",
    "# print(\"The trainData's shape is:\",trainData.shape)\n",
    "# print(\"The testData's shape is:\",testData.shape)\n",
    "# print(\"The trainSteps's shape is:\",trainSteps.shape)\n",
    "# print(\"The testSteps's shape is:\",testSteps.shape)\n",
    "# print(\"The trainLabels's shape is:\",trainLabels.shape)\n",
    "# print(\"The testLabels's shape is:\",np.array(testLabels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500,)\n",
      "(22500, 300)\n",
      "(22500, 2)\n",
      "(2,)\n",
      "()\n",
      "(22500, 300)\n",
      "(22500, 2)\n",
      "(22500,)\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# print(type(train_posArray))\n",
    "# print(train_posArray.shape)\n",
    "# print(type(train_negArray))\n",
    "print(trainSteps.shape)\n",
    "print(trainData.shape)\n",
    "print(trainLabels.shape)\n",
    "print(trainLabels[1].shape)\n",
    "print(trainSteps[2].shape)\n",
    "    \n",
    "    \n",
    "\n",
    "print(trainData.shape)\n",
    "print(trainLabels.shape)\n",
    "print(trainSteps.shape)\n",
    "print(trainLabels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 54025   4234 399999   3814     14    154      6    997     59      7\n",
      "   2615  24857    108     18   1534    561   4388      5     86   2159\n",
      "     88     20    670     18     31      4    891     20      4     26\n",
      "   1749  40851   8964  16794    720     63     32    174   2284   5753\n",
      " 201534   1340    174   2284     17    191    333   2955   1928   1955\n",
      "   3532      5   1984  19183     32 201534   3198      3   3143      4\n",
      " 130902     20   1534      7   1340   2841     12   1534    456    191\n",
      "   5466     81     86   2159    275     34   9674     61   1174   1432\n",
      "   1797     17    158   2284     63     32     77    353   3826    697\n",
      "    158   2153   1074   5918  14800   1381   8283  26136    289     14\n",
      "   6989   3265  16857     14  22782    785  13378     14  34996   3763\n",
      " 269483     14  13741  26321    354   1381     26    835    560   7084\n",
      "   5964  30607   1381  22782   1534    835    560  55783    649  18330\n",
      "     52   1938   1381  26156 201534  29570   1856      6      7  10568\n",
      "     37     14      7   1786  10014  15175     41   2032   7546     20\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "trainData = np.array(trainData)\n",
    "np.save(\"training_data/data_saved/trainData.npy\", trainData)\n",
    "# del trainData\n",
    "\n",
    "trainLabels = np.array(trainLabels)\n",
    "np.save(\"training_data/data_saved/trainLabels.npy\", trainLabels)\n",
    "testData = np.array(testData)\n",
    "np.save(\"training_data/data_saved/testData.npy\", testData)\n",
    "\n",
    "testLabels = np.array(testLabels)\n",
    "np.save(\"training_data/data_saved/testLabels.npy\", testLabels)\n",
    "print(testData[0])\n",
    "\n",
    "\n",
    "trainSteps = np.array(trainSteps)\n",
    "np.save(\"training_data/data_saved/trainSteps.npy\", trainSteps)\n",
    "testSteps = np.array(testSteps)\n",
    "np.save(\"training_data/data_saved/testSteps.npy\", testSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 300)\n",
      "(22500, 300)\n",
      "(22500, 2)\n",
      "(2500, 2)\n",
      "(22500,)\n",
      "[[    19 201534    319 ...  30410  30410 134744]\n",
      " [  1472 201534     96 ...      6 201534   1475]\n",
      " [     7   3830     59 ...     83    181      3]\n",
      " ...\n",
      " [     7   1153      4 ...    430   2865   5186]\n",
      " [    41    120   1026 ...      0      0      0]\n",
      " [    41   1993     36 ...      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "print(testData.shape)\n",
    "print(trainData.shape)\n",
    "print(trainLabels.shape)\n",
    "print(testLabels.shape)\n",
    "print(trainSteps.shape)\n",
    "print(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    \"\"\"\n",
    "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
    "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
    "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
    "    Variables notation is also inherited from the article\n",
    "    \n",
    "    Args:\n",
    "        inputs: The Attention inputs.\n",
    "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
    "                In case of RNN, this must be RNN outputs `Tensor`:\n",
    "                    If time_major == False (default), this must be a tensor of shape:\n",
    "                        `[batch_size, max_time, cell.output_size]`.\n",
    "                    If time_major == True, this must be a tensor of shape:\n",
    "                        `[max_time, batch_size, cell.output_size]`.\n",
    "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
    "                the backward RNN outputs `Tensor`.\n",
    "                    If time_major == False (default),\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
    "                    If time_major == True,\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
    "        attention_size: Linear size of the Attention weights.\n",
    "        time_major: The shape format of the `inputs` Tensors.\n",
    "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "            Using `time_major = True` is a bit more efficient because it avoids\n",
    "            transposes at the beginning and end of the RNN calculation.  However,\n",
    "            most TensorFlow data is batch-major, so by default this function\n",
    "            accepts input and emits output in batch-major form.\n",
    "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
    "            Used for visualization purpose.\n",
    "    Returns:\n",
    "        The Attention output `Tensor`.\n",
    "        In case of RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell.output_size]`.\n",
    "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "testSize = 2500\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 10001\n",
    "# try 10000 iteration\n",
    "\n",
    "# Bi-LSTM\n",
    "# import tensorflow as tf\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "\n",
    "#     labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "#     input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "#     steps = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "#     data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "# #     input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "# #     lstmCell = tf.contrib.rnn.BasicRNNCell(lstmUnits, activation=tf.nn.relu)\n",
    "\n",
    "#     lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "#     lstmCell_bk = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "#     lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "\n",
    "#     weight = tf.Variable(tf.truncated_normal([lstmUnits*2, numClasses]))\n",
    "#     bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    \n",
    "#     def model(put, steps):\n",
    "#         value, _ = tf.nn.bidirectional_dynamic_rnn(lstmCell, lstmCell_bk, put  ,dtype=tf.float32)\n",
    "        \n",
    "#         value = tf.concat(value, 2)\n",
    "# #         value, _ = tf.nn.dynamic_rnn(lstmCell, put, dtype=tf.float32, sequence_length=steps)\n",
    "# #         value, _ = tf.contrib.rnn.static_rnn(lstmCell, put, dtype=tf.float32)\n",
    "#         value = tf.transpose(value, [1, 0, 2])\n",
    "# # last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "#         last = value[-1]\n",
    "\n",
    "#         prediction = (tf.matmul(last, weight) + bias)\n",
    "#         return prediction\n",
    "    \n",
    "#     train_logits = model(data, steps)\n",
    "#     train_correctPred = tf.equal(tf.argmax(train_logits,1), tf.argmax(labels,1))\n",
    "#     train_accuracy = tf.reduce_mean(tf.cast(train_correctPred, tf.float32))\n",
    "    \n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=train_logits, labels=labels))\n",
    "\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# #     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \n",
    "#     tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    \n",
    "    \n",
    "#     input_test_data = tf.placeholder(tf.int32, [testSize, maxSeqLength])\n",
    "# #     input_test_data = tf.constant(input_test_data, tf.float32)\n",
    "#     test_labels = tf.placeholder(tf.float32, [testSize, numClasses])\n",
    "#     test_steps = tf.placeholder(tf.int32, [testSize])\n",
    "\n",
    "#     te_data = tf.Variable(tf.zeros([testSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     te_data = tf.nn.embedding_lookup(w2v_list,input_test_data)\n",
    "\n",
    "#     predictions = model(te_data, test_steps)\n",
    "#     prediction = tf.argmax(predictions, 1)\n",
    "#     test_label = tf.argmax(test_labels,1)\n",
    "#     test_loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=test_labels))\n",
    "#     correctPred = tf.equal(prediction, test_label)\n",
    "# # correctPred = tf.equal()\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "#     # calculate f1, recall\n",
    "#     ones_like_actuals = tf.ones_like(test_label)\n",
    "#     zeros_like_actuals = tf.zeros_like(test_label)\n",
    "#     ones_like_predictions = tf.ones_like(prediction)\n",
    "#     zeros_like_predictions = tf.zeros_like(prediction)\n",
    "    \n",
    "#     tp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     tn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "ATTENTION_SIZE = 50\n",
    "\n",
    "# import tensorflow as tf\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "\n",
    "#     labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "#     input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "#     steps = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "#     data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "# #     input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "#     keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')\n",
    "\n",
    "\n",
    "# #     lstmCell = tf.contrib.rnn.BasicRNNCell(lstmUnits, activation=tf.nn.relu)\n",
    "#     def get_a_cell():\n",
    "#         lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "# #     lstmCell_bk = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "# #         lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "#         return lstmCell\n",
    "    \n",
    "#     mlstmCell = tf.contrib.rnn.MultiRNNCell([get_a_cell() for i in range(3)])\n",
    "    \n",
    "#     weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "#     bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    \n",
    "#     def model(put, steps):\n",
    "# #         value, _ = tf.nn.bidirectional_dynamic_rnn(lstmCell, lstmCell_bk, put, dtype=tf.float32)\n",
    "        \n",
    "# #         value = tf.concat(value, 2)\n",
    "#         rnn_outputs, _ = tf.nn.dynamic_rnn(mlstmCell, put, dtype=tf.float32, sequence_length=steps)\n",
    "# #         value = tf.transpose(value, [1, 0, 2])\n",
    "# # last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "# #         last = _[-1][1]\n",
    "\n",
    "#         with tf.name_scope('Attention_layer'):\n",
    "#             attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "    \n",
    "#         drop = tf.nn.dropout(attention_output, keep_prob_ph)\n",
    "    \n",
    "#         prediction = (tf.matmul(drop, weight) + bias)\n",
    "#         return prediction\n",
    "    \n",
    "#     train_logits = model(data, steps)\n",
    "#     train_correctPred = tf.equal(tf.argmax(train_logits,1), tf.argmax(labels,1))\n",
    "#     train_accuracy = tf.reduce_mean(tf.cast(train_correctPred, tf.float32))\n",
    "    \n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=train_logits, labels=labels))\n",
    "\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# #     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "#     param = tf.trainable_variables()\n",
    "#     gradients_op = tf.gradients(loss, param)\n",
    "    \n",
    "#     tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    \n",
    "    \n",
    "#     input_test_data = tf.placeholder(tf.int32, [testSize, maxSeqLength])\n",
    "# #     input_test_data = tf.constant(input_test_data, tf.float32)\n",
    "#     test_labels = tf.placeholder(tf.float32, [testSize, numClasses])\n",
    "#     test_steps = tf.placeholder(tf.int32, [testSize])\n",
    "\n",
    "# #     te_data = tf.placeholder(tf.float32,[testSize, maxSeqLength, numDimensions])\n",
    "    \n",
    "#     te_data = tf.Variable(tf.zeros([testSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     te_data = tf.nn.embedding_lookup(w2v_list,input_test_data)\n",
    "# #     te_input = tf.unstack(te_data, maxSeqLength, 1)\n",
    "\n",
    "#     predictions = model(te_data, test_steps)\n",
    "#     prediction = tf.argmax(predictions, 1)\n",
    "#     test_label = tf.argmax(test_labels,1)\n",
    "#     test_loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=test_labels))\n",
    "#     correctPred = tf.equal(prediction, test_label)\n",
    "# # correctPred = tf.equal()\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "#     # calculate f1, recall\n",
    "#     ones_like_actuals = tf.ones_like(test_label)\n",
    "#     zeros_like_actuals = tf.zeros_like(test_label)\n",
    "#     ones_like_predictions = tf.ones_like(prediction)\n",
    "#     zeros_like_predictions = tf.zeros_like(prediction)\n",
    "    \n",
    "#     tp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     tn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRUCell\n",
    "# import tensorflow as tf\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "\n",
    "#     labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "#     input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "#     steps = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# #     data = tf.placeholder(tf.float32,[batchSize, maxSeqLength, numDimensions])\n",
    "    \n",
    "#     data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "# #     input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "# #     lstmCell = tf.contrib.rnn.BasicRNNCell(lstmUnits, activation=tf.nn.relu)\n",
    "# #     lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "\n",
    "#     lstmCell = tf.contrib.rnn.GRUCell(lstmUnits)\n",
    "# #     lstmCell_bk = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "#     lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "\n",
    "#     weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "#     bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    \n",
    "#     def model(put, steps):\n",
    "# #         value, _ = tf.nn.bidirectional_dynamic_rnn(lstmCell, lstmCell_bk, put, dtype=tf.float32)\n",
    "        \n",
    "# #         value = tf.concat(value, 2)\n",
    "#         value, _ = tf.nn.dynamic_rnn(lstmCell, put, dtype=tf.float32, sequence_length=steps)\n",
    "# #         value, _ = tf.contrib.rnn.static_rnn(lstmCell, put, dtype=tf.float32)\n",
    "#         value = tf.transpose(value, [1, 0, 2])\n",
    "# # last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "# #         last = value[-1]\n",
    "#         last = _\n",
    "# #         last = _\n",
    "#         print(value.shape, _.shape, weight.shape, bias.shape)\n",
    "#         # 为什么这里使用3种维度的输入就得用last_state来进行计算，而之前二重维度就用value来计算呢？\n",
    "#         prediction = (tf.matmul(last, weight) + bias)\n",
    "#         return prediction\n",
    "    \n",
    "#     train_logits = model(data, steps)\n",
    "#     train_correctPred = tf.equal(tf.argmax(train_logits,1), tf.argmax(labels,1))\n",
    "#     train_accuracy = tf.reduce_mean(tf.cast(train_correctPred, tf.float32))\n",
    "    \n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=train_logits, labels=labels))\n",
    "\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# #     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \n",
    "#     tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    \n",
    "    \n",
    "#     input_test_data = tf.placeholder(tf.int32, [testSize, maxSeqLength])\n",
    "# #     input_test_data = tf.constant(input_test_data, tf.float32)\n",
    "#     test_labels = tf.placeholder(tf.float32, [testSize, numClasses])\n",
    "#     test_steps = tf.placeholder(tf.int32, [testSize])\n",
    "\n",
    "# #     te_data = tf.placeholder(tf.float32,[testSize, maxSeqLength, numDimensions])\n",
    "    \n",
    "#     te_data = tf.Variable(tf.zeros([testSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     te_data = tf.nn.embedding_lookup(w2v_list,input_test_data)\n",
    "# #     te_input = tf.unstack(te_data, maxSeqLength, 1)\n",
    "# #     print(len(te_input))\n",
    "# #     print(te_data)\n",
    "# #     print(te_data.shape)\n",
    "# #     print(te_input[0].shape)\n",
    "# #     print(te_input[0])\n",
    "# #     prediction = model(te_input)\n",
    "#     predictions = model(te_data, test_steps)\n",
    "#     prediction = tf.argmax(predictions, 1)\n",
    "#     test_label = tf.argmax(test_labels,1)\n",
    "#     test_loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=test_labels))\n",
    "#     correctPred = tf.equal(prediction, test_label)\n",
    "# # correctPred = tf.equal()\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "#     # calculate f1, recall\n",
    "#     ones_like_actuals = tf.ones_like(test_label)\n",
    "#     zeros_like_actuals = tf.zeros_like(test_label)\n",
    "#     ones_like_predictions = tf.ones_like(prediction)\n",
    "#     zeros_like_predictions = tf.zeros_like(prediction)\n",
    "    \n",
    "#     tp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     tn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "# layer_num = 2\n",
    "# import tensorflow as tf\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "\n",
    "#     labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "#     input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "#     steps = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "#     data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "# #     input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "#     def get_a_cell():\n",
    "#         lstmCell = tf.contrib.rnn.BasicRNNCell(lstmUnits)\n",
    "# # , activation=tf.nn.relu\n",
    "# #     lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "# #     lstmCell_bk = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "#         lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "#         return lstmCell\n",
    "#     mlstmCell = tf.contrib.rnn.MultiRNNCell([get_a_cell() for i in range(layer_num)], state_is_tuple=True)\n",
    "# #     init_state = lstmCell.zero_state(batchSize, dtype=tf.float32)\n",
    "#     weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "#     bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    \n",
    "#     def model(put, steps):\n",
    "# #         value, _ = tf.nn.bidirectional_dynamic_rnn(lstmCell, lstmCell_bk, put, dtype=tf.float32)\n",
    "        \n",
    "# #         value = tf.concat(value, 2)\n",
    "#         value, _ = tf.nn.dynamic_rnn(mlstmCell, put, steps ,dtype=tf.float32)\n",
    "# #         value, _ = tf.contrib.rnn.static_rnn(lstmCell, put, dtype=tf.float32)\n",
    "#         value = tf.transpose(value, [1, 0, 2])\n",
    "# # last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "#         last = value[-1]\n",
    "    \n",
    "#         prediction = (tf.matmul(last, weight) + bias)\n",
    "#         return prediction\n",
    "    \n",
    "#     train_logits = model(data, steps)\n",
    "#     train_correctPred = tf.equal(tf.argmax(train_logits,1), tf.argmax(labels,1))\n",
    "#     train_accuracy = tf.reduce_mean(tf.cast(train_correctPred, tf.float32))\n",
    "    \n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=train_logits, labels=labels))\n",
    "#     gradients_op = tf.gradients(ys=loss, xs=weight)\n",
    "# #     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    \n",
    "#     tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    \n",
    "    \n",
    "#     input_test_data = tf.placeholder(tf.int32, [testSize, maxSeqLength])\n",
    "# #     input_test_data = tf.constant(input_test_data, tf.float32)\n",
    "#     test_labels = tf.placeholder(tf.float32, [testSize, numClasses])\n",
    "#     test_steps = tf.placeholder(tf.int32, [testSize])\n",
    "\n",
    "#     te_data = tf.Variable(tf.zeros([testSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     te_data = tf.nn.embedding_lookup(w2v_list,input_test_data)\n",
    "# #     te_input = tf.unstack(te_data, maxSeqLength, 1)\n",
    "# #     print(len(te_input))\n",
    "# #     print(te_data)\n",
    "# #     print(te_data.shape)\n",
    "# #     print(te_input[0].shape)\n",
    "# #     print(te_input[0])\n",
    "# #     prediction = model(te_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     predictions = model(te_data, test_steps)\n",
    "#     prediction = tf.argmax(predictions, 1)\n",
    "#     test_label = tf.argmax(test_labels,1)\n",
    "    \n",
    "#     correctPred = tf.equal(prediction, test_label)\n",
    "# # correctPred = tf.equal()\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "#     # calculate f1, recall\n",
    "#     ones_like_actuals = tf.ones_like(test_label)\n",
    "#     zeros_like_actuals = tf.zeros_like(test_label)\n",
    "#     ones_like_predictions = tf.ones_like(prediction)\n",
    "#     zeros_like_predictions = tf.zeros_like(prediction)\n",
    "    \n",
    "#     tp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     tn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 24, 64)\n",
      "(<tf.Tensor 'rnn/while/Exit_3:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/while/Exit_4:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/while/Exit_5:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/while/Exit_6:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/while/Exit_7:0' shape=(24, 64) dtype=float32>)\n",
      "(300, 2500, 64)\n",
      "(<tf.Tensor 'rnn_1/while/Exit_3:0' shape=(2500, 64) dtype=float32>, <tf.Tensor 'rnn_1/while/Exit_4:0' shape=(2500, 64) dtype=float32>, <tf.Tensor 'rnn_1/while/Exit_5:0' shape=(2500, 64) dtype=float32>, <tf.Tensor 'rnn_1/while/Exit_6:0' shape=(2500, 64) dtype=float32>, <tf.Tensor 'rnn_1/while/Exit_7:0' shape=(2500, 64) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "# # indRNN\n",
    "layer_num = 5\n",
    "import tensorflow as tf\n",
    "from ind_rnn_cell import IndRNNCell\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "    input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "    steps = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "    data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "#     input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "#     lstmCell = tf.contrib.rnn.BasicRNNCell(lstmUnits, activation=tf.nn.relu)\n",
    "\n",
    "    recurrent_max = pow(2, 1 / maxSeqLength)\n",
    "\n",
    "#     lstmCell = IndRNNCell(128,)\n",
    "#     lstmCell = IndRNNCell(lstmUnits)\n",
    "    def get_a_cell():\n",
    "#         cell = IndRNNCell(lstmUnits, )\n",
    "        cell = IndRNNCell(lstmUnits, recurrent_max_abs=recurrent_max)\n",
    "\n",
    "        return cell\n",
    "    lstmCell = tf.contrib.rnn.MultiRNNCell([get_a_cell() for i in range(layer_num)])\n",
    "#     lstmCell = tf.contrib.rnn.MultiRNNCell([IndRNNCell(lstmUnits, recurrent_max_abs=recurrent_max), IndRNNCell(lstmUnits, recurrent_max_abs=recurrent_max)])\n",
    "\n",
    "#     lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "#     lstmCell_bk = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "#     lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    \n",
    "    def model(put, steps):\n",
    "        value, _ = tf.nn.dynamic_rnn(lstmCell, put, dtype=tf.float32)\n",
    "        \n",
    "        value = tf.concat(value, 2)\n",
    "\n",
    "        value = tf.transpose(value, [1, 0, 2])\n",
    "# last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "\n",
    "        \n",
    "        last = value[-1]\n",
    "        print(value.shape)\n",
    "        print(_)\n",
    "    \n",
    "        prediction = (tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    train_logits = model(data, steps)\n",
    "    train_correctPred = tf.equal(tf.argmax(train_logits,1), tf.argmax(labels,1))\n",
    "    train_accuracy = tf.reduce_mean(tf.cast(train_correctPred, tf.float32))\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=train_logits, labels=labels))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    \n",
    "    \n",
    "    input_test_data = tf.placeholder(tf.int32, [testSize, maxSeqLength])\n",
    "#     input_test_data = tf.constant(input_test_data, tf.float32)\n",
    "    test_labels = tf.placeholder(tf.float32, [testSize, numClasses])\n",
    "    test_steps = tf.placeholder(tf.int32, [testSize])\n",
    "\n",
    "    te_data = tf.Variable(tf.zeros([testSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    te_data = tf.nn.embedding_lookup(w2v_list,input_test_data)\n",
    "#     te_input = tf.unstack(te_data, maxSeqLength, 1)\n",
    "#     print(len(te_input))\n",
    "#     print(te_data)\n",
    "#     print(te_data.shape)\n",
    "#     print(te_input[0].shape)\n",
    "#     print(te_input[0])\n",
    "#     prediction = model(te_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    predictions = model(te_data, test_steps)\n",
    "    prediction = tf.argmax(predictions, 1)\n",
    "    test_label = tf.argmax(test_labels,1)\n",
    "    \n",
    "    correctPred = tf.equal(prediction, test_label)\n",
    "# correctPred = tf.equal()\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "    # calculate f1, recall\n",
    "    ones_like_actuals = tf.ones_like(test_label)\n",
    "    zeros_like_actuals = tf.zeros_like(test_label)\n",
    "    ones_like_predictions = tf.ones_like(prediction)\n",
    "    zeros_like_predictions = tf.zeros_like(prediction)\n",
    "    \n",
    "    tp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(test_label, ones_like_actuals), \n",
    "        tf.equal(prediction, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "    tn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(test_label, zeros_like_actuals), \n",
    "        tf.equal(prediction, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "    fp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(test_label, zeros_like_actuals), \n",
    "        tf.equal(prediction, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "    fn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(test_label, ones_like_actuals), \n",
    "        tf.equal(prediction, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(value[0])\n",
    "# print(value.get_shape())\n",
    "# print(value.get_shape()[0])\n",
    "# print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test_prediction = tf.nn.softmax(prediction)\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(last))\n",
    "#     print(last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(int(value.get_shape()[0]) - 1)\n",
    "# print(last)\n",
    "# print(weight)\n",
    "# print(prediction)\n",
    "# print(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.argmax(prediction, 1))\n",
    "#     print(tf.argmax(prediction, 1))\n",
    "#     print(tf.argmax(labels, 1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import randint\n",
    "\n",
    "# def getTrainBatch():\n",
    "#     labels = []\n",
    "#     arr = np.zeros([batchSize, maxSeqLength])\n",
    "#     steps = []\n",
    "#     for i in range(batchSize):\n",
    "#         if (i % 2 == 0): \n",
    "#             num = randint(0,9700)\n",
    "#             labels.append([1,0])\n",
    "#         else:\n",
    "#             num = randint(10695,)\n",
    "#             labels.append([0,1])\n",
    "# #         print(i, ids[num-1])\n",
    "#         steps.append(num)\n",
    "#         arr[i] = ids[num]\n",
    "#     return arr, labels, steps\n",
    "\n",
    "# # def getTestBatch():\n",
    "# #     labels = []\n",
    "# #     arr = np.zeros([batchSize, maxSeqLength])\n",
    "# #     for i in range(batchSize):\n",
    "# #         num = randint(1800,2199)\n",
    "# #         if (num < 2000):\n",
    "# #             labels.append([1,0])\n",
    "# #         else:\n",
    "# #             labels.append([0,1])\n",
    "# #         arr[i] = ids[num]\n",
    "# #     return arr, labels\n",
    "\n",
    "# def getTestBatch():\n",
    "#     labels = []\n",
    "#     arr = np.zeros([testSize, maxSeqLength])\n",
    "#     steps = []\n",
    "#     for i in range(1800,2200):\n",
    "#         if (i < 2000):\n",
    "#             labels.append([1,0])\n",
    "#         else:\n",
    "#             labels.append([0,1])\n",
    "#         steps.append(i)\n",
    "#         arr[i-1800] = ids[i]\n",
    "#     return arr, labels, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData = ids[0:1799]+ids[2200:3999]\n",
    "# trainLabels = []\n",
    "# for i in range(1800):\n",
    "#     trainLabels.append(np.array([1,0]))\n",
    "# for i in range(1800):\n",
    "#     trainLabels.append(np.array([0,1]))\n",
    "# trainLabels = np.array(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainData.shape)\n",
    "# print(trainLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(graph=graph)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "# tf.summary.scalar('Loss', loss)\n",
    "# tf.summary.scalar('Accuracy', accuracy)\n",
    "# merged = tf.summary.merge_all()\n",
    "# logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "# writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 300)\n",
      "(22500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(trainData.shape)\n",
    "print(trainLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keep_prob_ph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-6dcbef3a298c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#                    steps:trainSteps[offset:offset + batchSize],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                \u001b[0mkeep_prob_ph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m               }\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keep_prob_ph' is not defined"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "#     offset = (i*batchSize) % (3600 - batchSize)\n",
    "#     feed_\n",
    "#     nextBatch, nextBatchLabels, nextSteps = getTrainBatch();\n",
    "    offset = (i * batchSize) % (len(trainLabels)-batchSize)\n",
    "    feed_dict={input_data:trainData[offset:offset + batchSize],\n",
    "                   labels:trainLabels[offset:offset + batchSize],\n",
    "#                    steps:trainSteps[offset:offset + batchSize],\n",
    "               keep_prob:0.75\n",
    "              }\n",
    "#     \n",
    "#     nextBatch = trainData[offset:offset+batchSize]\n",
    "#     nextBatchLabels = trainLabels[offset:offset+batchSize]\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    if (i % 500 == 0):\n",
    "#         batch_acc, lo = sess.run([train_accuracy,loss], {input_data: nextBatch, labels: nextBatchLabels, steps:nextSteps})\n",
    "        batch_acc, lo = sess.run([train_accuracy,loss], feed_dict=feed_dict)\n",
    "#         nextBatch, nextBatchLabels, nextSteps = getTestBatch();\n",
    "#         offset_test = (step * batch_size) % (len(trainLabels)-batch_size)\n",
    "        feed_dict_test={input_data:testData,\n",
    "                   labels:testLabels,\n",
    "#                    test_steps:testSteps,\n",
    "               keep_prob:0.75\n",
    "                       }\n",
    "#         feed_dict_test={input_test_data:testData,\n",
    "#                    test_labels:testLabels}\n",
    "#         print(nextBatchLabels.shape)\n",
    "#         print(nextBatchLabels)\n",
    "#         acc = sess.run(accuracy, {input_test_data: nextBatch, test_labels: nextBatchLabels, test_steps:nextSteps})\n",
    "        acc, test_loss, tp, tn, fp, fn= sess.run([accuracy, test_loss_op, tp_op, tn_op, fp_op, fn_op], feed_dict=feed_dict_test)\n",
    "        loss_list.append(lo)\n",
    "        acc_list.append(acc)\n",
    "        \n",
    "        tpr = float(tp)/(float(tp) + float(fn))\n",
    "        fpr = float(fp)/(float(tp) + float(fn))\n",
    "\n",
    "        \n",
    "        accuracy_off = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "\n",
    "        \n",
    "        recall = tpr\n",
    "        try:\n",
    "            precision = float(tp)/(float(tp) + float(fp))\n",
    "        except ZeroDivisionError:\n",
    "            precision = 10.0\n",
    "\n",
    "        f1_score = (2 * (precision * recall)) / (float(precision) + float(recall))\n",
    "        \n",
    "#         print(\"gradients:%s\" % (gradients))\n",
    "        print(\"loss:%s batch_Acc: %s test_loss:%s acc:%s\" % (lo, batch_acc, test_loss,acc))\n",
    "        print(\"acc_off:%.3f recall:%.3f precision:%.3f f1:%.3f\" % (accuracy_off, recall, precision, f1_score))\n",
    "#         print(summary)\n",
    "    \n",
    "#    Save the network every 10,000 training iterations\n",
    "#     if (i % 10000 == 0 and i != 0):\n",
    "#         save_path = saver.save(sess, \"models_lstm/pretrained_lstm.ckpt\", global_step=i)\n",
    "#         print(\"saved to %s\" % save_path)\n",
    "print(\"answer\")\n",
    "end = time.time()\n",
    "print(\"Used:%.3f\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt.hist(loss_list, 50)\n",
    "plt.plot(range(21), loss_list)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.axis([0, 20, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "%matplotlib inline\n",
    "# plt.hist(loss_list, 50)\n",
    "plt.plot(range(21), acc_list)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Acc')\n",
    "# plt.axis([0, 20, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# testData = ids[1800:1999]+ids[2000:2199]\n",
    "# testLabels = []\n",
    "# for i in range(200):\n",
    "#     testLabels.append(np.array([1,0]))\n",
    "# for i in range(200):\n",
    "#     testLabels.append(np.array([0,1]))\n",
    "# testLabels = np.array(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# iterations = 10\n",
    "# acc_total = 0;\n",
    "# for i in range(iterations):\n",
    "#     offset = (i*batchSize) % (400 - batchSize)\n",
    "# #     feed_\n",
    "# #     nextBatch, nextBatchLabels = getTrainBatch();\n",
    "# #     nextBatch = testData[offset:offset+batchSize]\n",
    "# #     nextBatchLabels = testLabels[offset:offset+batchSize]\n",
    "#     nextBatch, nextBatchLabels = getTestBatch();\n",
    "    \n",
    "# #     summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "# #     writer.add_summary(summary, i)\n",
    "    \n",
    "#     acc_this_batch = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "#     acc_total +=acc_this_batch\n",
    "#     print(\"Accuracy for this batch:\",acc_total * 100 / (i+1))\n",
    "#     nextBatch, nextBatchLabels = getTestBatch();\n",
    "#     print(\"Loss for this batch:\", (sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "# # writer.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceMatrix(sentence):\n",
    "    arr_step = np.zeros([testSize], dtype='int32')\n",
    "    sentenceMatrix = np.zeros([testSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = jieba.cut(cleanedSentence)\n",
    "    \n",
    "    indexCounter = 0\n",
    "    for word in split:\n",
    "        if word in stoplist:\n",
    "            continue\n",
    "        try:\n",
    "            print(word)\n",
    "            sentenceMatrix[0][indexCounter] = word_list.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0][indexCounter] = 400000 #Vector for unkown\n",
    "        indexCounter += 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    arr_step[0] = indexCounter\n",
    "    return sentenceMatrix, arr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"非常差。国内最差的酒店之一。各方面都不好。\"\n",
    "inputMatrix, inputSteps = getSentenceMatrix(inputText)\n",
    "print(inputMatrix.shape)\n",
    "print(input_test_data.shape)\n",
    "print(test_steps.shape)\n",
    "print(inputSteps.shape)\n",
    "\n",
    "\n",
    "predictedSentiment = sess.run(predictions, {input_test_data: inputMatrix, test_steps: inputSteps})\n",
    "print(predictedSentiment)\n",
    "# if (predictedSentiment > ):\n",
    "#     print(\"Positive Sentiment\")\n",
    "# else:\n",
    "#     print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.argmax(predictedSentiment,1).eval())\n",
    "\n",
    "# if (predictedSentiment > 0.5):\n",
    "#     print(\"Positive Sentiment\")\n",
    "# else:\n",
    "#     print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
