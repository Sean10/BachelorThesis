{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = {line.strip() for line in open(filepath, \"r\", encoding='utf-8').readlines()}\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def cleanSentences(string):\n",
    "    return re.sub(\"[a-zA-Z0-9\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！<>《》，。？、~@#￥%……&*（） ]+\", \"\",string)\n",
    "\n",
    "\n",
    "program = os.path.basename(\"deep learning\")\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "# positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "# negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "\n",
    "# files = positiveFiles + negativeFiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [\"corpus.txt\"]\n",
    "\n",
    "# vec = []\n",
    "\n",
    "\n",
    "# # logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "# i = 0\n",
    "# for file in files:\n",
    "#     with open(file, \"r\",encoding='utf-8') as f:\n",
    "#         if os.path.exists(\"corpus_segment.txt\"):\n",
    "#             continue\n",
    "        \n",
    "#         fp = codecs.open(\"corpus_segment.txt\", \"a+\", encoding=\"utf-8\")\n",
    "        \n",
    "#         for line in f:\n",
    "#             line = cleanSentences(line)\n",
    "#             if line.strip() is \"\n",
    "#                 continue\n",
    "            \n",
    "#             sentence_seged = jieba.cut(line.strip())\n",
    "# #             print(type(sentence_seged))\n",
    "# #             tmp = []\n",
    "# #             for word in sentence_seged:\n",
    "# #                 if word.strip() not in stoplist:\n",
    "# #                     tmp.append(word)\n",
    "# #             vec.append(tmp)\n",
    "# #             tmp.append(word)\n",
    "#             fp.write(\" \".join(sentence_seged))\n",
    "            \n",
    "#             i += 1\n",
    "#             if i % 1000 == 0:\n",
    "#                 logger.info(\"Saved \"+ str(i) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(i) + \" articles\")\n",
    "# # print(vec)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-25 14:01:37,605: INFO: 'pattern' package not found; tag filters are not available for English\n",
      "2018-04-25 14:01:37,615: INFO: loading Word2Vec object from model_word2vec\n",
      "2018-04-25 14:01:41,023: INFO: loading wv recursively from model_word2vec.wv.* with mmap=None\n",
      "2018-04-25 14:01:41,024: INFO: loading vectors from model_word2vec.wv.vectors.npy with mmap=None\n",
      "2018-04-25 14:01:42,549: INFO: setting ignored attribute vectors_norm to None\n",
      "2018-04-25 14:01:42,551: INFO: loading vocabulary recursively from model_word2vec.vocabulary.* with mmap=None\n",
      "2018-04-25 14:01:42,552: INFO: loading trainables recursively from model_word2vec.trainables.* with mmap=None\n",
      "2018-04-25 14:01:42,553: INFO: loading syn1neg from model_word2vec.trainables.syn1neg.npy with mmap=None\n",
      "2018-04-25 14:01:44,828: INFO: setting ignored attribute cum_table to None\n",
      "2018-04-25 14:01:44,830: INFO: loaded model_word2vec\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "似乎不需要去除停用词\n",
    "'''\n",
    "# stoplist = stopwordslist(\"stopwords.txt\")\n",
    "# fout = \"corpus_seg_without_stop.txt\"\n",
    "# cnt = 0\n",
    "# with codecs.open(\"corpus_segment.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         tmp = []\n",
    "#         for word in line.split():\n",
    "#             if word.strip() in stoplist:\n",
    "#                continue\n",
    "#             tmp.append(word)\n",
    "#         fout.write(\" \".join(tmp))\n",
    "#         cnt += 1\n",
    "#         if cnt % 1000 == 0:\n",
    "#             logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# input = \"corpus_segment.txt\"\n",
    "# model = Word2Vec(LineSentence(input), size=400, window=5,min_count=5,workers=multiprocessing.cpu_count())\n",
    "# model.save('model_word2vec')\n",
    "# model.wv.save_word2vec_format(\"model_saved_format_version\", binary=False)  \n",
    "\n",
    "model = Word2Vec.load(\"model_word2vec\")\n",
    "# for mo in model.wv.vocab:\n",
    "#     print(mo, model.wv[mo])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w2v_list = np.array([model.wv[word] for word in model.wv.vocab])\n",
    "word_list = np.array([word for word in model.wv.vocab]).tolist()\n",
    "# print(w2v_np)\n",
    "\n",
    "\n",
    "# print(model[u'n'])\n",
    "# print(model.wv[u'机会'])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-25 14:01:52,545: INFO: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572297, 400)\n",
      "[('婚姻', 0.6089010834693909), ('情感', 0.6044034957885742), ('感情', 0.5982906818389893), ('恋爱', 0.5788689851760864), ('爱恋', 0.5739886164665222), ('情爱', 0.573617696762085), ('真爱', 0.5732499361038208), ('人生', 0.567855715751648), ('爱情故事', 0.5486623644828796), ('情感故事', 0.5435651540756226)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_list.shape)\n",
    "print(model.wv.most_similar('爱情'))\n",
    "# print(model.wv.similarity('爱情','宾馆'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-25 14:02:01,771: INFO: Saved 0 articles\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwordslist(\"stopwords.txt\")\n",
    "\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "\n",
    "seg_positiveFiles = ['seged_positiveReviews/' + f for f in listdir('seged_positiveReviews/') if isfile(join('seged_positiveReviews/', f))]\n",
    "seg_negativeFiles = ['seged_negativeReviews/' + f for f in listdir('seged_negativeReviews/') if isfile(join('seged_negativeReviews/', f))]\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "if os.path.exists(\"seged_positiveReviews\"):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(\"seged_positiveReviews\")\n",
    "\n",
    "if os.path.exists(\"seged_negativeReviews\"):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(\"seged_negativeReviews\")\n",
    "    \n",
    "for pf in positiveFiles:\n",
    "    positive_seged = \"seged_\"+pf\n",
    "    \n",
    "    if os.path.exists(positive_seged):\n",
    "        continue\n",
    "        \n",
    "    with open(pf, \"r\") as f:\n",
    "        file = []\n",
    "        for line in f:\n",
    "            line = cleanSentences(line).strip()\n",
    "            sentence_seged = jieba.cut(line)\n",
    "            \n",
    "            temp = []\n",
    "            for word in sentence_seged:\n",
    "                if word in stoplist:\n",
    "                    continue;\n",
    "                temp.append(word)\n",
    "            file.append(\" \".join(temp))\n",
    "        with open(positive_seged, \"w\",encoding='utf-8') as of:\n",
    "            of.write(\"\\n\".join(file))\n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "for pf in negativeFiles:\n",
    "    negative_seged = \"seged_\"+pf\n",
    "    \n",
    "    if os.path.exists(negative_seged):\n",
    "        continue\n",
    "    \n",
    "    with open(pf, \"r\") as f:\n",
    "        file = []\n",
    "        for line in f:\n",
    "            line = cleanSentences(line).strip()\n",
    "            sentence_seged = jieba.cut(line)\n",
    "            \n",
    "            temp = []\n",
    "            for word in sentence_seged:\n",
    "                if word in stoplist:\n",
    "                    continue;\n",
    "                temp.append(word)\n",
    "            file.append(\" \".join(temp))\n",
    "        with open(negative_seged, \"w\",encoding='utf-8') as of:\n",
    "            of.write(\"\\n\".join(file))\n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "logger.info(\"Saved \"+ str(cnt) + \" articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 4000\n",
      "The total number of words in the files is 129414\n",
      "The average number of words in the files is 32.3535\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "numWords = []\n",
    "for pf in seg_positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(\"\".join(line.split()))\n",
    "#         print(line.split(), len(line.split()))\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in seg_negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 200, 0, 2500])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "maxSeqLength = 50\n",
    "numDimensions = 300\n",
    "# firstSentense = np.zeros((maxSeqLength), dtype='int32')\n",
    "# firstSentense[0] = word_list.index('宾馆')\n",
    "# firstSentense[1] = word_list.index('喜欢')\n",
    "# firstSentense[2] = word_list.index('爱')\n",
    "# print(firstSentense.shape)\n",
    "# print(firstSentense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "\n",
    "for pf in seg_positiveFiles:\n",
    "    with open(pf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "#         print(line.split())\n",
    "        \n",
    "        for word in line.split():\n",
    "            try:\n",
    "#                 print(word, word_list.index(word))\n",
    "                ids[fileCounter][indexCounter] = word_list.index(word)\n",
    "            except ValueError:\n",
    "#                 print(word)\n",
    "                ids[fileCounter][indexCounter] = 572296 #Vector for unkown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        fileCounter = fileCounter + 1\n",
    "\n",
    "for nf in seg_negativeFiles:\n",
    "    with open(nf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        for word in line.split():\n",
    "            try:\n",
    "#                 print(word, word_list.index(word))\n",
    "                ids[fileCounter][indexCounter] = word_list.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 572296 #Vector for unkown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        fileCounter = fileCounter + 1 \n",
    "\n",
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "np.save('idsMatrix', ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15636  4671  4932  4750  6089   480  8474  8472  6322 35935 34064   331\n",
      " 33169 49905 14319   797     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(ids[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x11d682518>\n"
     ]
    }
   ],
   "source": [
    "batchSize = 24\n",
    "Units = 64\n",
    "numClasses = 2\n",
    "iterations = 10000\n",
    "# try 10000 iteration\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "print(labels.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "将向量转换成嵌入矩阵的过程\n",
    "'''\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNCell = tf.contrib.\n",
    "RNNCell = tf.contrib.rnn.BasicRNNCell(Units)\n",
    "RNNCell = tf.contrib.rnn.DropoutWrapper(cell=RNNCell, output_keep_prob=0.75)\n",
    "\n",
    "input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "\n",
    "# value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "value, _ = tf.contrib.rnn.static_rnn(RNNCell, input, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'rnn/dropout/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_1/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_2/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_3/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_4/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_5/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_6/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_7/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_8/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_9/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_10/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_11/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_12/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_13/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_14/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_15/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_16/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_17/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_18/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_19/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_20/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_21/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_22/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_23/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_24/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_25/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_26/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_27/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_28/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_29/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_30/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_31/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_32/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_33/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_34/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_35/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_36/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_37/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_38/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_39/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_40/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_41/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_42/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_43/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_44/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_45/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_46/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_47/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_48/mul:0' shape=(24, 64) dtype=float32>, <tf.Tensor 'rnn/dropout_49/mul:0' shape=(24, 64) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# print(value[0])\n",
    "# print(value.get_shape())\n",
    "# print(value.get_shape()[0])\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([Units, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "# value = tf.transpose(value, [1, 0, 2])\n",
    "# last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "last = value[-1]\n",
    "# print(value)\n",
    "# print(last)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(last))\n",
    "#     print(last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# print(int(value.get_shape()[0]) - 1)\n",
    "# print(last)\n",
    "# print(weight)\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.argmax(prediction, 1))\n",
    "#     print(tf.argmax(prediction, 1))\n",
    "#     print(tf.argmax(labels, 1))\n",
    "\n",
    "\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,1800)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(2200,3999)\n",
    "            labels.append([0,1])\n",
    "#         print(i, ids[num-1])\n",
    "        arr[i] = ids[num-1]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1801,2199)\n",
    "        if (num <= 2000):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5416667, loss:4.249918\n",
      "acc: 0.5, loss:2.2003753\n",
      "acc: 0.45833334, loss:1.4240781\n",
      "acc: 0.45833334, loss:1.8227111\n",
      "acc: 0.5416667, loss:1.4293278\n",
      "acc: 0.5416667, loss:2.3557274\n",
      "acc: 0.625, loss:1.825015\n",
      "acc: 0.7083333, loss:1.662933\n",
      "acc: 0.5416667, loss:1.9536266\n",
      "acc: 0.5, loss:1.5085148\n",
      "acc: 0.5, loss:1.1289454\n",
      "acc: 0.5833333, loss:1.6365818\n",
      "acc: 0.5833333, loss:1.5152602\n",
      "acc: 0.625, loss:1.0962486\n",
      "acc: 0.5833333, loss:2.0134935\n",
      "acc: 0.5833333, loss:1.4623518\n",
      "acc: 0.75, loss:1.4494401\n",
      "acc: 0.5833333, loss:1.4236269\n",
      "acc: 0.6666667, loss:1.3673633\n",
      "acc: 0.45833334, loss:1.0259522\n",
      "acc: 0.375, loss:1.4418453\n",
      "saved to models_rnn/pretrained_lstm.ckpt-1000\n",
      "acc: 0.5, loss:1.4585844\n",
      "acc: 0.5, loss:1.0489204\n",
      "acc: 0.6666667, loss:0.731766\n",
      "acc: 0.45833334, loss:1.5084099\n",
      "acc: 0.5833333, loss:0.8714094\n",
      "acc: 0.5, loss:1.2819722\n",
      "acc: 0.625, loss:0.7172076\n",
      "acc: 0.5, loss:1.6774119\n",
      "acc: 0.5416667, loss:1.094158\n",
      "acc: 0.5833333, loss:0.8225134\n",
      "acc: 0.45833334, loss:1.2247051\n",
      "acc: 0.7916667, loss:1.219021\n",
      "acc: 0.45833334, loss:1.2733514\n",
      "acc: 0.33333334, loss:1.3667539\n",
      "acc: 0.375, loss:1.0677179\n",
      "acc: 0.5, loss:0.7408538\n",
      "acc: 0.6666667, loss:0.83765835\n",
      "acc: 0.5, loss:1.0134189\n",
      "acc: 0.5, loss:1.0370423\n",
      "acc: 0.6666667, loss:0.66675085\n",
      "saved to models_rnn/pretrained_lstm.ckpt-2000\n",
      "acc: 0.625, loss:1.249575\n",
      "acc: 0.41666666, loss:0.8947156\n",
      "acc: 0.5416667, loss:1.0383748\n",
      "acc: 0.6666667, loss:0.48798642\n",
      "acc: 0.625, loss:1.0255507\n",
      "acc: 0.625, loss:0.882786\n",
      "acc: 0.5416667, loss:1.1269144\n",
      "acc: 0.5, loss:0.8105189\n",
      "acc: 0.625, loss:0.59838045\n",
      "acc: 0.5416667, loss:0.68010306\n",
      "acc: 0.5, loss:0.7770949\n",
      "acc: 0.5833333, loss:0.7262252\n",
      "acc: 0.6666667, loss:0.48224416\n",
      "acc: 0.45833334, loss:0.81146353\n",
      "acc: 0.5416667, loss:0.7087814\n",
      "acc: 0.5833333, loss:0.7914004\n",
      "acc: 0.625, loss:0.65887606\n",
      "acc: 0.6666667, loss:0.6676896\n",
      "acc: 0.375, loss:0.6117573\n",
      "acc: 0.41666666, loss:0.8344583\n",
      "saved to models_rnn/pretrained_lstm.ckpt-3000\n",
      "acc: 0.5833333, loss:0.72596663\n",
      "acc: 0.5416667, loss:0.83055925\n",
      "acc: 0.625, loss:0.6570442\n",
      "acc: 0.5, loss:0.7844596\n",
      "acc: 0.6666667, loss:0.82614\n",
      "acc: 0.625, loss:0.615218\n",
      "acc: 0.75, loss:0.5266358\n",
      "acc: 0.5833333, loss:0.77212864\n",
      "acc: 0.625, loss:0.6369269\n",
      "acc: 0.7083333, loss:0.54758006\n",
      "acc: 0.7916667, loss:0.5125513\n",
      "acc: 0.45833334, loss:0.7115914\n",
      "acc: 0.625, loss:0.6776629\n",
      "acc: 0.6666667, loss:0.62014157\n",
      "acc: 0.41666666, loss:0.7776153\n",
      "acc: 0.5, loss:0.5794813\n",
      "acc: 0.5416667, loss:0.67281103\n",
      "acc: 0.7083333, loss:0.5070222\n",
      "acc: 0.75, loss:0.6440661\n",
      "acc: 0.5833333, loss:0.58035254\n",
      "saved to models_rnn/pretrained_lstm.ckpt-4000\n",
      "acc: 0.5, loss:0.6357937\n",
      "acc: 0.7083333, loss:0.59973\n",
      "acc: 0.5, loss:0.84133244\n",
      "acc: 0.5833333, loss:0.7638757\n",
      "acc: 0.625, loss:0.7524288\n",
      "acc: 0.5, loss:0.6233487\n",
      "acc: 0.5833333, loss:0.66757745\n",
      "acc: 0.8333333, loss:0.5945513\n",
      "acc: 0.45833334, loss:0.5942376\n",
      "acc: 0.75, loss:0.47573408\n",
      "acc: 0.5833333, loss:0.56644636\n",
      "acc: 0.5, loss:0.6418652\n",
      "acc: 0.6666667, loss:0.7022607\n",
      "acc: 0.5, loss:0.64697576\n",
      "acc: 0.625, loss:0.77429277\n",
      "acc: 0.45833334, loss:0.64061576\n",
      "acc: 0.5416667, loss:0.64602655\n",
      "acc: 0.6666667, loss:0.59300804\n",
      "acc: 0.6666667, loss:0.6171736\n",
      "acc: 0.625, loss:0.73808676\n",
      "saved to models_rnn/pretrained_lstm.ckpt-5000\n",
      "acc: 0.5416667, loss:0.62450236\n",
      "acc: 0.75, loss:0.54161626\n",
      "acc: 0.625, loss:0.6519776\n",
      "acc: 0.45833334, loss:0.70878106\n",
      "acc: 0.7916667, loss:0.56115717\n",
      "acc: 0.625, loss:0.6529688\n",
      "acc: 0.5833333, loss:0.7101783\n",
      "acc: 0.5416667, loss:0.73973894\n",
      "acc: 0.5416667, loss:0.62847996\n",
      "acc: 0.5, loss:0.6473686\n",
      "acc: 0.5416667, loss:0.5939942\n",
      "acc: 0.5833333, loss:0.5817635\n",
      "acc: 0.5, loss:0.7542533\n",
      "acc: 0.5833333, loss:0.6114137\n",
      "acc: 0.5, loss:0.6760209\n",
      "acc: 0.5833333, loss:0.61898106\n",
      "acc: 0.6666667, loss:0.6260081\n",
      "acc: 0.625, loss:0.68322736\n",
      "acc: 0.5416667, loss:0.6108449\n",
      "acc: 0.7083333, loss:0.60439104\n",
      "saved to models_rnn/pretrained_lstm.ckpt-6000\n",
      "acc: 0.625, loss:0.523726\n",
      "acc: 0.625, loss:0.68075675\n",
      "acc: 0.5833333, loss:0.56604165\n",
      "acc: 0.7083333, loss:0.48265848\n",
      "acc: 0.5416667, loss:0.58559567\n",
      "acc: 0.5833333, loss:0.5724531\n",
      "acc: 0.45833334, loss:0.6743389\n",
      "acc: 0.7083333, loss:0.5921084\n",
      "acc: 0.5833333, loss:0.5730644\n",
      "acc: 0.7083333, loss:0.6152624\n",
      "acc: 0.45833334, loss:0.6394562\n",
      "acc: 0.8333333, loss:0.5647307\n",
      "acc: 0.5833333, loss:0.70045096\n",
      "acc: 0.625, loss:0.6223249\n",
      "acc: 0.5833333, loss:0.6428996\n",
      "acc: 0.5, loss:0.6810816\n",
      "acc: 0.45833334, loss:0.55228263\n",
      "acc: 0.6666667, loss:0.59213144\n",
      "acc: 0.45833334, loss:0.71477515\n",
      "acc: 0.45833334, loss:0.6796429\n",
      "saved to models_rnn/pretrained_lstm.ckpt-7000\n",
      "acc: 0.625, loss:0.62549335\n",
      "acc: 0.7083333, loss:0.6261949\n",
      "acc: 0.6666667, loss:0.64735115\n",
      "acc: 0.5833333, loss:0.627546\n",
      "acc: 0.5, loss:0.6671874\n",
      "acc: 0.7916667, loss:0.5858235\n",
      "acc: 0.625, loss:0.6117806\n",
      "acc: 0.6666667, loss:0.58242863\n",
      "acc: 0.6666667, loss:0.65225506\n",
      "acc: 0.7083333, loss:0.6005818\n",
      "acc: 0.8333333, loss:0.5194097\n",
      "acc: 0.5833333, loss:0.6764149\n",
      "acc: 0.5, loss:0.7387803\n",
      "acc: 0.625, loss:0.5951736\n",
      "acc: 0.5, loss:0.7768778\n",
      "acc: 0.625, loss:0.7047705\n",
      "acc: 0.5, loss:0.67146844\n",
      "acc: 0.625, loss:0.61668634\n",
      "acc: 0.6666667, loss:0.59392744\n",
      "acc: 0.7083333, loss:0.73642594\n",
      "saved to models_rnn/pretrained_lstm.ckpt-8000\n",
      "acc: 0.7916667, loss:0.61880666\n",
      "acc: 0.625, loss:0.6221002\n",
      "acc: 0.75, loss:0.61744523\n",
      "acc: 0.6666667, loss:0.6638481\n",
      "acc: 0.625, loss:0.5985244\n",
      "acc: 0.5833333, loss:0.7157094\n",
      "acc: 0.5416667, loss:0.6531318\n",
      "acc: 0.45833334, loss:0.6655535\n",
      "acc: 0.45833334, loss:0.69276214\n",
      "acc: 0.6666667, loss:0.5900428\n",
      "acc: 0.7083333, loss:0.65477026\n",
      "acc: 0.75, loss:0.61570674\n",
      "acc: 0.33333334, loss:0.6309002\n",
      "acc: 0.75, loss:0.720916\n",
      "acc: 0.5833333, loss:0.6942615\n",
      "acc: 0.7083333, loss:0.71886396\n",
      "acc: 0.6666667, loss:0.5829618\n",
      "acc: 0.45833334, loss:0.68736076\n",
      "acc: 0.375, loss:0.63031304\n",
      "acc: 0.5, loss:0.6186661\n",
      "saved to models_rnn/pretrained_lstm.ckpt-9000\n",
      "acc: 0.7083333, loss:0.6168743\n",
      "acc: 0.6666667, loss:0.6304813\n",
      "acc: 0.5416667, loss:0.7485334\n",
      "acc: 0.5416667, loss:0.63116866\n",
      "acc: 0.5833333, loss:0.5919249\n",
      "acc: 0.5416667, loss:0.60793823\n",
      "acc: 0.625, loss:0.56301457\n",
      "acc: 0.5, loss:0.7137153\n",
      "acc: 0.41666666, loss:0.66641384\n",
      "acc: 0.625, loss:0.6087147\n",
      "acc: 0.5, loss:0.71883297\n",
      "acc: 0.625, loss:0.7579368\n",
      "acc: 0.625, loss:0.6645151\n",
      "acc: 0.5, loss:0.6517831\n",
      "acc: 0.5833333, loss:0.72072345\n",
      "acc: 0.5, loss:0.5650473\n",
      "acc: 0.5833333, loss:0.58394504\n",
      "acc: 0.45833334, loss:0.64041734\n",
      "acc: 0.5833333, loss:0.62202245\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "   #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "#         pred = = sess.run(prediction,{input_data: nextBatch, labels: nextBatchLabels})\n",
    "#         pred = sess.run(prediction,{input_data: nextBatch, labels: nextBatchLabels})\n",
    "#         correct = sess.run(correctPred,{input_data: nextBatch, labels: nextBatchLabels})\n",
    "        acc = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        lo = sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        print(\"acc: %s, loss:%s\" % (acc ,lo))\n",
    "#         print(summary)\n",
    "    \n",
    "   #Save the network every 10,000 training iterations\n",
    "    if (i % 1000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models_rnn/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 58.33333134651184\n",
      "Loss for this batch: 81.34996891021729\n",
      "Accuracy for this batch: 25.0\n",
      "Loss for this batch: 81.71536326408386\n",
      "Accuracy for this batch: 50.0\n",
      "Loss for this batch: 79.48489189147949\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Loss for this batch: 81.8208396434784\n",
      "Accuracy for this batch: 33.33333432674408\n",
      "Loss for this batch: 83.60758423805237\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Loss for this batch: 92.39735007286072\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Loss for this batch: 73.68962168693542\n",
      "Accuracy for this batch: 50.0\n",
      "Loss for this batch: 72.28562235832214\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Loss for this batch: 81.4687430858612\n",
      "Accuracy for this batch: 50.0\n",
      "Loss for this batch: 96.14739418029785\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Loss for this batch:\", (sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = jieba.cut(cleanedSentence)\n",
    "    \n",
    "    \n",
    "    for indexCounter,word in enumerate(split):\n",
    "        if word in stoplist:\n",
    "            continue\n",
    "        try:\n",
    "            print(word)\n",
    "            sentenceMatrix[0,indexCounter] = word_list.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 572296 #Vector for unkown\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国内\n",
      "酒店\n",
      "[[ -5.6510015  -5.958766 ]\n",
      " [-10.923809  -11.048624 ]\n",
      " [ -6.087435   -5.435936 ]\n",
      " [ -9.092943   -8.381775 ]\n",
      " [-10.959107  -10.46796  ]\n",
      " [ -6.5145383  -6.532525 ]\n",
      " [ -4.9692116  -4.4619546]\n",
      " [ -6.7310834  -5.949607 ]\n",
      " [ -8.631697   -7.8913603]\n",
      " [ -6.425059   -5.825375 ]\n",
      " [ -2.9456453  -2.3401647]\n",
      " [ -9.728808   -9.498568 ]\n",
      " [ -8.792174   -8.685799 ]\n",
      " [ -7.6330914  -7.335785 ]\n",
      " [-11.383596  -10.711954 ]\n",
      " [ -7.737132   -7.9112444]\n",
      " [ -9.216162   -8.787365 ]\n",
      " [ -2.65586    -2.3008668]\n",
      " [ -5.338141   -4.5021763]\n",
      " [ -7.2876563  -6.3981667]\n",
      " [ -9.853491  -10.079935 ]\n",
      " [ -5.2526484  -4.7884493]\n",
      " [ -4.542602   -4.538004 ]\n",
      " [ -8.063134   -7.933921 ]]\n",
      "Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "inputText = \"非常好。国内最好的酒店之一。各方面都可以。\"\n",
    "inputMatrix = getSentenceMatrix(inputText)\n",
    "\n",
    "\n",
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})\n",
    "print(predictedSentiment)\n",
    "if (predictedSentiment[0][0] > predictedSentiment[0][1]):\n",
    "    print(\"Positive Sentiment\")\n",
    "else:\n",
    "    print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
