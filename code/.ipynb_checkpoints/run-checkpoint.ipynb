{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = {line.strip() for line in open(filepath, \"r\", encoding='utf-8').readlines()}\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def cleanSentences(string):\n",
    "    return re.sub(\"[a-zA-Z0-9\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！<>《》，。？、~@#￥%……&*（） ]+\", \"\",string)\n",
    "\n",
    "\n",
    "program = os.path.basename(\"deep learning\")\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "# positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "# negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "\n",
    "# files = positiveFiles + negativeFiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [\"corpus.txt\"]\n",
    "\n",
    "# vec = []\n",
    "\n",
    "\n",
    "# # logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "# i = 0\n",
    "# for file in files:\n",
    "#     with open(file, \"r\",encoding='utf-8') as f:\n",
    "#         if os.path.exists(\"corpus_segment.txt\"):\n",
    "#             continue\n",
    "        \n",
    "#         fp = codecs.open(\"corpus_segment.txt\", \"a+\", encoding=\"utf-8\")\n",
    "        \n",
    "#         for line in f:\n",
    "#             line = cleanSentences(line)\n",
    "#             if line.strip() is \"\n",
    "#                 continue\n",
    "            \n",
    "#             sentence_seged = jieba.cut(line.strip())\n",
    "# #             print(type(sentence_seged))\n",
    "# #             tmp = []\n",
    "# #             for word in sentence_seged:\n",
    "# #                 if word.strip() not in stoplist:\n",
    "# #                     tmp.append(word)\n",
    "# #             vec.append(tmp)\n",
    "# #             tmp.append(word)\n",
    "#             fp.write(\" \".join(sentence_seged))\n",
    "            \n",
    "#             i += 1\n",
    "#             if i % 1000 == 0:\n",
    "#                 logger.info(\"Saved \"+ str(i) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(i) + \" articles\")\n",
    "# # print(vec)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-24 15:58:39,954: INFO: 'pattern' package not found; tag filters are not available for English\n",
      "2018-04-24 15:58:39,964: INFO: loading Word2Vec object from model_word2vec\n",
      "2018-04-24 15:58:41,885: INFO: loading wv recursively from model_word2vec.wv.* with mmap=None\n",
      "2018-04-24 15:58:41,886: INFO: loading vectors from model_word2vec.wv.vectors.npy with mmap=None\n",
      "2018-04-24 15:58:43,109: INFO: setting ignored attribute vectors_norm to None\n",
      "2018-04-24 15:58:43,112: INFO: loading vocabulary recursively from model_word2vec.vocabulary.* with mmap=None\n",
      "2018-04-24 15:58:43,113: INFO: loading trainables recursively from model_word2vec.trainables.* with mmap=None\n",
      "2018-04-24 15:58:43,114: INFO: loading syn1neg from model_word2vec.trainables.syn1neg.npy with mmap=None\n",
      "2018-04-24 15:58:44,846: INFO: setting ignored attribute cum_table to None\n",
      "2018-04-24 15:58:44,849: INFO: loaded model_word2vec\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "似乎不需要去除停用词\n",
    "'''\n",
    "# stoplist = stopwordslist(\"stopwords.txt\")\n",
    "# fout = \"corpus_seg_without_stop.txt\"\n",
    "# cnt = 0\n",
    "# with codecs.open(\"corpus_segment.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         tmp = []\n",
    "#         for word in line.split():\n",
    "#             if word.strip() in stoplist:\n",
    "#                continue\n",
    "#             tmp.append(word)\n",
    "#         fout.write(\" \".join(tmp))\n",
    "#         cnt += 1\n",
    "#         if cnt % 1000 == 0:\n",
    "#             logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# input = \"corpus_segment.txt\"\n",
    "# model = Word2Vec(LineSentence(input), size=400, window=5,min_count=5,workers=multiprocessing.cpu_count())\n",
    "# model.save('model_word2vec')\n",
    "# model.wv.save_word2vec_format(\"model_saved_format_version\", binary=False)  \n",
    "\n",
    "model = Word2Vec.load(\"model_word2vec\")\n",
    "# for mo in model.wv.vocab:\n",
    "#     print(mo, model.wv[mo])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w2v_list = np.array([model.wv[word] for word in model.wv.vocab])\n",
    "word_list = np.array([word for word in model.wv.vocab]).tolist()\n",
    "# print(w2v_np)\n",
    "\n",
    "\n",
    "# print(model[u'n'])\n",
    "# print(model.wv[u'机会'])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-24 15:58:51,524: INFO: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572297, 400)\n",
      "[('婚姻', 0.6089010834693909), ('情感', 0.6044034957885742), ('感情', 0.5982906818389893), ('恋爱', 0.5788689851760864), ('爱恋', 0.5739886164665222), ('情爱', 0.573617696762085), ('真爱', 0.5732499361038208), ('人生', 0.567855715751648), ('爱情故事', 0.5486623644828796), ('情感故事', 0.5435651540756226)]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_list.shape)\n",
    "print(model.wv.most_similar('爱情'))\n",
    "# print(model.wv.similarity('爱情','宾馆'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-24 15:59:00,866: INFO: Saved 0 articles\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwordslist(\"stopwords.txt\")\n",
    "\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "\n",
    "seg_positiveFiles = ['seged_positiveReviews/' + f for f in listdir('seged_positiveReviews/') if isfile(join('seged_positiveReviews/', f))]\n",
    "seg_negativeFiles = ['seged_negativeReviews/' + f for f in listdir('seged_negativeReviews/') if isfile(join('seged_negativeReviews/', f))]\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "if os.path.exists(\"seged_positiveReviews\"):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(\"seged_positiveReviews\")\n",
    "\n",
    "if os.path.exists(\"seged_negativeReviews\"):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(\"seged_negativeReviews\")\n",
    "    \n",
    "for pf in positiveFiles:\n",
    "    positive_seged = \"seged_\"+pf\n",
    "    \n",
    "    if os.path.exists(positive_seged):\n",
    "        continue\n",
    "        \n",
    "    with open(pf, \"r\") as f:\n",
    "        file = []\n",
    "        for line in f:\n",
    "            line = cleanSentences(line).strip()\n",
    "            sentence_seged = jieba.cut(line)\n",
    "            \n",
    "            temp = []\n",
    "            for word in sentence_seged:\n",
    "                if word in stoplist:\n",
    "                    continue;\n",
    "                temp.append(word)\n",
    "            file.append(\" \".join(temp))\n",
    "        with open(positive_seged, \"w\",encoding='utf-8') as of:\n",
    "            of.write(\"\\n\".join(file))\n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "for pf in negativeFiles:\n",
    "    negative_seged = \"seged_\"+pf\n",
    "    \n",
    "    if os.path.exists(negative_seged):\n",
    "        continue\n",
    "    \n",
    "    with open(pf, \"r\") as f:\n",
    "        file = []\n",
    "        for line in f:\n",
    "            line = cleanSentences(line).strip()\n",
    "            sentence_seged = jieba.cut(line)\n",
    "            \n",
    "            temp = []\n",
    "            for word in sentence_seged:\n",
    "                if word in stoplist:\n",
    "                    continue;\n",
    "                temp.append(word)\n",
    "            file.append(\" \".join(temp))\n",
    "        with open(negative_seged, \"w\",encoding='utf-8') as of:\n",
    "            of.write(\"\\n\".join(file))\n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "logger.info(\"Saved \"+ str(cnt) + \" articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 4000\n",
      "The total number of words in the files is 129414\n",
      "The average number of words in the files is 32.3535\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "numWords = []\n",
    "for pf in seg_positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(\"\".join(line.split()))\n",
    "#         print(line.split(), len(line.split()))\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in seg_negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 200, 0, 2500])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "maxSeqLength = 50\n",
    "numDimensions = 300\n",
    "# firstSentense = np.zeros((maxSeqLength), dtype='int32')\n",
    "# firstSentense[0] = word_list.index('宾馆')\n",
    "# firstSentense[1] = word_list.index('喜欢')\n",
    "# firstSentense[2] = word_list.index('爱')\n",
    "# print(firstSentense.shape)\n",
    "# print(firstSentense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "\n",
    "for pf in seg_positiveFiles:\n",
    "    with open(pf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "#         print(line.split())\n",
    "        \n",
    "        for word in line.split():\n",
    "            try:\n",
    "#                 print(word, word_list.index(word))\n",
    "                ids[fileCounter][indexCounter] = word_list.index(word)\n",
    "            except ValueError:\n",
    "#                 print(word)\n",
    "                ids[fileCounter][indexCounter] = 572296 #Vector for unkown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        fileCounter = fileCounter + 1\n",
    "\n",
    "for nf in seg_negativeFiles:\n",
    "    with open(nf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        for word in line.split():\n",
    "            try:\n",
    "#                 print(word, word_list.index(word))\n",
    "                ids[fileCounter][indexCounter] = word_list.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 572296 #Vector for unkown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        fileCounter = fileCounter + 1 \n",
    "\n",
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "np.save('idsMatrix', ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15636  4671  4932  4750  6089   480  8474  8472  6322 35935 34064   331\n",
      " 33169 49905 14319   797     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(ids[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x11f798be0>\n"
     ]
    }
   ],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 1000\n",
    "# try 10000 iteration\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "print(labels.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "将向量转换成嵌入矩阵的过程\n",
    "'''\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_9:0\", shape=(50, 64), dtype=float32)\n",
      "(24, 50, 64)\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "print(value[0])\n",
    "print(value.get_shape())\n",
    "print(value.get_shape()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "# value = tf.transpose(value, [1, 0, 2])\n",
    "# last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "last = tf.unstack(data, maxSeqLength, 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(last))\n",
    "#     print(last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "Tensor(\"Gather_4:0\", shape=(24, 64), dtype=float32)\n",
      "<tf.Variable 'Variable_9:0' shape=(64, 2) dtype=float32_ref>\n",
      "Tensor(\"strided_slice_11:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(int(value.get_shape()[0]) - 1)\n",
    "print(last)\n",
    "print(weight)\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.argmax(prediction, 1))\n",
    "#     print(tf.argmax(prediction, 1))\n",
    "#     print(tf.argmax(labels, 1))\n",
    "\n",
    "\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,0), tf.argmax(labels,0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,1800)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(2200,3999)\n",
    "            labels.append([0,1])\n",
    "#         print(i, ids[num-1])\n",
    "        arr[i] = ids[num-1]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1801,2199)\n",
    "        if (num <= 2000):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "import datetime\n",
    "\n",
    "# tf.summary.scalar('Loss', loss)\n",
    "# tf.summary.scalar('Accuracy', accuracy)\n",
    "# merged = tf.summary.merge_all()\n",
    "# logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "# writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "   #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "#         summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "#         writer.add_summary(summary, i)\n",
    "#         pred = = sess.run(prediction,{input_data: nextBatch, labels: nextBatchLabels})\n",
    "        pred = sess.run(prediction,{input_data: nextBatch, labels: nextBatchLabels})\n",
    "        correct = sess.run(correctPred,{input_data: nextBatch, labels: nextBatchLabels})\n",
    "        acc = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        lo = sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        print(\"pred: %s, correctPred: %s, acc: %s, loss:%s\" % (pred, correct, acc ,lo))\n",
    "#         print(summary)\n",
    "    \n",
    "   #Save the network every 10,000 training iterations\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = word_list.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 572296 #Vector for unkown\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"这部电影非常好.\"\n",
    "inputMatrix = getSentenceMatrix(inputText)\n",
    "\n",
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print(\"Positive Sentiment\")\n",
    "else:\n",
    "    print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
