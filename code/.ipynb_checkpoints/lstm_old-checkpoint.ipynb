{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = {line.strip() for line in open(filepath, \"r\", encoding='utf-8').readlines()}\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def cleanSentences(string):\n",
    "    return re.sub(\"[a-zA-Z0-9\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！<>《》，。？、~@#￥%……&*（） ]+\", \"\",string)\n",
    "\n",
    "\n",
    "program = os.path.basename(\"deep learning\")\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "# positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "# negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "\n",
    "# files = positiveFiles + negativeFiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [\"corpus.txt\"]\n",
    "\n",
    "# vec = []\n",
    "\n",
    "\n",
    "# # logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "# i = 0\n",
    "# for file in files:\n",
    "#     with open(file, \"r\",encoding='utf-8') as f:\n",
    "#         if os.path.exists(\"corpus_segment.txt\"):\n",
    "#             continue\n",
    "        \n",
    "#         fp = codecs.open(\"corpus_segment.txt\", \"a+\", encoding=\"utf-8\")\n",
    "        \n",
    "#         for line in f:\n",
    "#             line = cleanSentences(line)\n",
    "#             if line.strip() is \"\n",
    "#                 continue\n",
    "            \n",
    "#             sentence_seged = jieba.cut(line.strip())\n",
    "# #             print(type(sentence_seged))\n",
    "# #             tmp = []\n",
    "# #             for word in sentence_seged:\n",
    "# #                 if word.strip() not in stoplist:\n",
    "# #                     tmp.append(word)\n",
    "# #             vec.append(tmp)\n",
    "# #             tmp.append(word)\n",
    "#             fp.write(\" \".join(sentence_seged))\n",
    "            \n",
    "#             i += 1\n",
    "#             if i % 1000 == 0:\n",
    "#                 logger.info(\"Saved \"+ str(i) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(i) + \" articles\")\n",
    "# # print(vec)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-11 04:00:18,724: INFO: 'pattern' package not found; tag filters are not available for English\n",
      "2018-06-11 04:00:18,780: INFO: loading Word2Vec object from model_word2vec\n",
      "2018-06-11 04:00:23,922: INFO: loading wv recursively from model_word2vec.wv.* with mmap=None\n",
      "2018-06-11 04:00:23,923: INFO: loading vectors from model_word2vec.wv.vectors.npy with mmap=None\n",
      "2018-06-11 04:01:20,234: INFO: setting ignored attribute vectors_norm to None\n",
      "2018-06-11 04:01:20,236: INFO: loading vocabulary recursively from model_word2vec.vocabulary.* with mmap=None\n",
      "2018-06-11 04:01:20,245: INFO: loading trainables recursively from model_word2vec.trainables.* with mmap=None\n",
      "2018-06-11 04:01:20,247: INFO: loading syn1neg from model_word2vec.trainables.syn1neg.npy with mmap=None\n",
      "2018-06-11 04:02:15,755: INFO: setting ignored attribute cum_table to None\n",
      "2018-06-11 04:02:15,757: INFO: loaded model_word2vec\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "似乎不需要去除停用词\n",
    "'''\n",
    "# stoplist = stopwordslist(\"stopwords.txt\")\n",
    "# fout = \"corpus_seg_without_stop.txt\"\n",
    "# cnt = 0\n",
    "# with codecs.open(\"corpus_segment.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         tmp = []\n",
    "#         for word in line.split():\n",
    "#             if word.strip() in stoplist:\n",
    "#                continue\n",
    "#             tmp.append(word)\n",
    "#         fout.write(\" \".join(tmp))\n",
    "#         cnt += 1\n",
    "#         if cnt % 1000 == 0:\n",
    "#             logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# input = \"corpus_segment.txt\"\n",
    "# model = Word2Vec(LineSentence(input), size=400, window=5,min_count=5,workers=multiprocessing.cpu_count())\n",
    "# model.save('model_word2vec')\n",
    "# model.wv.save_word2vec_format(\"model_saved_format_version\", binary=False)  \n",
    "\n",
    "\n",
    "model = Word2Vec.load(\"model_word2vec\")\n",
    "# for mo in model.wv.vocab:\n",
    "#     print(mo, model.wv[mo])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w2v_list = np.array([model.wv[word] for word in model.wv.vocab])\n",
    "word_list = np.array([word for word in model.wv.vocab]).tolist()\n",
    "# print(w2v_np)\n",
    "\n",
    "\n",
    "# print(model[u'n'])\n",
    "# print(model.wv[u'机会'])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w2v_list.shape)\n",
    "# print(model.wv.most_similar('爱情'))\n",
    "# print(model.wv.similarity('爱情','宾馆'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-11 04:02:22,047: INFO: Saved 0 articles\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwordslist(\"stopwords.txt\")\n",
    "\n",
    "# positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "# negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "\n",
    "# seg_positiveFiles = ['seged_positiveReviews/' + f for f in listdir('seged_positiveReviews/') if isfile(join('seged_positiveReviews/', f))]\n",
    "# seg_negativeFiles = ['seged_negativeReviews/' + f for f in listdir('seged_negativeReviews/') if isfile(join('seged_negativeReviews/', f))]\n",
    "\n",
    "positiveFiles = ['data_new/B/Pos-train.txt','data_new/B/Pos-test.txt']\n",
    "negativeFiles = ['data_new/B/Neg-train.txt','data_new/B/Neg-test.txt']\n",
    "seg_positiveFiles = ['seged_data_new/B/Pos-train.txt','seged_data_new/B/Pos-test.txt']\n",
    "seg_negativeFiles = ['seged_data_new/B/Neg-train.txt','seged_data_new/B/Neg-test.txt']\n",
    "\n",
    "# pos_test_files = []\n",
    "# neg_test_files = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "if os.path.exists(\"seged_data_new\"):\n",
    "    if os.path.exists(\"B\"):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(\"B\")\n",
    "else:\n",
    "    os.mkdir(\"seged_data_new\")\n",
    "    \n",
    "for pf in positiveFiles:\n",
    "    positive_seged = \"seged_\"+pf\n",
    "    \n",
    "    if os.path.exists(positive_seged):\n",
    "        continue\n",
    "        \n",
    "    with open(pf, \"r\") as f:\n",
    "#         file = []\n",
    "        for line in f:\n",
    "            line = cleanSentences(line).strip()\n",
    "            sentence_seged = jieba.cut(line)\n",
    "            \n",
    "            temp = []\n",
    "            for word in sentence_seged:\n",
    "                if word in stoplist:\n",
    "                    continue;\n",
    "                temp.append(word)\n",
    "#             file.append(\" \".join(temp))\n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "            with open(positive_seged, \"w\",encoding='utf-8') as of:\n",
    "                of.write(\" \".join(temp))\n",
    "                os.write(\"\\n\")\n",
    "            \n",
    "\n",
    "for pf in negativeFiles:\n",
    "    negative_seged = \"seged_\"+pf\n",
    "    \n",
    "    if os.path.exists(negative_seged):\n",
    "        continue\n",
    "    \n",
    "    with open(pf, \"r\") as f:\n",
    "#         file = []\n",
    "        for line in f:\n",
    "            line = cleanSentences(line).strip()\n",
    "            sentence_seged = jieba.cut(line)\n",
    "            \n",
    "            temp = []\n",
    "            for word in sentence_seged:\n",
    "                if word in stoplist:\n",
    "                    continue;\n",
    "                temp.append(word)\n",
    "#             file.append(\" \".join(temp))\n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "            with open(negative_seged, \"w\",encoding='utf-8') as of:\n",
    "                of.write(\" \".join(temp))\n",
    "                os.write(\"\\n\")\n",
    "\n",
    "logger.info(\"Saved \"+ str(cnt) + \" articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "# numWords = []\n",
    "# for pf in seg_positiveFiles:\n",
    "#     with open(pf, \"r\", encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "# #         line=f.readline()\n",
    "#             counter = len(\"\".join(line.split()))\n",
    "# #         print(line.split(), len(line.split()))\n",
    "#             numWords.append(counter)       \n",
    "# print('Positive files finished')\n",
    "\n",
    "# for nf in seg_negativeFiles:\n",
    "#     with open(nf, \"r\", encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#         #         line=f.readline()\n",
    "#             counter = len(line.split())\n",
    "#             numWords.append(counter)  \n",
    "# print('Negative files finished')\n",
    "\n",
    "# numFiles = len(numWords)\n",
    "# print('The total number of files is', numFiles)\n",
    "# print('The total number of words in the files is', sum(numWords))\n",
    "# print('The average number of words in the files is', sum(numWords)/len(numWords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.hist(numWords, 50)\n",
    "# plt.xlabel('Sequence Length')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.axis([0, 200, 0, 10000])\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "maxSeqLength = 50\n",
    "numDimensions = 400\n",
    "# firstSentense = np.zeros((maxSeqLength), dtype='int32')\n",
    "# firstSentense[0] = word_list.index('宾馆')\n",
    "# firstSentense[1] = word_list.index('喜欢')\n",
    "# firstSentense[2] = word_list.index('爱')\n",
    "# print(firstSentense.shape)\n",
    "# print(firstSentense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "# fileCounter = 0\n",
    "# steps = []\n",
    "\n",
    "# for pf in seg_positiveFiles:\n",
    "#     with open(pf, \"r\") as f:\n",
    "#         for line in f:\n",
    "#         #         line=f.readline()\n",
    "# #         print(line.split())\n",
    "#             indexCounter = 0\n",
    "            \n",
    "#             for word in line.split():\n",
    "#                 try:\n",
    "# #                 print(word, word_list.index(word))\n",
    "#                     ids[fileCounter][indexCounter] = word_list.index(word)\n",
    "#                 except ValueError:\n",
    "# #                 print(word)\n",
    "#                     ids[fileCounter][indexCounter] = 572296 #Vector for unkown words\n",
    "#                 indexCounter = indexCounter + 1\n",
    "#                 if indexCounter >= maxSeqLength:\n",
    "#                     break\n",
    "#             steps.append(indexCounter)\n",
    "#             fileCounter = fileCounter + 1\n",
    "\n",
    "# for nf in seg_negativeFiles:\n",
    "#     with open(nf, \"r\") as f:\n",
    "        \n",
    "#         for line in f:\n",
    "#         #         line=f.readline()\n",
    "#             indexCounter = 0\n",
    "#             for word in line.split():\n",
    "#                 try:\n",
    "# #                 print(word, word_list.index(word))\n",
    "#                     ids[fileCounter][indexCounter] = word_list.index(word)\n",
    "#                 except ValueError:\n",
    "#                     ids[fileCounter][indexCounter] = 572296 #Vector for unkown words\n",
    "#                 indexCounter = indexCounter + 1\n",
    "#                 if indexCounter >= maxSeqLength:\n",
    "#                     break\n",
    "#             steps.append(indexCounter)\n",
    "#             fileCounter = fileCounter + 1 \n",
    "\n",
    "# #Pass into embedding function and see if it evaluates. \n",
    "\n",
    "# np.save('idsMatrix', ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ids[6])\n",
    "from random import randint, shuffle\n",
    "\n",
    "\n",
    "def getWords(file):\n",
    "#     files = [filePath + f for f in listdir(filePath) if isfile(join(filePath, f))]\n",
    "# #     print(files)\n",
    "#     lineList = []\n",
    "#     for nf in files:\n",
    "        \n",
    "#         with open(nf, \"r\") as f:\n",
    "#             indexCounter = 0\n",
    "# #             print(f)\n",
    "#             line=f.readline()\n",
    "# #             print(line)\n",
    "#             wordList = []\n",
    "#             for word in line.split():\n",
    "#                 wordList.append(word)\n",
    "#         lineList.append(wordList)\n",
    "#     return lineList\n",
    "    content = []\n",
    "    cnt = 0\n",
    "    with codecs.open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = cleanSentences(line).strip()\n",
    "            sentence_seged = jieba.cut(line)\n",
    "            \n",
    "            temp = []\n",
    "            for word in sentence_seged:\n",
    "                if word in stoplist:\n",
    "                    continue;\n",
    "                temp.append(word)\n",
    "            content.append(\" \".join(temp))\n",
    "        \n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "    logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "    if os.path.exists(\"seged_\"+file):\n",
    "        return content\n",
    "    \n",
    "    with open(\"seged_\"+file, \"w\",encoding='utf-8') as of:\n",
    "        print(\"seged_\"+file)\n",
    "        of.write(\"\\n\".join(content))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2Array(lineList):\n",
    "    numFiles = len(lineList)\n",
    "    linesArray=np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "    \n",
    "    steps = []\n",
    "    fileCounter = 0\n",
    "    for line in lineList:\n",
    "        p = 0\n",
    "        \n",
    "        for word in line.split():\n",
    "#                 wordsArray.append(model.wv.word_vec(line[i]))\n",
    "            try:\n",
    "                linesArray[fileCounter][p] = word_list.index(word)\n",
    "            except ValueError:\n",
    "                linesArray[fileCounter][p] = 572296\n",
    "            p = p + 1\n",
    "            if p >= maxSeqLength:\n",
    "                break\n",
    "        steps.append(p)\n",
    "#         linesArray.append(wordsArray)\n",
    "        fileCounter += 1\n",
    "#     linesArray = np.array(linesArray)\n",
    "#     steps = np.array(steps)\n",
    "    print(\"words2Array succeed\")\n",
    "    return linesArray, steps\n",
    "\n",
    "def convert2Data(posArray, negArray, posStep, negStep):\n",
    "    randIt = []\n",
    "    data = []\n",
    "    steps = []\n",
    "    labels = []\n",
    "    for i in range(len(posArray)):\n",
    "        randIt.append([posArray[i], posStep[i], [1,0]])\n",
    "    for i in range(len(negArray)):\n",
    "        randIt.append([negArray[i], negStep[i], [0,1]])\n",
    "    shuffle(randIt)\n",
    "    for i in range(len(randIt)):\n",
    "        data.append(randIt[i][0])\n",
    "        steps.append(randIt[i][1])\n",
    "        labels.append(randIt[i][2])\n",
    "#     print(len(data))\n",
    "#     data = np.array(data)\n",
    "#     steps = np.array(steps)\n",
    "    return data, steps, labels\n",
    "\n",
    "def makeData(posPath,negPath):\n",
    "    #获取词汇，返回类型为[[word1,word2...],[word1,word2...],...]\n",
    "    pos = getWords(posPath)\n",
    "    print(\"The positive data's length is :\",len(pos))\n",
    "    neg = getWords(negPath)\n",
    "    print(\"The negative data's length is :\",len(neg))\n",
    "    #将评价数据转换为矩阵，返回类型为array\n",
    "    posArray, posSteps = words2Array(pos)\n",
    "    negArray, negSteps = words2Array(neg)\n",
    "    #将积极数据和消极数据混合在一起打乱，制作数据集\n",
    "#     Data, Steps, Labels = convert2Data(posArray, negArray, posSteps, negSteps)\n",
    "    return posArray, negArray, posSteps, negSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeed to load numpy data\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "if os.path.exists(\"np_data\"):\n",
    "    trainData = np.load(\"np_data/trainData.npy\")\n",
    "    trainSteps = np.load(\"np_data/trainSteps.npy\")\n",
    "    trainLabels = np.load(\"np_data/trainLabels.npy\")\n",
    "    testData = np.load(\"np_data/testData.npy\")\n",
    "    testSteps = np.load(\"np_data/testSteps.npy\")\n",
    "    testLabels = np.load(\"np_data/testLabels.npy\")\n",
    "    print(\"Succeed to load numpy data\")\n",
    "    \n",
    "    \n",
    "#     print(tempData.shape)\n",
    "\n",
    "   \n",
    "    \n",
    "else: \n",
    "\n",
    "    train_posArray, train_negArray, train_posSteps, train_negSteps = makeData('data_new/B/Pos-train.txt', 'data_new/B/Neg-train.txt')\n",
    "\n",
    "#     print(\"In test data:\")\n",
    "#     test_posArray, test_negArray, test_posSteps, test_negSteps = makeData('data_new/B/Pos-test.txt', 'data_new/B/Neg-test.txt')\n",
    "\n",
    "# del model\n",
    "#     tempData_pos = np.concatenate((train_posArray, test_posArray), axis=0)\n",
    "    train_posLabels = np.array([[1,0] for i in range(len(train_posArray))])\n",
    "    \n",
    "    train_posArray, test_posArray, train_posLabels, test_posLabels, train_posSteps, test_posSteps = train_test_split(train_posArray, train_posLabels, train_posSteps, test_size=0.1)\n",
    "\n",
    "    train_negLabels = np.array([[0,1] for i in range(len(train_negArray))])\n",
    "    \n",
    "    train_negArray, test_negArray, train_negLabels, test_negLabels, train_negSteps, test_negSteps = train_test_split(train_negArray, train_negLabels, train_negSteps, test_size=0.1)\n",
    "    \n",
    "    trainData = np.concatenate((train_posArray, train_negArray),axis=0)\n",
    "    trainLabels = np.concatenate((train_posLabels, train_negLabels), axis=0)\n",
    "    testData = np.concatenate((test_posArray, test_negArray),axis=0)\n",
    "    testLabels = np.concatenate((test_posLabels, test_negLabels),axis=0)\n",
    "    trainSteps = np.concatenate((train_posSteps, train_negSteps), axis=0)\n",
    "    testSteps = np.concatenate((test_posSteps, test_negSteps), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    trainSteps = np.ndarray.flatten(trainSteps)\n",
    "    trainSteps = trainSteps[:, np.newaxis]\n",
    "    train_temp = np.hstack((trainData, trainLabels, trainSteps))\n",
    "    \n",
    "# print(train_temp[1])\n",
    "    np.random.shuffle(train_temp)\n",
    "\n",
    "# trainData = train_temp[:,:-1]\n",
    "# trainLabels = train_temp[:,:,-1]\n",
    "# trainSteps = \n",
    "    trainData, trainLabels, trainSteps = train_temp[:,0:50], train_temp[:,50:52], train_temp[:,52:]\n",
    "    trainSteps = np.ndarray.flatten(trainSteps)\n",
    "    \n",
    "#     trainData, testData, trainLabels, testLabels = train_test_split(tempData, tempLabel, test_size=0.1, random_state=0)\n",
    "\n",
    "#     trainData, trainSteps, trainLabels = convert2Data(train_posArray, train_negArray, train_posSteps, train_negSteps)\n",
    "\n",
    "#     testData, testSteps, testLabels = convert2Data(test_posArray, test_negArray, test_posSteps, test_negSteps)\n",
    "# print(\"In train data:\")\n",
    "# trainData, trainSteps, trainLabels = makeData('data_new/B/Pos-train.txt',\n",
    "#                                               'data_new/B/Neg-train.txt')\n",
    "# print(\"In test data:\")\n",
    "# testData, testSteps, testLabels = makeData('data_new/B/Pos-test.txt',\n",
    "#                                            'data_new/B/Neg-test.txt')\n",
    "# trainLabels = np.array(trainLabels)\n",
    "\n",
    "\n",
    "# del model\n",
    "\n",
    "# print(\"-\"*30)\n",
    "# print(\"The trainData's shape is:\",trainData.shape)\n",
    "# print(\"The testData's shape is:\",testData.shape)\n",
    "# print(\"The trainSteps's shape is:\",trainSteps.shape)\n",
    "# print(\"The testSteps's shape is:\",testSteps.shape)\n",
    "# print(\"The trainLabels's shape is:\",trainLabels.shape)\n",
    "# print(\"The testLabels's shape is:\",np.array(testLabels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15616,)\n",
      "(15616, 50)\n",
      "(15616, 2)\n",
      "(2,)\n",
      "()\n",
      "(15616, 50)\n",
      "(15616, 2)\n",
      "(15616,)\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# print(type(train_posArray))\n",
    "# print(train_posArray.shape)\n",
    "# print(type(train_negArray))\n",
    "print(trainSteps.shape)\n",
    "print(trainData.shape)\n",
    "print(trainLabels.shape)\n",
    "print(trainLabels[1].shape)\n",
    "print(trainSteps[2].shape)\n",
    "    \n",
    "    \n",
    "\n",
    "print(trainData.shape)\n",
    "print(trainLabels.shape)\n",
    "print(trainSteps.shape)\n",
    "print(trainLabels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6810 167791  16231   3097  11285   1066    307   6607  16941 336400\n",
      "   4750  14353  33935   1458   1688   2585 300419  69143   5814  21022\n",
      " 134741  17567  41291  26307  16283 572296   5814  12031    318      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "trainData = np.array(trainData)\n",
    "np.save(\"np_data/trainData.npy\", trainData)\n",
    "# del trainData\n",
    "\n",
    "trainLabels = np.array(trainLabels)\n",
    "np.save(\"np_data/trainLabels.npy\", trainLabels)\n",
    "testData = np.array(testData)\n",
    "np.save(\"np_data/testData.npy\", testData)\n",
    "\n",
    "testLabels = np.array(testLabels)\n",
    "np.save(\"np_data/testLabels.npy\", testLabels)\n",
    "print(testData[20])\n",
    "\n",
    "\n",
    "trainSteps = np.array(trainSteps)\n",
    "np.save(\"np_data/trainSteps.npy\", trainSteps)\n",
    "testSteps = np.array(testSteps)\n",
    "np.save(\"np_data/testSteps.npy\", testSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1737, 50)\n",
      "(15616, 50)\n",
      "(15616, 2)\n",
      "(1737, 2)\n",
      "(15616,)\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(testData.shape)\n",
    "print(trainData.shape)\n",
    "print(trainLabels.shape)\n",
    "print(testLabels.shape)\n",
    "print(trainSteps.shape)\n",
    "print(trainSteps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    \"\"\"\n",
    "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
    "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
    "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
    "    Variables notation is also inherited from the article\n",
    "    \n",
    "    Args:\n",
    "        inputs: The Attention inputs.\n",
    "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
    "                In case of RNN, this must be RNN outputs `Tensor`:\n",
    "                    If time_major == False (default), this must be a tensor of shape:\n",
    "                        `[batch_size, max_time, cell.output_size]`.\n",
    "                    If time_major == True, this must be a tensor of shape:\n",
    "                        `[max_time, batch_size, cell.output_size]`.\n",
    "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
    "                the backward RNN outputs `Tensor`.\n",
    "                    If time_major == False (default),\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
    "                    If time_major == True,\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
    "        attention_size: Linear size of the Attention weights.\n",
    "        time_major: The shape format of the `inputs` Tensors.\n",
    "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "            Using `time_major = True` is a bit more efficient because it avoids\n",
    "            transposes at the beginning and end of the RNN calculation.  However,\n",
    "            most TensorFlow data is batch-major, so by default this function\n",
    "            accepts input and emits output in batch-major form.\n",
    "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
    "            Used for visualization purpose.\n",
    "    Returns:\n",
    "        The Attention output `Tensor`.\n",
    "        In case of RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell.output_size]`.\n",
    "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas\n",
    "    \n",
    "from multihead import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "testSize = 1737\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 10001\n",
    "ATTENTION_SIZE = 50\n",
    "\n",
    "# try 10000 iteration\n",
    "\n",
    "# Bi-LSTM\n",
    "# import tensorflow as tf\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "\n",
    "#     labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "#     input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "#     steps = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "#     data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "# #     input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "# #     lstmCell = tf.contrib.rnn.BasicRNNCell(lstmUnits, activation=tf.nn.relu)\n",
    "\n",
    "#     lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "#     lstmCell_bk = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "#     lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "\n",
    "#     weight = tf.Variable(tf.truncated_normal([lstmUnits*2, numClasses]))\n",
    "#     bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    \n",
    "#     def model(put, steps):\n",
    "#         value, _ = tf.nn.bidirectional_dynamic_rnn(lstmCell, lstmCell_bk, put  ,dtype=tf.float32)\n",
    "        \n",
    "#         value = tf.concat(value, 2)\n",
    "# #         value, _ = tf.nn.dynamic_rnn(lstmCell, put, dtype=tf.float32, sequence_length=steps)\n",
    "# #         value, _ = tf.contrib.rnn.static_rnn(lstmCell, put, dtype=tf.float32)\n",
    "#         value = tf.transpose(value, [1, 0, 2])\n",
    "# # last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "#         last = value[-1]\n",
    "\n",
    "#         prediction = (tf.matmul(last, weight) + bias)\n",
    "#         return prediction\n",
    "    \n",
    "#     train_logits = model(data, steps)\n",
    "#     train_correctPred = tf.equal(tf.argmax(train_logits,1), tf.argmax(labels,1))\n",
    "#     train_accuracy = tf.reduce_mean(tf.cast(train_correctPred, tf.float32))\n",
    "    \n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=train_logits, labels=labels))\n",
    "\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# #     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \n",
    "#     tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    \n",
    "    \n",
    "#     input_test_data = tf.placeholder(tf.int32, [testSize, maxSeqLength])\n",
    "# #     input_test_data = tf.constant(input_test_data, tf.float32)\n",
    "#     test_labels = tf.placeholder(tf.float32, [testSize, numClasses])\n",
    "#     test_steps = tf.placeholder(tf.int32, [testSize])\n",
    "\n",
    "#     te_data = tf.Variable(tf.zeros([testSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     te_data = tf.nn.embedding_lookup(w2v_list,input_test_data)\n",
    "\n",
    "#     predictions = model(te_data, test_steps)\n",
    "#     prediction = tf.argmax(predictions, 1)\n",
    "#     test_label = tf.argmax(test_labels,1)\n",
    "#     test_loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=test_labels))\n",
    "#     correctPred = tf.equal(prediction, test_label)\n",
    "# # correctPred = tf.equal()\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "#     # calculate f1, recall\n",
    "#     ones_like_actuals = tf.ones_like(test_label)\n",
    "#     zeros_like_actuals = tf.zeros_like(test_label)\n",
    "#     ones_like_predictions = tf.ones_like(prediction)\n",
    "#     zeros_like_predictions = tf.zeros_like(prediction)\n",
    "    \n",
    "#     tp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     tn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "\n",
    "#     labels = tf.placeholder(tf.float32, [None, numClasses])\n",
    "#     input_data = tf.placeholder(tf.int32, [None, maxSeqLength])\n",
    "# #     steps = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# #     data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "# #     input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "#     keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')\n",
    "\n",
    "\n",
    "# #     lstmCell = tf.contrib.rnn.BasicRNNCell(lstmUnits, activation=tf.nn.relu)\n",
    "#     def get_a_cell():\n",
    "#         lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "# #     lstmCell_bk = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "#         lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "#         return lstmCell\n",
    "    \n",
    "#     mlstmCell = tf.contrib.rnn.MultiRNNCell([get_a_cell() for i in range(1)])\n",
    "    \n",
    "#     weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "#     bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    \n",
    "#     def model(put):\n",
    "# #         value, _ = tf.nn.bidirectional_dynamic_rnn(lstmCell, lstmCell_bk, put, dtype=tf.float32)\n",
    "        \n",
    "# #         value = tf.concat(value, 2)\n",
    "#         rnn_outputs, _ = tf.nn.dynamic_rnn(mlstmCell, put, dtype=tf.float32)\n",
    "# #         value = tf.transpose(value, [1, 0, 2])\n",
    "# # last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "#         last = _[-1][1]\n",
    "\n",
    "# #         with tf.name_scope('Attention_layer'):\n",
    "# #             attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "    \n",
    "# #         drop = tf.nn.dropout(attention_output, keep_prob_ph)\n",
    "    \n",
    "#         prediction = (tf.matmul(last, weight) + bias)\n",
    "#         return prediction\n",
    "    \n",
    "#     train_logits = model(data)\n",
    "#     prediction = tf.argmax(train_logits,1)\n",
    "#     train_correctPred = tf.equal(tf.argmax(train_logits,1), tf.argmax(labels,1))\n",
    "#     train_accuracy = tf.reduce_mean(tf.cast(train_correctPred, tf.float32))\n",
    "    \n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=train_logits, labels=labels))\n",
    "\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# #     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "#     param = tf.trainable_variables()\n",
    "#     gradients_op = tf.gradients(loss, param)\n",
    "    \n",
    "#     tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    \n",
    "    \n",
    "# #     input_test_data = tf.placeholder(tf.int32, [testSize, maxSeqLength])\n",
    "# # #     input_test_data = tf.constant(input_test_data, tf.float32)\n",
    "# #     test_labels = tf.placeholder(tf.float32, [testSize, numClasses])\n",
    "# #     test_steps = tf.placeholder(tf.int32, [testSize])\n",
    "\n",
    "# # #     te_data = tf.placeholder(tf.float32,[testSize, maxSeqLength, numDimensions])\n",
    "    \n",
    "# #     te_data = tf.Variable(tf.zeros([testSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "# #     te_data = tf.nn.embedding_lookup(w2v_list,input_test_data)\n",
    "# #     te_input = tf.unstack(te_data, maxSeqLength, 1)\n",
    "\n",
    "# #     predictions = model(data, test_steps)\n",
    "# #     prediction = tf.argmax(predictions, 1)\n",
    "# #     test_label = tf.argmax(test_labels,1)\n",
    "# #     test_loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=test_labels))\n",
    "# #     correctPred = tf.equal(prediction, test_label)\n",
    "# # correctPred = tf.equal()\n",
    "# #     accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "#     label = tf.argmax(labels, 1)\n",
    "#     # calculate f1, recall\n",
    "#     ones_like_actuals = tf.ones_like(label)\n",
    "#     zeros_like_actuals = tf.zeros_like(label)\n",
    "#     ones_like_predictions = tf.ones_like(prediction)\n",
    "#     zeros_like_predictions = tf.zeros_like(prediction)\n",
    "    \n",
    "#     tp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(label, ones_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     tn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(label, ones_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRUCell\n",
    "# import tensorflow as tf\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "\n",
    "#     labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "#     input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "#     steps = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# #     data = tf.placeholder(tf.float32,[batchSize, maxSeqLength, numDimensions])\n",
    "    \n",
    "#     data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "# #     input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "# #     lstmCell = tf.contrib.rnn.BasicRNNCell(lstmUnits, activation=tf.nn.relu)\n",
    "# #     lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "\n",
    "#     lstmCell = tf.contrib.rnn.GRUCell(lstmUnits)\n",
    "# #     lstmCell_bk = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "#     lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "\n",
    "#     weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "#     bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    \n",
    "#     def model(put, steps):\n",
    "# #         value, _ = tf.nn.bidirectional_dynamic_rnn(lstmCell, lstmCell_bk, put, dtype=tf.float32)\n",
    "        \n",
    "# #         value = tf.concat(value, 2)\n",
    "#         value, _ = tf.nn.dynamic_rnn(lstmCell, put, dtype=tf.float32, sequence_length=steps)\n",
    "# #         value, _ = tf.contrib.rnn.static_rnn(lstmCell, put, dtype=tf.float32)\n",
    "#         value = tf.transpose(value, [1, 0, 2])\n",
    "# # last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "# #         last = value[-1]\n",
    "#         last = _\n",
    "# #         last = _\n",
    "#         print(value.shape, _.shape, weight.shape, bias.shape)\n",
    "#         # 为什么这里使用3种维度的输入就得用last_state来进行计算，而之前二重维度就用value来计算呢？\n",
    "#         prediction = (tf.matmul(last, weight) + bias)\n",
    "#         return prediction\n",
    "    \n",
    "#     train_logits = model(data, steps)\n",
    "#     train_correctPred = tf.equal(tf.argmax(train_logits,1), tf.argmax(labels,1))\n",
    "#     train_accuracy = tf.reduce_mean(tf.cast(train_correctPred, tf.float32))\n",
    "    \n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=train_logits, labels=labels))\n",
    "\n",
    "#     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# #     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \n",
    "#     tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    \n",
    "    \n",
    "#     input_test_data = tf.placeholder(tf.int32, [testSize, maxSeqLength])\n",
    "# #     input_test_data = tf.constant(input_test_data, tf.float32)\n",
    "#     test_labels = tf.placeholder(tf.float32, [testSize, numClasses])\n",
    "#     test_steps = tf.placeholder(tf.int32, [testSize])\n",
    "\n",
    "# #     te_data = tf.placeholder(tf.float32,[testSize, maxSeqLength, numDimensions])\n",
    "    \n",
    "#     te_data = tf.Variable(tf.zeros([testSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     te_data = tf.nn.embedding_lookup(w2v_list,input_test_data)\n",
    "# #     te_input = tf.unstack(te_data, maxSeqLength, 1)\n",
    "# #     print(len(te_input))\n",
    "# #     print(te_data)\n",
    "# #     print(te_data.shape)\n",
    "# #     print(te_input[0].shape)\n",
    "# #     print(te_input[0])\n",
    "# #     prediction = model(te_input)\n",
    "#     predictions = model(te_data, test_steps)\n",
    "#     prediction = tf.argmax(predictions, 1)\n",
    "#     test_label = tf.argmax(test_labels,1)\n",
    "#     test_loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=test_labels))\n",
    "#     correctPred = tf.equal(prediction, test_label)\n",
    "# # correctPred = tf.equal()\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "#     # calculate f1, recall\n",
    "#     ones_like_actuals = tf.ones_like(test_label)\n",
    "#     zeros_like_actuals = tf.zeros_like(test_label)\n",
    "#     ones_like_predictions = tf.ones_like(prediction)\n",
    "#     zeros_like_predictions = tf.zeros_like(prediction)\n",
    "    \n",
    "#     tp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     tn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "# layer_num = 2\n",
    "# import tensorflow as tf\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "\n",
    "#     labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "#     input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "#     steps = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "#     data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "# #     input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "#     def get_a_cell():\n",
    "#         lstmCell = tf.contrib.rnn.BasicRNNCell(lstmUnits)\n",
    "# # , activation=tf.nn.relu\n",
    "# #     lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "# #     lstmCell_bk = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "#         lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "#         return lstmCell\n",
    "#     mlstmCell = tf.contrib.rnn.MultiRNNCell([get_a_cell() for i in range(layer_num)], state_is_tuple=True)\n",
    "# #     init_state = lstmCell.zero_state(batchSize, dtype=tf.float32)\n",
    "#     weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "#     bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    \n",
    "#     def model(put, steps):\n",
    "# #         value, _ = tf.nn.bidirectional_dynamic_rnn(lstmCell, lstmCell_bk, put, dtype=tf.float32)\n",
    "        \n",
    "# #         value = tf.concat(value, 2)\n",
    "#         value, _ = tf.nn.dynamic_rnn(mlstmCell, put, steps ,dtype=tf.float32)\n",
    "# #         value, _ = tf.contrib.rnn.static_rnn(lstmCell, put, dtype=tf.float32)\n",
    "#         value = tf.transpose(value, [1, 0, 2])\n",
    "# # last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "#         last = value[-1]\n",
    "    \n",
    "#         prediction = (tf.matmul(last, weight) + bias)\n",
    "#         return prediction\n",
    "    \n",
    "#     train_logits = model(data, steps)\n",
    "#     train_correctPred = tf.equal(tf.argmax(train_logits,1), tf.argmax(labels,1))\n",
    "#     train_accuracy = tf.reduce_mean(tf.cast(train_correctPred, tf.float32))\n",
    "    \n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=train_logits, labels=labels))\n",
    "#     gradients_op = tf.gradients(ys=loss, xs=weight)\n",
    "# #     optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    \n",
    "#     tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    \n",
    "    \n",
    "#     input_test_data = tf.placeholder(tf.int32, [testSize, maxSeqLength])\n",
    "# #     input_test_data = tf.constant(input_test_data, tf.float32)\n",
    "#     test_labels = tf.placeholder(tf.float32, [testSize, numClasses])\n",
    "#     test_steps = tf.placeholder(tf.int32, [testSize])\n",
    "\n",
    "#     te_data = tf.Variable(tf.zeros([testSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "#     te_data = tf.nn.embedding_lookup(w2v_list,input_test_data)\n",
    "# #     te_input = tf.unstack(te_data, maxSeqLength, 1)\n",
    "# #     print(len(te_input))\n",
    "# #     print(te_data)\n",
    "# #     print(te_data.shape)\n",
    "# #     print(te_input[0].shape)\n",
    "# #     print(te_input[0])\n",
    "# #     prediction = model(te_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     predictions = model(te_data, test_steps)\n",
    "#     prediction = tf.argmax(predictions, 1)\n",
    "#     test_label = tf.argmax(test_labels,1)\n",
    "    \n",
    "#     correctPred = tf.equal(prediction, test_label)\n",
    "# # correctPred = tf.equal()\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "    \n",
    "#     # calculate f1, recall\n",
    "#     ones_like_actuals = tf.ones_like(test_label)\n",
    "#     zeros_like_actuals = tf.zeros_like(test_label)\n",
    "#     ones_like_predictions = tf.ones_like(prediction)\n",
    "#     zeros_like_predictions = tf.zeros_like(prediction)\n",
    "    \n",
    "#     tp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     tn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fp_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, zeros_like_actuals), \n",
    "#         tf.equal(prediction, ones_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )\n",
    "\n",
    "#     fn_op = tf.reduce_sum(\n",
    "#     tf.cast(\n",
    "#       tf.logical_and(\n",
    "#         tf.equal(test_label, ones_like_actuals), \n",
    "#         tf.equal(prediction, zeros_like_predictions)\n",
    "#       ), \n",
    "#       \"float\"\n",
    "#     )\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, ?, 128)\n",
      "(<tf.Tensor 'Bi_IndRNN4/fw/fw/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'Bi_IndRNN4/bw/bw/while/Exit_3:0' shape=(?, 64) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "# # indRNN\n",
    "layer_num = 5\n",
    "import tensorflow as tf\n",
    "from ind_rnn_cell import IndRNNCell\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    labels = tf.placeholder(tf.float32, [None, numClasses])\n",
    "    input_data = tf.placeholder(tf.int32, [None, maxSeqLength])\n",
    "#     steps = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "#     data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "#     input = tf.unstack(data, maxSeqLength, 1)\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob_ph')\n",
    "\n",
    "#     lstmCell = tf.contrib.rnn.BasicRNNCell(lstmUnits, activation=tf.nn.relu)\n",
    "\n",
    "    recurrent_max = pow(2, 1 / maxSeqLength)\n",
    "\n",
    "#     lstmCell = IndRNNCell(128,)\n",
    "#     lstmCell = IndRNNCell(lstmUnits)\n",
    "    def get_a_cell():\n",
    "        cell = IndRNNCell(lstmUnits, recurrent_max_abs=recurrent_max)\n",
    "        return cell\n",
    "#     lstmCell = tf.contrib.rnn.MultiRNNCell([get_a_cell() for i in range(layer_num)])\n",
    "\n",
    "\n",
    "#     lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "#     lstmCell_bk = tf.contrib.rnn.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n",
    "    \n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal([lstmUnits * 2, numClasses]))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "    \n",
    "    def model(put):\n",
    "#         value, _ = tf.nn.dynamic_rnn(lstmCell, put, dtype=tf.float32)\n",
    "        for i in range(5):\n",
    "        \n",
    "            cell = get_a_cell()\n",
    "            cell_bk = get_a_cell()\n",
    "    \n",
    "            value, _ = tf.nn.bidirectional_dynamic_rnn(cell, cell_bk, put, scope=\"Bi_IndRNN\"+str(i) ,dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "            value = tf.concat(value, 2)\n",
    "        \n",
    "#             attention_output, alphas = attention(value, ATTENTION_SIZE, return_alphas=True)\n",
    "#             drop = tf.nn.dropout(attention_output, keep_prob)\n",
    "#         value, _ = tf.nn.dynamic_rnn(lstmCell, put, dtype=tf.float32, sequence_length=steps)\n",
    "#         value, _ = tf.contrib.rnn.static_rnn(lstmCell, put, dtype=tf.float32)\n",
    "#         value = tf.transpose(value, [1, 0, 2])\n",
    "# last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "#         last = value[-1]\n",
    "            put = value\n",
    "\n",
    "#         attention_output, alphas = attention(value, ATTENTION_SIZE, return_alphas=True)\n",
    "#         drop = tf.nn.dropout(attention_output, keep_prob)\n",
    "\n",
    "      \n",
    "        value = tf.transpose(value, [1, 0, 2])\n",
    "        \n",
    "        print(value.shape)\n",
    "        print(_)\n",
    "        \n",
    "        last = value[-1]\n",
    "        prediction = (tf.matmul(last, weight) + bias)\n",
    "        \n",
    "#         print(drop.shape)\n",
    "#         print(_)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    \n",
    "    train_logits = model(data)\n",
    "    train_correctPred = tf.equal(tf.argmax(train_logits,1), tf.argmax(labels,1))\n",
    "    train_accuracy = tf.reduce_mean(tf.cast(train_correctPred, tf.float32))\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=train_logits, labels=labels))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    \n",
    "    prediction = tf.argmax(train_logits,1)\n",
    "    label = tf.argmax(labels, 1)\n",
    "    # calculate f1, recall\n",
    "    ones_like_actuals = tf.ones_like(label)\n",
    "    zeros_like_actuals = tf.zeros_like(label)\n",
    "    ones_like_predictions = tf.ones_like(prediction)\n",
    "    zeros_like_predictions = tf.zeros_like(prediction)\n",
    "    \n",
    "    tp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(label, ones_like_actuals), \n",
    "        tf.equal(prediction, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "    tn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(label, zeros_like_actuals), \n",
    "        tf.equal(prediction, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "    fp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(label, zeros_like_actuals), \n",
    "        tf.equal(prediction, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "    fn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(label, ones_like_actuals), \n",
    "        tf.equal(prediction, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(value[0])\n",
    "# print(value.get_shape())\n",
    "# print(value.get_shape()[0])\n",
    "# print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test_prediction = tf.nn.softmax(prediction)\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(last))\n",
    "#     print(last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(int(value.get_shape()[0]) - 1)\n",
    "# print(last)\n",
    "# print(weight)\n",
    "# print(prediction)\n",
    "# print(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.argmax(prediction, 1))\n",
    "#     print(tf.argmax(prediction, 1))\n",
    "#     print(tf.argmax(labels, 1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import randint\n",
    "\n",
    "# def getTrainBatch():\n",
    "#     labels = []\n",
    "#     arr = np.zeros([batchSize, maxSeqLength])\n",
    "#     steps = []\n",
    "#     for i in range(batchSize):\n",
    "#         if (i % 2 == 0): \n",
    "#             num = randint(0,9700)\n",
    "#             labels.append([1,0])\n",
    "#         else:\n",
    "#             num = randint(10695,)\n",
    "#             labels.append([0,1])\n",
    "# #         print(i, ids[num-1])\n",
    "#         steps.append(num)\n",
    "#         arr[i] = ids[num]\n",
    "#     return arr, labels, steps\n",
    "\n",
    "# # def getTestBatch():\n",
    "# #     labels = []\n",
    "# #     arr = np.zeros([batchSize, maxSeqLength])\n",
    "# #     for i in range(batchSize):\n",
    "# #         num = randint(1800,2199)\n",
    "# #         if (num < 2000):\n",
    "# #             labels.append([1,0])\n",
    "# #         else:\n",
    "# #             labels.append([0,1])\n",
    "# #         arr[i] = ids[num]\n",
    "# #     return arr, labels\n",
    "\n",
    "# def getTestBatch():\n",
    "#     labels = []\n",
    "#     arr = np.zeros([testSize, maxSeqLength])\n",
    "#     steps = []\n",
    "#     for i in range(1800,2200):\n",
    "#         if (i < 2000):\n",
    "#             labels.append([1,0])\n",
    "#         else:\n",
    "#             labels.append([0,1])\n",
    "#         steps.append(i)\n",
    "#         arr[i-1800] = ids[i]\n",
    "#     return arr, labels, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainData = ids[0:1799]+ids[2200:3999]\n",
    "# trainLabels = []\n",
    "# for i in range(1800):\n",
    "#     trainLabels.append(np.array([1,0]))\n",
    "# for i in range(1800):\n",
    "#     trainLabels.append(np.array([0,1]))\n",
    "# trainLabels = np.array(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainData.shape)\n",
    "# print(trainLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(graph=graph)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "# tf.summary.scalar('Loss', loss)\n",
    "# tf.summary.scalar('Accuracy', accuracy)\n",
    "# merged = tf.summary.merge_all()\n",
    "# logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "# writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15616, 50)\n",
      "(15616, 2)\n"
     ]
    }
   ],
   "source": [
    "print(trainData.shape)\n",
    "print(trainLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:145.1933 batch_Acc: 0.5416667 test_loss:217.46718 acc:0.49510652\n",
      "acc_off:0.495 recall:1.000 precision:0.495 f1:0.662\n",
      "loss:0.6959944 batch_Acc: 0.45833334 test_loss:0.7485552 acc:0.5048935\n",
      "acc_off:0.505 recall:0.000 precision:10.000 f1:0.000\n",
      "loss:0.6927583 batch_Acc: 0.5833333 test_loss:0.6931252 acc:0.5048935\n",
      "acc_off:0.505 recall:0.000 precision:10.000 f1:0.000\n",
      "loss:0.69727355 batch_Acc: 0.375 test_loss:0.693432 acc:0.49510652\n",
      "acc_off:0.495 recall:1.000 precision:0.495 f1:0.662\n",
      "loss:0.69212914 batch_Acc: 0.5416667 test_loss:0.6930964 acc:0.5048935\n",
      "acc_off:0.505 recall:0.000 precision:10.000 f1:0.000\n",
      "loss:0.6939408 batch_Acc: 0.45833334 test_loss:0.69310087 acc:0.5048935\n",
      "acc_off:0.505 recall:0.000 precision:10.000 f1:0.000\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "#     offset = (i*batchSize) % (3600 - batchSize)\n",
    "#     feed_\n",
    "#     nextBatch, nextBatchLabels, nextSteps = getTrainBatch();\n",
    "    offset = (i * batchSize) % (len(trainLabels)-batchSize)\n",
    "    feed_dict={input_data:trainData[offset:offset + batchSize],\n",
    "                   labels:trainLabels[offset:offset + batchSize],\n",
    "              keep_prob:0.8}\n",
    "#                    steps:trainSteps[offset:offset + batchSize]}\n",
    "#     \n",
    "#     nextBatch = trainData[offset:offset+batchSize]\n",
    "#     nextBatchLabels = trainLabels[offset:offset+batchSize]\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    if (i % 500 == 0):\n",
    "#         batch_acc, lo = sess.run([train_accuracy,loss], {input_data: nextBatch, labels: nextBatchLabels, steps:nextSteps})\n",
    "        batch_acc, lo = sess.run([train_accuracy,loss], feed_dict=feed_dict)\n",
    "#         nextBatch, nextBatchLabels, nextSteps = getTestBatch();\n",
    "#         offset_test = (step * batch_size) % (len(trainLabels)-batch_size)\n",
    "        feed_dict_test={input_data:testData,\n",
    "                   labels:testLabels,\n",
    "                       keep_prob:0.8}\n",
    "#                    test_steps:testSteps}\n",
    "#         feed_dict_test={input_test_data:testData,\n",
    "#                    test_labels:testLabels}\n",
    "#         print(nextBatchLabels.shape)\n",
    "#         print(nextBatchLabels)\n",
    "#         acc = sess.run(accuracy, {input_test_data: nextBatch, test_labels: nextBatchLabels, test_steps:nextSteps})\n",
    "        acc, test_loss, tp, tn, fp, fn= sess.run([train_accuracy, loss, tp_op, tn_op, fp_op, fn_op], feed_dict=feed_dict_test)\n",
    "        loss_list.append(lo)\n",
    "        acc_list.append(acc)\n",
    "        \n",
    "        tpr = float(tp)/(float(tp) + float(fn))\n",
    "        fpr = float(fp)/(float(tp) + float(fn))\n",
    "\n",
    "        \n",
    "        accuracy_off = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "\n",
    "        \n",
    "        recall = tpr\n",
    "        try:\n",
    "            precision = float(tp)/(float(tp) + float(fp))\n",
    "        except ZeroDivisionError:\n",
    "            precision = 10.0\n",
    "\n",
    "        f1_score = (2 * (precision * recall)) / (float(precision) + float(recall))\n",
    "        \n",
    "#         print(\"gradients:%s\" % (gradients))\n",
    "        print(\"loss:%s batch_Acc: %s test_loss:%s acc:%s\" % (lo, batch_acc, test_loss,acc))\n",
    "        print(\"acc_off:%.3f recall:%.3f precision:%.3f f1:%.3f\" % (accuracy_off, recall, precision, f1_score))\n",
    "#         print(summary)\n",
    "    \n",
    "#    Save the network every 10,000 training iterations\n",
    "#     if (i % 10000 == 0 and i != 0):\n",
    "#         save_path = saver.save(sess, \"models_lstm/pretrained_lstm.ckpt\", global_step=i)\n",
    "#         print(\"saved to %s\" % save_path)\n",
    "print(\"answer\")\n",
    "end = time.time()\n",
    "print(\"Used:%s\" %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt.hist(loss_list, 50)\n",
    "plt.plot(range(21), loss_list)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.axis([0, 20, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "%matplotlib inline\n",
    "# plt.hist(loss_list, 50)\n",
    "plt.plot(range(21), acc_list)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Acc')\n",
    "# plt.axis([0, 20, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# testData = ids[1800:1999]+ids[2000:2199]\n",
    "# testLabels = []\n",
    "# for i in range(200):\n",
    "#     testLabels.append(np.array([1,0]))\n",
    "# for i in range(200):\n",
    "#     testLabels.append(np.array([0,1]))\n",
    "# testLabels = np.array(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# iterations = 10\n",
    "# acc_total = 0;\n",
    "# for i in range(iterations):\n",
    "#     offset = (i*batchSize) % (400 - batchSize)\n",
    "# #     feed_\n",
    "# #     nextBatch, nextBatchLabels = getTrainBatch();\n",
    "# #     nextBatch = testData[offset:offset+batchSize]\n",
    "# #     nextBatchLabels = testLabels[offset:offset+batchSize]\n",
    "#     nextBatch, nextBatchLabels = getTestBatch();\n",
    "    \n",
    "# #     summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "# #     writer.add_summary(summary, i)\n",
    "    \n",
    "#     acc_this_batch = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "#     acc_total +=acc_this_batch\n",
    "#     print(\"Accuracy for this batch:\",acc_total * 100 / (i+1))\n",
    "#     nextBatch, nextBatchLabels = getTestBatch();\n",
    "#     print(\"Loss for this batch:\", (sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "# # writer.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceMatrix(sentence):\n",
    "    arr_step = np.zeros([testSize], dtype='int32')\n",
    "    sentenceMatrix = np.zeros([testSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = jieba.cut(cleanedSentence)\n",
    "    \n",
    "    indexCounter = 0\n",
    "    for word in split:\n",
    "        if word in stoplist:\n",
    "            continue\n",
    "        try:\n",
    "            print(word)\n",
    "            sentenceMatrix[0][indexCounter] = word_list.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0][indexCounter] = 572296 #Vector for unkown\n",
    "        indexCounter += 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    arr_step[0] = indexCounter\n",
    "    return sentenceMatrix, arr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"非常差。国内最差的酒店之一。各方面都不好。\"\n",
    "inputMatrix, inputSteps = getSentenceMatrix(inputText)\n",
    "print(inputMatrix.shape)\n",
    "print(input_test_data.shape)\n",
    "print(test_steps.shape)\n",
    "print(inputSteps.shape)\n",
    "\n",
    "\n",
    "predictedSentiment = sess.run(predictions, {input_test_data: inputMatrix, test_steps: inputSteps})\n",
    "print(predictedSentiment)\n",
    "# if (predictedSentiment > ):\n",
    "#     print(\"Positive Sentiment\")\n",
    "# else:\n",
    "#     print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.argmax(predictedSentiment,1).eval())\n",
    "\n",
    "# if (predictedSentiment > 0.5):\n",
    "#     print(\"Positive Sentiment\")\n",
    "# else:\n",
    "#     print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
