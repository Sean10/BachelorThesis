{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = {line.strip() for line in open(filepath, \"r\", encoding='utf-8').readlines()}\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def cleanSentences(string):\n",
    "    return re.sub(\"[a-zA-Z0-9\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！<>《》，。？、~@#￥%……&*（） ]+\", \"\",string)\n",
    "\n",
    "\n",
    "program = os.path.basename(\"deep learning\")\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "# positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "# negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "\n",
    "# files = positiveFiles + negativeFiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [\"corpus.txt\"]\n",
    "\n",
    "# vec = []\n",
    "\n",
    "\n",
    "# # logger.info(\"running %s\" % ' '.join(sys.argv))\n",
    "\n",
    "# i = 0\n",
    "# for file in files:\n",
    "#     with open(file, \"r\",encoding='utf-8') as f:\n",
    "#         if os.path.exists(\"corpus_segment.txt\"):\n",
    "#             continue\n",
    "        \n",
    "#         fp = codecs.open(\"corpus_segment.txt\", \"a+\", encoding=\"utf-8\")\n",
    "        \n",
    "#         for line in f:\n",
    "#             line = cleanSentences(line)\n",
    "#             if line.strip() is \"\n",
    "#                 continue\n",
    "            \n",
    "#             sentence_seged = jieba.cut(line.strip())\n",
    "# #             print(type(sentence_seged))\n",
    "# #             tmp = []\n",
    "# #             for word in sentence_seged:\n",
    "# #                 if word.strip() not in stoplist:\n",
    "# #                     tmp.append(word)\n",
    "# #             vec.append(tmp)\n",
    "# #             tmp.append(word)\n",
    "#             fp.write(\" \".join(sentence_seged))\n",
    "            \n",
    "#             i += 1\n",
    "#             if i % 1000 == 0:\n",
    "#                 logger.info(\"Saved \"+ str(i) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(i) + \" articles\")\n",
    "# # print(vec)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-18 03:58:26,403: INFO: 'pattern' package not found; tag filters are not available for English\n",
      "2018-05-18 03:58:26,460: INFO: loading Word2Vec object from model_word2vec\n",
      "2018-05-18 03:58:31,321: INFO: loading wv recursively from model_word2vec.wv.* with mmap=None\n",
      "2018-05-18 03:58:31,340: INFO: loading vectors from model_word2vec.wv.vectors.npy with mmap=None\n",
      "2018-05-18 03:59:28,356: INFO: setting ignored attribute vectors_norm to None\n",
      "2018-05-18 03:59:28,358: INFO: loading vocabulary recursively from model_word2vec.vocabulary.* with mmap=None\n",
      "2018-05-18 03:59:28,372: INFO: loading trainables recursively from model_word2vec.trainables.* with mmap=None\n",
      "2018-05-18 03:59:28,377: INFO: loading syn1neg from model_word2vec.trainables.syn1neg.npy with mmap=None\n",
      "2018-05-18 04:00:24,862: INFO: setting ignored attribute cum_table to None\n",
      "2018-05-18 04:00:24,863: INFO: loaded model_word2vec\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "似乎不需要去除停用词\n",
    "'''\n",
    "# stoplist = stopwordslist(\"stopwords.txt\")\n",
    "# fout = \"corpus_seg_without_stop.txt\"\n",
    "# cnt = 0\n",
    "# with codecs.open(\"corpus_segment.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         tmp = []\n",
    "#         for word in line.split():\n",
    "#             if word.strip() in stoplist:\n",
    "#                continue\n",
    "#             tmp.append(word)\n",
    "#         fout.write(\" \".join(tmp))\n",
    "#         cnt += 1\n",
    "#         if cnt % 1000 == 0:\n",
    "#             logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# input = \"corpus_segment.txt\"\n",
    "# model = Word2Vec(LineSentence(input), size=400, window=5,min_count=5,workers=multiprocessing.cpu_count())\n",
    "# model.save('model_word2vec')\n",
    "# model.wv.save_word2vec_format(\"model_saved_format_version\", binary=False)  \n",
    "\n",
    "model = Word2Vec.load(\"model_word2vec\")\n",
    "# for mo in model.wv.vocab:\n",
    "#     print(mo, model.wv[mo])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# w2v_list = np.array([model.wv[word] for word in model.wv.vocab])\n",
    "# word_list = np.array([word for word in model.wv.vocab]).tolist()\n",
    "# print(w2v_np)\n",
    "\n",
    "\n",
    "# print(model[u'n'])\n",
    "# print(model.wv[u'机会'])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w2v_list.shape)\n",
    "# print(model.wv.most_similar('爱情'))\n",
    "# print(model.wv.similarity('爱情','宾馆'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwordslist(\"stopwords.txt\")\n",
    "\n",
    "# positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "# negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "\n",
    "# seg_positiveFiles = ['seged_positiveReviews/' + f for f in listdir('seged_positiveReviews/') if isfile(join('seged_positiveReviews/', f))]\n",
    "# seg_negativeFiles = ['seged_negativeReviews/' + f for f in listdir('seged_negativeReviews/') if isfile(join('seged_negativeReviews/', f))]\n",
    "\n",
    "# cnt = 0\n",
    "\n",
    "# if os.path.exists(\"seged_positiveReviews\"):\n",
    "#     pass\n",
    "# else:\n",
    "#     os.mkdir(\"seged_positiveReviews\")\n",
    "\n",
    "# if os.path.exists(\"seged_negativeReviews\"):\n",
    "#     pass\n",
    "# else:\n",
    "#     os.mkdir(\"seged_negativeReviews\")\n",
    "    \n",
    "# for pf in positiveFiles:\n",
    "#     positive_seged = \"seged_\"+pf\n",
    "    \n",
    "#     if os.path.exists(positive_seged):\n",
    "#         continue\n",
    "        \n",
    "#     with codecs.open(pf, \"r\", encoding=\"utf-8\") as f:\n",
    "#         print(pf)\n",
    "#         file = []\n",
    "#         for line in f:\n",
    "#             line = cleanSentences(line).strip()\n",
    "#             sentence_seged = jieba.cut(line)\n",
    "            \n",
    "#             temp = []\n",
    "#             for word in sentence_seged:\n",
    "#                 if word in stoplist:\n",
    "#                     continue;\n",
    "#                 temp.append(word)\n",
    "#             file.append(\" \".join(temp))\n",
    "#         with open(positive_seged, \"w\",encoding='utf-8') as of:\n",
    "#             of.write(\"\\n\".join(file))\n",
    "#         cnt += 1\n",
    "#         if cnt % 1000 == 0:\n",
    "#             logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "# for pf in negativeFiles:\n",
    "#     negative_seged = \"seged_\"+pf\n",
    "    \n",
    "#     if os.path.exists(negative_seged):\n",
    "#         continue\n",
    "    \n",
    "#     with codecs.open(pf, \"r\", encoding=\"utf-8\") as f:\n",
    "#         print(pf)\n",
    "#         file = []\n",
    "#         for line in f:\n",
    "#             line = cleanSentences(line).strip()\n",
    "#             sentence_seged = jieba.cut(line)\n",
    "            \n",
    "#             temp = []\n",
    "#             for word in sentence_seged:\n",
    "#                 if word in stoplist:\n",
    "#                     continue;\n",
    "#                 temp.append(word)\n",
    "#             file.append(\" \".join(temp))\n",
    "#         with open(negative_seged, \"w\",encoding='utf-8') as of:\n",
    "#             of.write(\"\\n\".join(file))\n",
    "#         cnt += 1\n",
    "#         if cnt % 1000 == 0:\n",
    "#             logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "\n",
    "# logger.info(\"Saved \"+ str(cnt) + \" articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "# numWords = []\n",
    "# for pf in seg_positiveFiles:\n",
    "#     with open(pf, \"r\", encoding='utf-8') as f:\n",
    "#         line=f.readline()\n",
    "#         counter = len(\"\".join(line.split()))\n",
    "# #         print(line.split(), len(line.split()))\n",
    "#         numWords.append(counter)       \n",
    "# print('Positive files finished')\n",
    "\n",
    "# for nf in seg_negativeFiles:\n",
    "#     with open(nf, \"r\", encoding='utf-8') as f:\n",
    "#         line=f.readline()\n",
    "#         counter = len(line.split())\n",
    "#         numWords.append(counter)  \n",
    "# print('Negative files finished')\n",
    "\n",
    "# numFiles = len(numWords)\n",
    "# print('The total number of files is', numFiles)\n",
    "# print('The total number of words in the files is', sum(numWords))\n",
    "# print('The average number of words in the files is', sum(numWords)/len(numWords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.hist(numWords, 50)\n",
    "# plt.xlabel('Sequence Length')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.axis([0, 200, 0, 2500])\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "# firstSentense = np.zeros((maxSeqLength), dtype='int32')\n",
    "# firstSentense[0] = word_list.index('宾馆')\n",
    "# firstSentense[1] = word_list.index('喜欢')\n",
    "# firstSentense[2] = word_list.index('爱')\n",
    "# print(firstSentense.shape)\n",
    "# print(firstSentense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "# iterations = 10000\n",
    "num_steps = 10001\n",
    "\n",
    "\n",
    "maxSeqLength = 50\n",
    "numDimensions = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "# fileCounter = 0\n",
    "\n",
    "# for pf in seg_positiveFiles:\n",
    "#     with open(pf, \"r\") as f:\n",
    "#         indexCounter = 0\n",
    "#         line=f.readline()\n",
    "# #         print(line.split())\n",
    "        \n",
    "#         for word in line.split():\n",
    "#             try:\n",
    "# #                 print(word, word_list.index(word))\n",
    "#                 ids[fileCounter][indexCounter] = word_list.index(word)\n",
    "#             except ValueError:\n",
    "# #                 print(word)\n",
    "#                 ids[fileCounter][indexCounter] = 572296 #Vector for unkown words\n",
    "#             indexCounter = indexCounter + 1\n",
    "#             if indexCounter >= maxSeqLength:\n",
    "#                 break\n",
    "#         fileCounter = fileCounter + 1\n",
    "\n",
    "# for nf in seg_negativeFiles:\n",
    "#     with open(nf, \"r\") as f:\n",
    "#         indexCounter = 0\n",
    "#         line=f.readline()\n",
    "#         for word in line.split():\n",
    "#             try:\n",
    "# #                 print(word, word_list.index(word))\n",
    "#                 ids[fileCounter][indexCounter] = word_list.index(word)\n",
    "#             except ValueError:\n",
    "#                 ids[fileCounter][indexCounter] = 572296 #Vector for unkown words\n",
    "#             indexCounter = indexCounter + 1\n",
    "#             if indexCounter >= maxSeqLength:\n",
    "#                 break\n",
    "#         fileCounter = fileCounter + 1 \n",
    "\n",
    "# #Pass into embedding function and see if it evaluates. \n",
    "\n",
    "# np.save('idsMatrix', ids)\n",
    "\n",
    "from random import randint, shuffle\n",
    "\n",
    "\n",
    "def getWords(file):\n",
    "#     files = [filePath + f for f in listdir(filePath) if isfile(join(filePath, f))]\n",
    "# #     print(files)\n",
    "#     lineList = []\n",
    "#     for nf in files:\n",
    "        \n",
    "#         with open(nf, \"r\") as f:\n",
    "#             indexCounter = 0\n",
    "# #             print(f)\n",
    "#             line=f.readline()\n",
    "# #             print(line)\n",
    "#             wordList = []\n",
    "#             for word in line.split():\n",
    "#                 wordList.append(word)\n",
    "#         lineList.append(wordList)\n",
    "#     return lineList\n",
    "    content = []\n",
    "    cnt = 0\n",
    "    with codecs.open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = cleanSentences(line).strip()\n",
    "            sentence_seged = jieba.cut(line)\n",
    "            \n",
    "            temp = []\n",
    "            for word in sentence_seged:\n",
    "                if word in stoplist:\n",
    "                    continue;\n",
    "                temp.append(word)\n",
    "            content.append(\" \".join(temp))\n",
    "        \n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "    logger.info(\"Saved \"+ str(cnt) + \" articles\")\n",
    "    with open(\"seged_\"+file, \"w\",encoding='utf-8') as of:\n",
    "        print(\"seged_\"+file)\n",
    "        of.write(\"\\n\".join(content))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2Array(lineList):\n",
    "    linesArray=[]\n",
    "    \n",
    "    steps = []\n",
    "    for line in lineList:\n",
    "        t = 0\n",
    "        p = 0\n",
    "        wordsArray=[]\n",
    "        for i in range(maxSeqLength):\n",
    "            if i<len(line):\n",
    "                try:\n",
    "                    wordsArray.append(model.wv.word_vec(line[i]))\n",
    "                    p = p + 1\n",
    "                except KeyError:\n",
    "                    t=t+1\n",
    "                    continue\n",
    "            else:\n",
    "                wordsArray.append(np.array([0.0]*numDimensions))\n",
    "        for i in range(t):\n",
    "            wordsArray.append(np.array([0.0]*numDimensions))\n",
    "        steps.append(p)\n",
    "        linesArray.append(wordsArray)\n",
    "    linesArray = np.array(linesArray)\n",
    "    steps = np.array(steps)\n",
    "    return linesArray, steps\n",
    "\n",
    "def convert2Data(posArray, negArray, posStep, negStep):\n",
    "    randIt = []\n",
    "    data = []\n",
    "    steps = []\n",
    "    labels = []\n",
    "    for i in range(len(posArray)):\n",
    "        randIt.append([posArray[i], posStep[i], [1,0]])\n",
    "    for i in range(len(negArray)):\n",
    "        randIt.append([negArray[i], negStep[i], [0,1]])\n",
    "    shuffle(randIt)\n",
    "    for i in range(len(randIt)):\n",
    "        data.append(randIt[i][0])\n",
    "        steps.append(randIt[i][1])\n",
    "        labels.append(randIt[i][2])\n",
    "#     print(len(data))\n",
    "    data = np.array(data)\n",
    "    steps = np.array(steps)\n",
    "    return data, steps, labels\n",
    "\n",
    "def makeData(posPath,negPath):\n",
    "    #获取词汇，返回类型为[[word1,word2...],[word1,word2...],...]\n",
    "    pos = getWords(posPath)\n",
    "    print(\"The positive data's length is :\",len(pos))\n",
    "    neg = getWords(negPath)\n",
    "    print(\"The negative data's length is :\",len(neg))\n",
    "    #将评价数据转换为矩阵，返回类型为array\n",
    "    posArray, posSteps = words2Array(pos)\n",
    "    negArray, negSteps = words2Array(neg)\n",
    "    #将积极数据和消极数据混合在一起打乱，制作数据集\n",
    "#     Data, Steps, Labels = convert2Data(posArray, negArray, posSteps, negSteps)\n",
    "    return posArray, negArray, posSteps, negSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2018-05-18 04:00:26,751: DEBUG: Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2018-05-18 04:00:26,785: DEBUG: Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.919 seconds.\n",
      "2018-05-18 04:00:27,674: DEBUG: Loading model cost 0.919 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2018-05-18 04:00:27,677: DEBUG: Prefix dict has been built succesfully.\n",
      "2018-05-18 04:00:29,238: INFO: Saved 1000 articles\n",
      "2018-05-18 04:00:30,562: INFO: Saved 2000 articles\n",
      "2018-05-18 04:00:31,878: INFO: Saved 3000 articles\n",
      "2018-05-18 04:00:32,767: INFO: Saved 4000 articles\n",
      "2018-05-18 04:00:33,566: INFO: Saved 5000 articles\n",
      "2018-05-18 04:00:34,043: INFO: Saved 6000 articles\n",
      "2018-05-18 04:00:34,494: INFO: Saved 7000 articles\n",
      "2018-05-18 04:00:34,892: INFO: Saved 8000 articles\n",
      "2018-05-18 04:00:35,658: INFO: Saved 9000 articles\n",
      "2018-05-18 04:00:35,915: INFO: Saved 9701 articles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seged_data_new/B/Pos-train.txt\n",
      "The positive data's length is : 9701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-18 04:00:37,014: INFO: Saved 1000 articles\n",
      "2018-05-18 04:00:37,839: INFO: Saved 2000 articles\n",
      "2018-05-18 04:00:38,546: INFO: Saved 3000 articles\n",
      "2018-05-18 04:00:39,811: INFO: Saved 4000 articles\n",
      "2018-05-18 04:00:41,204: INFO: Saved 5000 articles\n",
      "2018-05-18 04:00:41,651: INFO: Saved 6000 articles\n",
      "2018-05-18 04:00:42,133: INFO: Saved 7000 articles\n",
      "2018-05-18 04:00:42,429: INFO: Saved 8000 articles\n",
      "2018-05-18 04:00:43,110: INFO: Saved 9000 articles\n",
      "2018-05-18 04:00:43,374: INFO: Saved 9429 articles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seged_data_new/B/Neg-train.txt\n",
      "The negative data's length is : 9429\n",
      "In test data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-18 04:02:40,391: INFO: Saved 995 articles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seged_data_new/B/Pos-test.txt\n",
      "The positive data's length is : 995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-18 04:02:47,702: INFO: Saved 999 articles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seged_data_new/B/Neg-test.txt\n",
      "The negative data's length is : 999\n",
      "------------------------------\n",
      "The trainData's shape is: (19130, 50, 400)\n",
      "The testData's shape is: (1994, 50, 400)\n",
      "The trainSteps's shape is: (19130,)\n",
      "The testSteps's shape is: (1994,)\n",
      "The trainLabels's shape is: (19130, 2)\n",
      "The testLabels's shape is: (1994, 2)\n"
     ]
    }
   ],
   "source": [
    "train_posArray, train_negArray, train_posSteps, train_negSteps = makeData('data_new/B/Pos-train.txt', 'data_new/B/Neg-train.txt')\n",
    "\n",
    "print(\"In test data:\")\n",
    "test_posArray, test_negArray, test_posSteps, test_negSteps = makeData('data_new/B/Pos-test.txt', 'data_new/B/Neg-test.txt')\n",
    "\n",
    "del model\n",
    "\n",
    "trainData, trainSteps, trainLabels = convert2Data(train_posArray, train_negArray, train_posSteps, train_negSteps)\n",
    "\n",
    "testData, testSteps, testLabels = convert2Data(test_posArray, test_negArray, test_posSteps, test_negSteps)\n",
    "# print(\"In train data:\")\n",
    "# trainData, trainSteps, trainLabels = makeData('data_new/B/Pos-train.txt',\n",
    "#                                               'data_new/B/Neg-train.txt')\n",
    "# print(\"In test data:\")\n",
    "# testData, testSteps, testLabels = makeData('data_new/B/Pos-test.txt',\n",
    "#                                            'data_new/B/Neg-test.txt')\n",
    "trainLabels = np.array(trainLabels)\n",
    "\n",
    "\n",
    "# del model\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"The trainData's shape is:\",trainData.shape)\n",
    "print(\"The testData's shape is:\",testData.shape)\n",
    "print(\"The trainSteps's shape is:\",trainSteps.shape)\n",
    "print(\"The testSteps's shape is:\",testSteps.shape)\n",
    "print(\"The trainLabels's shape is:\",trainLabels.shape)\n",
    "print(\"The testLabels's shape is:\",np.array(testLabels).shape)\n",
    "# print(ids[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(trainData[0][0]))\n",
    "# types = testData[0][0]\n",
    "# for enui,i in enumerate(testData):\n",
    "#     for enuj,j in enumerate(i):\n",
    "# #         if type(j) != types:\n",
    "#             print(enui, enuj, j.shape)\n",
    "# # print(type(testData[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [64], [64,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [64], [64,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-96bec7227b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtrain_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     loss = tf.reduce_mean(\n\u001b[1;32m     44\u001b[0m         tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels,\n",
      "\u001b[0;32m<ipython-input-15-96bec7227b9d>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(dataset, steps)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2122\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   4277\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4278\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4279\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4280\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4281\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1734\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1735\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [64], [64,32]."
     ]
    }
   ],
   "source": [
    "\n",
    "# try 10000 iteration\n",
    "\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size,maxSeqLength,numDimensions))\n",
    "    tf_train_steps = tf.placeholder(tf.int32,shape=(batch_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32,shape=(batch_size,numClasses))\n",
    "\n",
    "    tf_test_dataset = tf.constant(testData,tf.float32)\n",
    "    tf_test_steps = tf.constant(testSteps,tf.int32)\n",
    "\n",
    "\n",
    "#     lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units = lstmUnits,state_is_tuple=True)\n",
    "    lstm_cell = tf.contrib.rnn.BasicRNNCell(num_units = lstmUnits)\n",
    "    lstm_cell = tf.contrib.rnn.DropoutWrapper(cell=lstm_cell, output_keep_prob=0.75)\n",
    "#     init_state = cell.zero_state(batch_size, dtype=tf.float32) \n",
    "\n",
    "    w1 = tf.Variable(tf.truncated_normal([lstmUnits,lstmUnits // 2], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.truncated_normal([lstmUnits // 2], stddev=0.1))\n",
    "\n",
    "    w2 = tf.Variable(tf.truncated_normal([lstmUnits // 2, 2], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.truncated_normal([2], stddev=0.1))\n",
    "    \n",
    "    def model(dataset, steps):\n",
    "#         outputs, last_states = tf.nn.dynamic_rnn(cell = lstm_cell,\n",
    "#                                                  dtype = tf.float32,\n",
    "#                                                  sequence_length = steps,\n",
    "#                                                  inputs = dataset,\n",
    "#                                                 initial_state=init_state)\n",
    "        outputs, last_states = tf.nn.dynamic_rnn(cell = lstm_cell,\n",
    "                                                 dtype = tf.float32,\n",
    "                                                 sequence_length = steps,\n",
    "                                                 inputs = dataset)\n",
    "        hidden = last_states[-1]\n",
    "\n",
    "        hidden = tf.matmul(hidden, w1) + b1\n",
    "        logits = tf.matmul(hidden, w2) + b2\n",
    "        return logits\n",
    "    \n",
    "    train_logits = model(tf_train_dataset, tf_train_steps)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels,\n",
    "                                                logits=train_logits))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "#     test_prediction = tf.argmax(tf.nn.softmax(model(tf_test_dataset, tf_test_steps)), axis=1)\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, tf_test_steps))\n",
    "    \n",
    "\n",
    "summary_frequency = 500\n",
    "\n",
    "session = tf.InteractiveSession(graph=graph)\n",
    "saver = tf.train.Saver()\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "    \n",
    "    \n",
    "    \n",
    "loss_list = []\n",
    "acc_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "TPR_list = []\n",
    "FPR_list = []\n",
    "mean_loss = 0\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (len(trainLabels)-batch_size)\n",
    "    feed_dict={tf_train_dataset:trainData[offset:offset + batch_size],\n",
    "                   tf_train_labels:trainLabels[offset:offset + batch_size],\n",
    "                   tf_train_steps:trainSteps[offset:offset + batch_size]}\n",
    "    _, l = session.run([optimizer,loss],\n",
    "                           feed_dict = feed_dict)\n",
    "    mean_loss += l\n",
    "    if step >0 and step % summary_frequency == 0:\n",
    "#             summary = sess.run(merged, feed_dict = feed_dict)\n",
    "#             writer.add_summary(summary, step)\n",
    "            \n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "        print(\"The step is: %d\"%(step))\n",
    "        print(\"In train data,the loss is:%.4f\"%(mean_loss))\n",
    "        loss_list.append(mean_loss)\n",
    "        mean_loss = 0\n",
    "        acrc = 0\n",
    "        prediction = session.run(test_prediction)\n",
    "#  s       print(prediction)\n",
    "        for i in range(len(prediction)):\n",
    "            if prediction[i][testLabels[i].index(1)] > 0.5:\n",
    "                acrc = acrc + 1\n",
    "                \n",
    "            if testLabels[i][0] == 1:\n",
    "                if prediction[i][0] > 0.5:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "            else:\n",
    "                if prediction[i][0] > 0.5:\n",
    "                    TN += 1\n",
    "                else:\n",
    "                    FN += 1\n",
    "        \n",
    "        precision = TP / (TP+FP)\n",
    "        precision_list.append(precision)\n",
    "        recall = TP/ (TP+FN)\n",
    "        recall_list.append(recall)\n",
    "        TPR = TP / (TP+FN)\n",
    "        TPR_list.append(TPR)\n",
    "        FPR = FP / (FP + TN)\n",
    "        FPR_list.append(FPR)\n",
    "        \n",
    "        acc = acrc/len(testLabels)*100\n",
    "        print(\"In test data,the accuracy is:%.2f%%\"%(acc))\n",
    "        acc_list.append(acc/100)\n",
    "#####################################\n",
    "save_path = saver.save(session, \"models_new_lstm/pretrained_lstm.ckpt\", global_step=10000)\n",
    "print(\"saved to %s\" % save_path)\n",
    "    \n",
    "    \n",
    "# writer.close()\n",
    "# timeB=time.time()\n",
    "# print(\"time cost:\",int(timeB-timeA))\n",
    "\n",
    "\n",
    "# labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "# input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "# print(labels.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(test_prediction.shape)\n",
    "\n",
    "# print(loss_list)\n",
    "# print(acc_list)\n",
    "\n",
    "%matplotlib inline\n",
    "# plt.hist(loss_list, 50)\n",
    "plt.plot(range(20), loss_list)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.axis([0, 20, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "%matplotlib inline\n",
    "# plt.hist(acc_list, 50)\n",
    "plt.plot(range(20), acc_list)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Acc')\n",
    "# plt.axis([0, 20, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "%matplotlib inline\n",
    "# plt.hist(acc_list, 50)\n",
    "plt.plot(recall_list,precision_list)\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('recall')\n",
    "# plt.axis([0, 20, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# plt.hist(acc_list, 50)\n",
    "plt.plot(FPR_list, TPR_list)\n",
    "plt.xlabel('FPR_list')\n",
    "plt.ylabel('TPR_list')\n",
    "# plt.axis([0, 20, 0, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "将向量转换成嵌入矩阵的过程\n",
    "'''\n",
    "\n",
    "# data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "# data = tf.nn.embedding_lookup(w2v_list,input_data)\n",
    "\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FPR_list)\n",
    "print(TPR_list)\n",
    "print(precision_list)\n",
    "print(recall_list)\n",
    "\n",
    "# lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "# lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "\n",
    "# input = tf.unstack(data, maxSeqLength, 1)\n",
    "\n",
    "\n",
    "\n",
    "# value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(value[0])\n",
    "# print(value.get_shape())\n",
    "# print(value.get_shape()[0])\n",
    "# print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "# bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "# # value = tf.transpose(value, [1, 0, 2])\n",
    "# # last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "# last = value[-1]\n",
    "# # print(value)\n",
    "# # print(last)\n",
    "# prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "\n",
    "# # with tf.Session() as sess:\n",
    "# #     print(sess.run(last))\n",
    "# #     print(last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(int(value.get_shape()[0]) - 1)\n",
    "# # print(last)\n",
    "# # print(weight)\n",
    "# print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # with tf.Session() as sess:\n",
    "# #     sess.run(tf.argmax(prediction, 1))\n",
    "# #     print(tf.argmax(prediction, 1))\n",
    "# #     print(tf.argmax(labels, 1))\n",
    "\n",
    "# print(acc)\n",
    "\n",
    "# correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # def getTrainBatch():\n",
    "# #     labels = []\n",
    "# #     arr = np.zeros([batchSize, maxSeqLength])\n",
    "    \n",
    "# #     for i in range(batchSize):\n",
    "# #         if (i % 2 == 0): \n",
    "# #             num = randint(1,1800)\n",
    "# #             labels.append([1,0])\n",
    "# #         else:\n",
    "# #             num = randint(2200,3999)\n",
    "# #             labels.append([0,1])\n",
    "# # #         print(i, ids[num-1])\n",
    "# #         arr[i] = ids[num-1]\n",
    "# #     return arr, labels\n",
    "\n",
    "# # def getTestBatch():\n",
    "# #     labels = []\n",
    "# #     arr = np.zeros([batchSize, maxSeqLength])\n",
    "# #     for i in range(batchSize):\n",
    "# #         num = randint(1801,2199)\n",
    "# #         if (num <= 2000):\n",
    "# #             labels.append([1,0])\n",
    "# #         else:\n",
    "# #             labels.append([0,1])\n",
    "# #         arr[i] = ids[num-1]\n",
    "# #     return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "   #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "#         pred = = sess.run(prediction,{input_data: nextBatch, labels: nextBatchLabels})\n",
    "#         pred = sess.run(prediction,{input_data: nextBatch, labels: nextBatchLabels})\n",
    "#         correct = sess.run(correctPred,{input_data: nextBatch, labels: nextBatchLabels})\n",
    "        acc = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        lo = sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        print(\"acc: %s, loss:%s\" % (acc ,lo))\n",
    "#         print(summary)\n",
    "    \n",
    "   #Save the network every 10,000 training iterations\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Loss for this batch:\", (sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = jieba.cut(cleanedSentence)\n",
    "    \n",
    "    \n",
    "    for indexCounter,word in enumerate(split):\n",
    "        if word in stoplist:\n",
    "            continue\n",
    "        try:\n",
    "            print(word)\n",
    "            sentenceMatrix[0,indexCounter] = word_list.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 572296 #Vector for unkown\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"非常差。国内最差的酒店之一。各方面都不好。\"\n",
    "inputMatrix = getSentenceMatrix(inputText)\n",
    "\n",
    "\n",
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})\n",
    "print(predictedSentiment)\n",
    "if (predictedSentiment[0][0] > predictedSentiment[0][1]):\n",
    "    print(\"Positive Sentiment\")\n",
    "else:\n",
    "    print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
