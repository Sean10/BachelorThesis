
@article{zhangDeepLearningSentiment2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.07883},
  primaryClass = {cs, stat},
  title = {Deep {{Learning}} for {{Sentiment Analysis}} : {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Sentiment Analysis}}},
  abstract = {Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many other application domains, deep learning is also popularly used in sentiment analysis in recent years. This paper first gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.},
  journal = {arXiv:1801.07883 [cs, stat]},
  author = {Zhang, Lei and Wang, Shuai and Liu, Bing},
  month = jan,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Learning,Statistics - Machine Learning},
  file = {/Users/sean10/zotero/storage/SVF56GLA/Zhang et al. - 2018 - Deep Learning for Sentiment Analysis  A Survey.pdf}
}

@inproceedings{wangDimensionalSentimentAnalysis2016,
  title = {Dimensional {{Sentiment Analysis Using}} a {{Regional CNN}}-{{LSTM Model}}},
  doi = {10.18653/v1/P16-2037},
  author = {Wang, Jin and Yu, Liang-Chih and Lai, K and Zhang, Xuejie},
  month = jan,
  year = {2016},
  pages = {225-230},
  file = {/Users/sean10/zotero/storage/7QNXI2VN/Wang et al. - 2016 - Dimensional Sentiment Analysis Using a Regional CN.pdf}
}

@article{jiangJiYuPingLunQingGanFenXiDeGeXingHuaTuiJianCeLueYanJiuYiDouBanYingPingWeiLi2017,
  title = {{基于评论情感分析的个性化推荐策略研究\textemdash\textemdash{}以豆瓣影评为例}},
  issn = {1000-7490},
  lccn = {11-1762/G3},
  abstract = {[目的/意义]随着社会化媒体的兴起,信息资源的数量呈现爆炸式增长,如何在海量的信息中帮助用户发现有用的知识成为亟须解决的问题。互联网上已经存在的各类用户评论信息中蕴含着大量的可再开发的知识资源,包括用户的个人信息、选择偏好和消费习惯等,有助于解决"信息过载"问题。[方法/过程]文章通过对豆瓣电影评论信息进行细粒度的情感分析进而有效地获取集体智慧,并且利用评论挖掘技术发掘用户的偏好,为用户选择产品提供更加有效的推荐策略。[结果/结论]实验表明,将大众智慧与个性化服务两者有机地结合起来,能够真实地反映出不同用户对于电影的感受特性,并为用户观影提供更加合理的参考。},
  language = {中文;},
  number = {08},
  journal = {情报理论与实践},
  author = {姜, 霖 and 张, 麒麟},
  year = {2017},
  keywords = {information overload,opinion mining,personalized recommendation,sentimental analysis,user review,个性化推荐,信息过载,情感分析,用户评论,评论挖掘},
  pages = {99-104},
  file = {/Users/sean10/zotero/storage/KJBCEENS/姜 and 张 - 2017 - 基于评论情感分析的个性化推荐策略研究——以豆瓣影评为例.pdf}
}

@article{ghoseDesigningNovelReview,
  title = {Designing {{Novel Review Ranking Systems}}: {{Predicting Usefulness}} and {{Impact}} of {{Reviews}}},
  abstract = {With the rapid growth of the Internet, users' ability to publish content has created active electronic communities that provide a wealth of product information. Consumers naturally gravitate to reading reviews in order to decide whether to buy a product. However, the high volume of reviews that are typically published for a single product makes it harder for individuals to locate the best reviews and understand the true underlying quality of a product based on the reviews. Similarly, the manufacturer of a product needs to identify the reviews that influence the customer base, and examine the content of these reviews. In this paper we propose two ranking mechanisms for ranking product reviews: a consumer-oriented ranking mechanism ranks the reviews according to their expected helpfulness, and a manufactureroriented ranking mechanism ranks the reviews according to their expected effect on sales. Our ranking mechanism combines econometric analysis with text mining techniques and with subjectivity analysis in particular. We show that subjectivity analysis can give useful clues about the helpfulness of a review and about its impact on sales. Our results can have several implications for the market design of online opinion forums.},
  author = {Ghose, Anindya and Ipeirotis, Panagiotis G},
  pages = {7},
  file = {/Users/sean10/zotero/storage/2BVMWTB4/Ghose and Ipeirotis - Designing Novel Review Ranking Systems Predicting.pdf}
}

@article{yinguopengWangLuoSheQuZaiXianPingLunYouYongXingYingXiangMoXingYanJiuJiYuXinXiCaiNaYuSheHuiWangLuoShiJiao2012,
  title = {{网络社区在线评论有用性影响模型研究\textemdash\textemdash{}基于信息采纳与社会网络视角}},
  issn = {0252-3116},
  lccn = {11-1541/G2},
  language = {Chinese},
  number = {16},
  journal = {图书情报工作},
  author = {殷国鹏 and 刘雯雯 and 祝珊},
  year = {2012},
  keywords = {online community helpfulness of online review information adoption theory reviewer characteristics social network,信息采纳理论,在线评论有用性,社会网络,网络社区,评论者特征},
  pages = {140-147},
  file = {/Users/sean10/zotero/storage/ALYRTTXZ/殷 et al_2012_网络社区在线评论有用性影响模型研究——基于信息采纳与社会网络视角.pdf}
}

@article{mudambiResearchNoteWhat2010,
  title = {Research {{Note}}: {{What Makes}} a {{Helpful Online Review}}? {{A Study}} of {{Customer Reviews}} on {{Amazon}}.Com},
  volume = {34},
  issn = {02767783},
  shorttitle = {Research {{Note}}},
  doi = {10.2307/20721420},
  abstract = {Customer reviews are increasingly available online for a wide range of products and services. They supplement other information provided by electronic storefronts such as product descriptions, reviews from experts, and personalized advice generated by automated recommendation systems.},
  number = {1},
  journal = {MIS Quarterly},
  author = {{Mudambi} and {Schuff}},
  year = {2010},
  pages = {185},
  file = {/Users/sean10/zotero/storage/5D84IVLC/Mudambi and Schuff - 2010 - Research Note What Makes a Helpful Online Review.pdf}
}

@misc{ZaiXianShangPinPingLunYouYongXingYingXiangYinSuYanJiuJiYuWenBenYuYiShiJiaoZhongGuoZhiWang,
  title = {在线商品评论有用性影响因素研究:基于文本语义视角 - 中国知网},
  howpublished = {http://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFQ\&dbname=CJFD2012\&filename=TSQB201210024\&uid=WEEvREcwSlJHSldRa1FhdXNXa0hHRXEyNkwyWTVkbG4wREwrQ2NLbGlTYz0=\$9A4hF\_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!\&v=MTg4NzVUcldNMUZyQ1VSTEtmWk9kb0ZDcmhVTHZOTVQ3YWJMRzRIOVBOcjQ5SFlJUjhlWDFMdXhZUzdEaDFUM3E=},
  file = {/Users/sean10/zotero/storage/BZY6VVRV/在线商品评论有用性影响因素研究基于文本语义视角 - 中国知网.pdf;/Users/sean10/zotero/storage/A4G5RNSV/detail.html}
}

@inproceedings{turneyThumbsThumbsSemantic2001,
  title = {Thumbs up or Thumbs down?: Semantic Orientation Applied to Unsupervised Classification of Reviews},
  shorttitle = {Thumbs up or Thumbs Down?},
  doi = {10.3115/1073083.1073153},
  abstract = {This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., ``subtle nuances'') and a negative semantic orientation when it has bad associations (e.g., ``very cavalier''). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word ``excellent'' minus the mutual information between the given phrase and the word ``poor''. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74\% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84\% for automobile reviews to 66\% for movie reviews.},
  language = {en},
  publisher = {{Association for Computational Linguistics}},
  author = {Turney, Peter D.},
  year = {2001},
  pages = {417},
  file = {/Users/sean10/zotero/storage/XHF2B5IE/Turney - 2001 - Thumbs up or thumbs down semantic orientation ap.pdf}
}

@misc{Cs0212032Thumbs,
  title = {[Cs/0212032] {{Thumbs Up}} or {{Thumbs Down}}? {{Semantic Orientation Applied}} to {{Unsupervised Classification}} of {{Reviews}}},
  howpublished = {https://arxiv.org/abs/cs/0212032}
}

@inproceedings{zhuangMovieReviewMining2006,
  title = {Movie Review Mining and Summarization},
  abstract = {With the flourish of the Web, online review is becoming a more and more useful and important information resource for people. As a result, automatic review mining and summarization has become a hot research topic recently. Different from traditional text summarization, review mining and summarization aims at extracting the features on which the reviewers express their opinions and determining whether the opinions are positive or negative. In this paper, we focus on a specific domain \textendash{} movie review. A multi-knowledge based approach is proposed, which integrates WordNet, statistical analysis and movie knowledge. The experimental results show the effectiveness of the proposed approach in movie review mining and summarization.},
  booktitle = {In {{Proceedings}} of the {{International Conference}} on {{Information}} and {{Knowledge Management}} ({{CIKM}}},
  author = {Zhuang, Li and Jing, Feng and Zhu, Xiao-yan},
  year = {2006},
  file = {/Users/sean10/zotero/storage/N5U79JFV/Zhuang et al_2006_Movie review mining and summarization.pdf;/Users/sean10/zotero/storage/ETN45N9I/summary.html}
}

@article{duanOnlineReviewsMatter2008,
  title = {Do Online Reviews Matter? \textemdash{} {{An}} Empirical Investigation of Panel Data},
  volume = {45},
  issn = {01679236},
  shorttitle = {Do Online Reviews Matter?},
  doi = {10.1016/j.dss.2008.04.001},
  abstract = {This study examines the persuasive effect and awareness effect of online user reviews on movies' daily box office performance. In contrast to earlier studies that take online user reviews as an exogenous factor, we consider reviews both influencing and influenced by movie sales. The consideration of the endogenous nature of online user reviews significantly changes the analysis. Our result shows that the rating of online user reviews has no significant impact on movies' box office revenues after accounting for the endogeneity, indicating that online user reviews have little persuasive effect on consumer purchase decisions. Nevertheless, we find that box office sales are significantly influenced by the volume of online posting, suggesting the importance of awareness effect. The finding of awareness effect for online user reviews is surprising as online reviews under the analysis are posted to the same website and are not expected to increase product awareness. We attribute the effect to online user reviews as an indicator of the intensity of underlying word-of-mouth that plays a dominant role in driving box office revenues.},
  language = {en},
  number = {4},
  journal = {Decision Support Systems},
  author = {Duan, Wenjing and Gu, Bin and Whinston, Andrew B.},
  month = nov,
  year = {2008},
  pages = {1007-1016},
  file = {/Users/sean10/zotero/storage/ZHA36H9A/Duan et al. - 2008 - Do online reviews matter — An empirical investiga.pdf}
}

@article{haoJiYuDianYingMianBanShuJuDeZaiXianPingLunQingGanQingXiangDuiXiaoShouShouRuYingXiangDeShiZhengYanJiu2009,
  title = {{基于电影面板数据的在线评论情感倾向对销售收入影响的实证研究}},
  issn = {1003-1952},
  lccn = {11-5057/F},
  abstract = {在面板数据环境下分析了商品在线评论情感倾向与商品销售收入的关系,以揭示在线口碑劝说作用对消费者总体购买行为的影响机理。本文以电影行业为背景,基于面板数据环境,研究了不同情感倾向的影评是否影响电影票房收入以及在电影发布后的什么阶段存在影响,并进一步比较了各情感等级的评论影响效应的差异性。分析结果表明,仅在电影发布后第3周,在线影评的情感倾向对电影票房收入存在显著影响,且这种影响超过在线影评数量的影响,口碑的劝说功能发挥主要作用;极端好评(5星评论)的正向影响大于极端差评(1星评论)的负向影响,而中评(2-4星评论)没有显著影响。},
  language = {中文;},
  number = {10},
  journal = {管理评论},
  author = {郝, 媛媛 and 邹, 鹏 and 李, 一军 and 叶, 强},
  year = {2009},
  keywords = {online review;word-of-mouth;sentimental orientation;panel data;box-office income,口碑,在线评论,情感倾向,票房收入,面板数据},
  pages = {95-103},
  file = {/Users/sean10/zotero/storage/86WJ9PWP/郝 et al_2009_基于电影面板数据的在线评论情感倾向对销售收入影响的实证研究.pdf;/Users/sean10/zotero/storage/S8V9EY9D/郝 et al_2009_基于电影面板数据的在线评论情感倾向对销售收入影响的实证研究.pdf}
}

@phdthesis{shenyaoZhongGuoDianYingZaiXianPiaoWuFaZhanYanJiu2016,
  address = {北京},
  title = {{中国电影在线票务发展研究}},
  language = {Chinese},
  school = {中国电影艺术研究中心},
  author = {沈尧},
  year = {2016},
  file = {/Users/sean10/zotero/storage/47BMZTXA/中国电影在线票务发展研究 - 中国知网.pdf;/Users/sean10/zotero/storage/Q9SR8NWS/detail.html}
}

@phdthesis{zhangJiYuShenDuShenJingWangLuoDeWeiBoDuanWenBenQingGanFenXiYanJiu2017,
  type = {{硕士}},
  title = {{基于深度神经网络的微博短文本情感分析研究}},
  abstract = {近年来,随着社交网络的逐渐成熟和移动终端技术的迅猛发展,微博作为一种网络传播的主要媒体形式,越来越受到人们的青睐。用户通过在微博上表达观点传播思想,抒发个人情感的同时,也产生了大量带有个人主观情感特征的信息,这些信息中包含着不同趋向的情感特征,进而对网络舆情的传播能产生巨大的影响。本文使用深度学习的方法,对互联网上微博短文本的情感分析问题进行了相关研究。具体研究内容如下:(1)为了更好的判定微博短文本的情感极性,提出一种基于深度卷积神经网络模型的情感分类方法。该方法首先将训练的词向量作为原始特征向量,然后把特征向量送入卷积神经网络(CNNs,Convolutional Neural Networks)模型进一步提取特征,训练出基于该网络的情感分类模型,再使用该分类器对互联网短文本进行情感分类。实验比较了基于传统机器学习的SVM算法与深度学习的随机生成向量的CNNs模型法和本文提出的方法,最终通过实验结果证明了采用本文方法可以有效的进行情感分类。(2)针对微博短文本中评价对象抽取的问题,提出了一种基于双向长短时记忆循环神经网络(Bidirectional Long Short-Term Memory,BLSTM)模型的情感要素抽取方法。通过实验对比传统机器学习模型与循环神经网络(RNN,Recurrent Neural Networks)、长短时循环神经记忆网络(Long Short-Term Memory,LSTM)和双向长短时记忆循环神经网络这三种深度学习模型发现,采用基于深度学习的双向长短时记忆循环神经网络模型处理评价对象抽取任务可以获得最佳效果。},
  language = {中文;},
  school = {中原工学院},
  author = {张, 英},
  year = {2017},
  keywords = {情感分析,Analysis Of The Emotion,Deep Learning,Emotion Classification,Evaluation Object Extraction,Microblog Short Text,微博短文本,情感分类,深度学习,评价对象抽取},
  file = {/Users/sean10/zotero/storage/JLYZLVVW/张_2017_基于深度神经网络的微博短文本情感分析研究.pdf}
}

@article{huangzhaotingYiChongJiYuYingPingWenBenQingGanFenXiDeDianYingTeZhengMoXing2017,
  title = {{一种基于影评文本情感分析的电影特征模型}},
  issn = {1673-3215},
  lccn = {11-5336/TB},
  abstract = {鉴于网络影评已成为人们观影选择时的重要参考指标,如何能够利用影评数据挖掘出观众的观影感受,为其他用户提供可量化的参考和推荐,是本文试图解决的问题。本文通过对电影的评论进行文本情感分析和建模,分析出每一部电影的情感特征,并使用这一情感特征来进行影片聚类分析。在IMDB电影评论数据集上进行测试表明,此方法取得了较好的结果。},
  language = {中文;},
  number = {09},
  journal = {现代电影技术},
  author = {黄昭婷 and 刘媛 and 丁鑫},
  year = {2017},
  keywords = {情感建模,文本情感分析,聚类分析},
  pages = {26-30},
  file = {/Users/sean10/zotero/storage/7S5T87AG/黄 et al_2017_一种基于影评文本情感分析的电影特征模型.pdf}
}

@phdthesis{likeJiYuDuoYuanTeZhengRongHeHeLSTMShenJingWangLuoDeZhongWenPingLunQingGanFenXi2017,
  address = {太原},
  type = {{硕士}},
  title = {{基于多元特征融合和LSTM神经网络的中文评论情感分析}},
  abstract = {随着移动互联网的飞速发展,网购成了人们日常生活的一部分。电商网站上存在大量的产品评论信息。挖掘这些评论的情感倾向不仅可以为商家提供商品的各种信息,方便商家做出销售决策,也有利于买家对商品做出客观判断,从而做出购买决策。面对数量庞大的评论文本信息,仅靠人工浏览去获取评论的情感倾向费时且费力,如何利用人工智能领域中的相关技术对产品评论自动化地进行情感分析成为了一个重要且有意义的课题。现有的情感分析方法主要有基于规则的方法、基于机器学习的方法和基于深度神经网络的方法,随着大数据技术的发展以及语言的形式越来越多元化,深度神经网络技术成为了自然语言处理领域的主流技术,在情感分析领域也取得了很大的突破,本文主要研究基于深度神经网络的情感分析方法。本文的主要研究工作如下:(1)针对文本情感分析中对文本表示时遇到的维度过高和语义不相关的问题,本文采用word embedding机制,通过神经网络语言模型对大量评论文本进行训练,并在此基础上通过distributed representation的方式表示文本,从而将文本映射为一个低维实数向量。这种文本表示方法同时也可以表达文本的语义信息,有助于神经网络模型对文本更好地理解。(2)针对情感分析任务的特殊性,本文提出了一种新的文本表示方法-\textemdash\textemdash{}多元特征词向量。这种表示方法是对distributed representation表示方法的优化。考虑到情感分析中含有情感要素的词对文本整体情感极性的影响,通过构建情感要素词典捕捉文本中含有情感要素的词,并通过构造词的情感特征向量来表达词的情感要素,接着与用distributed representation方式表示的词向量进行特征融合构成多元特征词向量。用多元特征词向量表示的文本不仅含有文本的语义信息,而且可以捕捉文本的情感特征,更适合情感分析任务。(3)情感分析的本质是一个分类问题,计算特征权重是分类问题的重要步骤,基于此理论,本文在提出的多元特征词向量的基础上,进一步对其优化,借鉴特征权重算法为多元特征词向量分配权重,从而突出对分类更重要的词。本文提出的基于权重分配的多元特征词向量的文本表示方法对传统的文本表示方法从两方面进行了改进,丰富了对文本语义的表达,将其作为神经网络分类模型的输入,更适合神经网络模型对文本进行深层次特征捕捉与情感分类。(4)本文使用LSTM神经网络模型挖掘文本的深层特征。用基于权重分配的多元特征词向量表示文本,并作为LSTM神经网络模型的输入,然后利用LSTM神经网络能够学习文本中远距离依赖的特性捕捉文本的序列特征及上下文的依赖关系。最后本文通过和传统的基于LSTM神经网络的情感分析方法做对比实验,验证本文提出的改进方案的有效性。在上述四个工作中,本文充分考虑情感分析任务的特性,将情感词典资源以及特征权重信息等先验知识引入神经网络模型,在此基础上提出的基于权重分配的多元特征词向量可以捕捉更适用于情感分析任务的特征,利用LSTM神经网络模型的特性可以捕捉更丰富的特征组合,从而有效提高情感分类模型对文本的理解以及情感分类的准确率。},
  language = {中文;},
  school = {太原理工大学},
  author = {李科},
  year = {2017},
  keywords = {情感分析,emotional feature,LSTM neural network,LSTM神经网络,sentiment analysis,weight information,word vector,多元特征,情感特征,权重信息},
  file = {/Users/sean10/zotero/storage/QYQJ9QIB/李_2017_基于多元特征融合和LSTM神经网络的中文评论情感分析.pdf}
}

@phdthesis{yangMianXiangSheJiaoMeiTiDeWenBenQingGanFenXiGuanJianJiZhuYanJiu2016,
  type = {{博士}},
  title = {{面向社交媒体的文本情感分析关键技术研究}},
  abstract = {近年来社交媒体蓬勃且迅速的发展,使其成为了全世界最大规模的公共数据源。文本作为社交媒体中一种重要的信息载体,有着指数级增长趋势且蕴含着极其丰富的研究与商业价值。文本情感分析作为分析文本的主要技术之一,可作为分析社交媒体文本的必要手段及技术支持,因此面向社交媒体文本的情感分析技术应运而生。面向社交媒体文本的情感分析主要包含情感信息检索、情感信息抽取、情感分类、情感归纳四大任务,实现对用户在各种网络平台中所表达观点、倾向等主观性文本进行分析与挖掘,进而获取用户所生成文本的倾向性和话题等重要信息,以辅助不同用户、研究者、商业组织及政府机构的决策需要。本文结合社交媒体文本情感分析的任务,分别从如下几个方面进行了创新性研究,具体完成的工作主要包括：1、针对社交媒体文本中的关键信息抽取问题,本文研究了如何从产品评论语料中准确且高效地抽取产品属性的信息,提出了将同一句子内部的局部信息与不同句子间的全局信息进行融合的方法,实现了对产品评论语料中的产品属性抽取；然后在依据其重要性对产品属性加以归并及排序,从而发现产品中的重要属性。实验结果表明本文提出的融合全局信息与局部信息的产品属性抽取方法适用于产品属性抽取任务,并能在一定程度上提高产品属性抽取的精度。2、针对社交媒体中文本情感复杂多样的特点,本文研究了如何将认知思维模式引入社交媒体文本情感分析中,并用于指导情感分类任务。对于不同民族和区域所存在的思维模式差异现象,及其所引发的语言与情感表达的多样化问题,本文提出了度量思维模式差异的具体模型,并利用思维模式差异模型来指导社交媒体文本情感分类,分别以中英文语料作为实验对象,实验结果表明在中英文语料中考虑各自思维模式特点有助于提高社交媒体文本情感分类的精度。3、针对社交媒体平台中层出不断的热点事件,本文研究了如何从海量微博信息中发现热点事件,并以新浪微博作为研究对象。依据当微博平台中热点事件出现时,用户的情感发生波动,所发微博中情感词数量增多的现象,提出了情感分布语言模型。通过分析相邻时段间情感分布语言模型的差异,实现了对微博平台中热点事件的发现。实验结果表明本文提出的方法可以有效地从微博平台中发现热点事件,并且有助于对微博平台中热点事件的管理和监控。4、针对社交媒体平台中的海量信息缺乏归纳的问题,本文以新浪微博作为研究对象,提出了一种基于权重量化的微博热点事件摘要生成方法。该方法融合微博信息熵、话题的重要性及微博在话题中的重要性等因素,并以话题中转发度最高的微博为中心,结合停用词比例及与中心微博的编辑距离等统计特征对候选微博进行排序,从而完成热点事件摘要生成的任务。在新浪微博数据集上的实验结果表明本文提出的方法可有效地对微博平台中的热点事件进行摘要生成,可以用于对新浪微博平台中的热点事件摘要生成与监管。},
  language = {中文;},
  school = {大连理工大学},
  author = {杨, 亮},
  year = {2016},
  keywords = {情感分析,Attribute extraction,Opinion mining,Sentiment analysis,Topic detection,Topic summarization,属性抽取,观点挖掘,话题发现,话题摘要},
  file = {/Users/sean10/zotero/storage/JEHSXL3X/杨_2016_面向社交媒体的文本情感分析关键技术研究.pdf}
}

@phdthesis{caiWangLuoPingLunWenBenDeXiLiDuQingGanFenXiYanJiu2017,
  type = {{硕士}},
  title = {{网络评论文本的细粒度情感分析研究}},
  abstract = {随着网络评论文本的爆炸式增长,评论中承载了大量的用户情感信息,分析评论的整体倾向性已经不能满足当前用户的需求,迫切需要更细粒度属性层面的情感分析,并且由于用户表达随意性造成的分词准确率过低,情感要素抽取准确率低和隐式情感信息丢失等问题也急需解决。本文首先对垃圾评论过滤和中文分词两种文本预处理任务进行了分析;其次基于CRFs模型对情感要素进行抽取,补充隐式情感对象后聚合处理;然后提出一种对聚合后特征类的对立观点进行情感强度分析的算法。本文研究内容有以下四个部分:(1)针对文本预处理问题,基于构建的评论特征分类来识别垃圾评论,并构建用户词典改善中文分词本文首先基于构建的评论特征进行文本分类,包括主客观文本分类,过滤掉垃圾观点信息评论数据,保留真实有价值的评论文本信息进行情感分析任务,并进行意群划分,便于后续语义情感聚合处理;中文分词采用NLPIR分词系统,基于新词、网络词汇和领域术语类关键词等未登录词构建用户词典,既可以纠正分词错误,提高情感对象抽取的准确率,又可以作为情感词典的补充,减少用户情感信息的丢失。(2)基于CRFs模型抽取情感要素,将情感对象、情感词及情感修饰词的联合识别任务转化为结构化序列标注任务采用条件随机场模型联合识别情感要素,首先选取特征构建特征模板和标注集,然后基于CRFs联合识别情感要素,利用显式情感对象-情感词对和评论语料中标签集组成的产品特征观点对构建训练文档,采用朴素贝叶斯分类器识别隐式情感对象,最后通过词义代码实现情感对象聚合,改进特征稀疏性问题。(3)提出了基于语境情感消岐的对立观点情感强度分析算法本文首先依据情感词的动态极性定义了情感歧义词,利用关联规则挖掘情感歧义词语搭配集,PMI剪枝过滤后构建出情感歧义词搭配词典,然后介绍了构建的网络词典及情感修饰词典等,提出了对立观点情感强度计算的方法,最后依据情感强度生成对立观点情感摘要完成细粒度情感分析,实验表明了本文词典构建及情感强度计算方法的有效性。(4)设计并实现了评论文本细粒度情感分析系统本文实现了细粒度情感分析系统,该系统各功能模可以完成评论采集、垃圾评论过滤、中文分词、情感要素抽取和细粒度情感分析全过程,并最终提供给用户直观的包含对立观点强度信息的细粒度分析结果。},
  language = {中文;},
  school = {山东师范大学},
  author = {蔡, 肖红},
  year = {2017},
  keywords = {CRFs,fine-grained,sentiment ambiguous,sentiment element,sentiment lexicon,情感歧义,情感要素,情感词典,细粒度},
  file = {/Users/sean10/zotero/storage/UFGPRG7C/蔡_2017_网络评论文本的细粒度情感分析研究.pdf}
}

@phdthesis{xieJiYuBuJunHengShuJuJiDeWenBenFenLeiSuanFaYanJiu2013,
  type = {{硕士}},
  title = {{基于不均衡数据集的文本分类算法研究}},
  abstract = {随着计算机网络技术的不断发展，电子文档逐渐成为人们获取文本信息的主要渠道。网络信息的多样性和文档的杂乱无章性对用户快速准确获取所需信息提出了巨大的挑战。文本分类是信息检索中对文档进行组织和整理的重要技术，然而不同于文本分类系统在实验室中处理的文本集合，在实际应用中尤其是出现在网络中的文本数据集合往往存在标注不全，数据集不平衡等问题。数据不均衡问题由于其在各领域应用的广泛性和重要性成为文本分类目前面临的一个主要问题，同时也是文本挖掘领域的研究热点。 	本文对不均衡数据集下的文本分类进行了一定的研究，从文本分类的特征选择方法和对量化后的文本数据层上的重取样两个角度出发提出了一种组合的针对不均衡数据集的文本分类方法。本文的主要研究内容如下： 	\ding{172}对文本分类中的传统CHI统计特征选择方法和对改进的仅保留类别正相关性特征的CHI统计特征选择算法进行了研究，并在不均衡数据集上进行了实验，实验结果表明在均衡数据集上表现良好的CHI统计特征选择方法所得到的分类效果并不理想。 	\ding{173}通过对不均衡数据集的研究分析，提出了对仅保留类别正相关性特征的单边CHI统计特征选择方法进行改进，首先引入一个小类加权因子用于保留部分对小类分类有贡献的表现为类别负相关性的特征词项，同时使用ICF(逆转类别频)增强特征选择过程中特征的类别区分能力，进而选择出最具类别代表性的特征词。使用特征集合将文档进行量化表示为向量空间模型。 	\ding{174}为更好地解决由数据不均衡导致分类效果不佳的问题，本文提出对量化后的文本集合在数据层上进行重取样处理。首先采用随机上采样和随机下采样结合的重取样方法，该方法可以很好的实现对文本数据集不平衡性的过滤，获得相对均衡的数据集用于分类器的训练。但由于随机上采样算法极易产生分类过拟合问题，而且随机下采样容易删除掉一些对分类贡献较大的样本。所以进一步对数据重取样方法进行改进，采用表现较好的SMOTE上采样方法和基于改进聚类的下采样相结合的重取样方法对文本数据集合进行处理，取得了较好的分类结果。},
  language = {中文;},
  school = {重庆大学},
  author = {谢, 娜娜},
  year = {2013},
  keywords = {CHI-square selection method,CHI统计特征选择,data distribution,Imbalanced data sets,re-sampling,text classification,不均衡数据集,数据分布,文本分类,重取样},
  file = {/Users/sean10/zotero/storage/4R3YN3TD/谢 - 2013 - 基于不均衡数据集的文本分类算法研究.pdf}
}

@phdthesis{lidanJiYuChangDuanShiJiYiWangLuoDeZhongWenWenBenQingGanFenXi2017,
  address = {北京},
  type = {{硕士}},
  title = {{基于长短时记忆网络的中文文本情感分析}},
  abstract = {目前,互联网包括社交网站、电子商务网站以及论坛等网站已经得到了快速发展,大量的用户可以随时随地的进行评论文本信息的发布、分享和更新。在多种多样的信息中,商品评论信息是非常重要的一类。电子商务的快速发展,带动了许多用户习惯在网上进行购物消费,同时,这些网站也为消费者提供了发表评论的平台。商家通过收集这些评论信息进行一系列分析,更好的为顾客服务。其中,文本情感分析是从评论中获取有用信息的重要部分,了解商品评论的情感趋向更好的改进商品的质量或销售策略,从而满足用户的喜好,促进产品销售。传统迭代神经网络(Recurrent neural network,简称RNN)模型虽然在理论上可以覆盖文本中整个句子的时序信息,但实践表明,由于梯度消失的原因,RNN语言模型无法识别较长文本的句子。本文针对传统RNN模型中存在的问题,尝试使用长短时记忆网络(Long Short-Term Memory,简称LSTM)模型的思路加以改进。由于LSTM模型可以保存较长文本的信息,在中文文本情感分析中发挥语言模型的优势,所以本文将使用LSTM模型作为语言模型对中文文本进行情感分析,LSTM语言模型可以有效的获取文本完整的时序信息,通过加入门的结构,来去除或者增加信息交互的能力。LSTM模型相比传统的RNN模型在实验效果上有了一定的改善。最后,本文将LSTM模型与其他两种模型进行融合,进一步提高模型对文本情感分析的准确率。},
  language = {Chinese},
  school = {北京邮电大学},
  author = {李丹},
  year = {2017},
  keywords = {情感分析,Emotion analysis,LSTM,neural network,神经网络},
  file = {/Users/sean10/zotero/storage/5IR9HDFM/李 - 2017 - 基于长短时记忆网络的中文文本情感分析.pdf;/Users/sean10/zotero/storage/CLQVLVFQ/李_2017_基于长短时记忆网络的中文文本情感分析.pdf}
}

@article{zhangJiYuWord2vecDeWeiBoDuanWenBenFenLeiYanJiu2017,
  title = {{基于Word2vec的微博短文本分类研究}},
  issn = {1671-1122},
  lccn = {31-1859/TN},
  abstract = {随着微博等社会化媒体的信息量急剧膨胀,人们迫切需要实现这些信息的自动分类处理,以帮助用户快速查找所需信息和过滤垃圾信息。针对传统文本分类模型存在的特征维数灾难、无语义特征等问题,文章基于Word2vec模型对微博短文本进行了分类研究。鉴于Word2vec模型无法区分文本中词汇的重要程度,进一步引入TFIDF对Word2vec词向量进行加权,实现加权的Word2vec分类模型。最后合并加权Word2vec和TFIDF两种模型,实验结果表明合并后模型分类准确率高于加权Word2vec模型和使用TFIDF的传统文本分类模型。},
  language = {中文;},
  number = {01},
  journal = {信息网络安全},
  author = {张, 谦 and 高, 章敏 and 刘, 嘉勇},
  year = {2017},
  keywords = {short text classification,SVM,TFIDF,Word2vec,支持向量机,短文本分类},
  pages = {57-62},
  file = {/Users/sean10/zotero/storage/DRUE3RE2/张 et al. - 2017 - 基于Word2vec的微博短文本分类研究.pdf}
}

@article{zhangJiYuword2vecHeSVMperfDeZhongWenPingLunQingGanFenLeiYanJiu2016,
  title = {{基于word2vec和SVMperf的中文评论情感分类研究}},
  issn = {1002-137X},
  lccn = {50-1075/TP},
  abstract = {利用有监督的机器学习的方法来对中文产品评论文本进行情感分类,该方法结合了word2vec和SVMperf两种工具。先由word2vec训练出语料中每个词语的词向量,通过计算相互之间的余弦距离来达到相似概念词语聚类的目的,通过相似特征聚类将高相似度领域词汇扩充到情感词典;再使用word2vec训练出词向量的高维度表示;然后采用主成分分析方法(PCA)对高维度向量进行降低维度处理,形成特征向量;最后使用两种方法抽取有效的情感特征,由SVMperf进行训练和预测,从而完成文本的情感分类。实验结果表明,采用相似概念聚类方法对词典进行扩充任务或情感分类任务都可以获得很好的效果。},
  language = {中文;},
  number = {S1},
  journal = {计算机科学},
  author = {张, 冬雯 and 杨, 鹏飞 and 许, 云峰},
  year = {2016},
  keywords = {情感分类,Word2vec,PCA,Semantic features,Sentiment classification,SVMperf,word2vec,语义特征},
  pages = {418-421+447},
  file = {/Users/sean10/zotero/storage/BU8QFKZS/张 et al. - 2016 - 基于word2vec和SVMperf的中文评论情感分类研究.pdf}
}

@article{zhangYiChongGaiJinDeJiangZaoZiBianMaShenJingWangLuoBuPingHengShuJuFenLeiSuanFa2017,
  title = {{一种改进的降噪自编码神经网络不平衡数据分类算法}},
  issn = {1001-3695},
  lccn = {51-1196/TP},
  abstract = {针对少数类样本合成过采样技术(synthetic minority over-sampling technique,SMOTE)在合成少数类新样本时会带来噪声问题,提出了一种改进降噪自编码神经网络不平衡数据分类算法(SMOTE-SDAE)。该算法通过SMOTE方法合成少数类新样本以均衡原始数据集,考虑到合成样本过程中会产生噪声的影响,利用降噪自编码神经网络算法的逐层无监督降噪学习和有监督微调过程,有效实现对过采样数据集的降噪处理与数据分类。在UCI不平衡数据集上实验结果表明,相比传统SVM算法,该算法显著提高了不平衡数据集中少数类的分类精度。},
  language = {中文;},
  number = {05},
  journal = {计算机应用研究},
  author = {张, 成刚 and 宋, 佳智 and 姜, 静清 and 裴, 志利},
  year = {2017},
  keywords = {neural network,神经网络,classification,imbalanced data,over-sampling,不平衡数据,分类,过采样},
  pages = {1329-1332},
  file = {/Users/sean10/zotero/storage/BFHRLYR4/张 et al. - 2017 - 一种改进的降噪自编码神经网络不平衡数据分类算法.pdf}
}

@article{taoBuJunHengShuJuFenLeiSuanFaDeZongShu2013,
  title = {{不均衡数据分类算法的综述}},
  issn = {1673-825X},
  lccn = {50-1181/N},
  abstract = {传统的分类方法都是建立在类分布大致平衡这一假设基础上的,然而实际情况中,数据往往都是不均衡的。因此,传统分类器分类性能通常比较有限。从数据层面和算法层面对国内外分类算法做了详细而系统的概述。并通过仿真实验,比较了多种不平衡分类算法在6个不同数据集上的分类性能,发现改进的分类算法在整体性能上得到不同程度的提高,最后列出了不均衡数据分类发展还需解决的一些问题。},
  language = {中文;},
  number = {01},
  journal = {重庆邮电大学学报(自然科学版)},
  author = {陶, 新民 and 郝, 思媛 and 张, 冬雪 and 徐, 鹏},
  year = {2013},
  keywords = {classification performance,improved approaches,unbalanced data,不均衡数据,分类性能,改进算法},
  pages = {101-110+121},
  file = {/Users/sean10/zotero/storage/WVMG5N66/陶 et al. - 2013 - 不均衡数据分类算法的综述.pdf}
}

@article{guoYiChongJiYuSuiJiGADeTiGaoBPWangLuoFanHuaNengLiDeFangFa2014,
  title = {{一种基于随机GA的提高BP网络泛化能力的方法}},
  issn = {1673-629X},
  lccn = {61-1450/TP},
  abstract = {LM-BP网络对其初始权值和阈值敏感,泛化能力不强,针对该缺点,采用遗传算法(GA)对其初始权阈值进行优化,在一定程度上能提高LM-BP网络的泛化能力。为进一步扩展GA初始种群的覆盖范围,进一步提高LM-BP网络的泛化能力,采用多次随机产生初始种群多次优化的方法。以伦河孝感段氟化物含量为实例,建立随机GA的LM-BP网络模型,对原始数据进行拟合及测试,结果表明该方法基本能100\%拟合,测试误差不超过2.3\%。经过对比实验,证明了该方法的有效性。},
  language = {中文;},
  number = {01},
  journal = {计算机技术与发展},
  author = {郭, 海如 and 李, 志敏 and 万, 兴 and 熊, 斌},
  year = {2014},
  keywords = {neural network,神经网络,generalization capability,random genetic algorithm,test error,泛化能力,测试误差,随机遗传算法},
  pages = {105-108},
  file = {/Users/sean10/zotero/storage/U5LCJ2BX/郭 et al. - 2014 - 一种基于随机GA的提高BP网络泛化能力的方法.pdf}
}

@article{glorotDeepSparseRectifier,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  pages = {9},
  file = {/Users/sean10/zotero/storage/MUC4MYPL/Glorot et al. - Deep Sparse Rectiﬁer Neural Networks.pdf}
}

@article{salinasModelMultiplicativeNeural1996,
  title = {A Model of Multiplicative Neural Responses in Parietal Cortex.},
  volume = {93},
  issn = {0027-8424},
  abstract = {Visual responses of neurons in parietal area 7a are modulated by a combined eye and head position signal in a multiplicative manner. Neurons with multiplicative responses can act as powerful computational elements in neural networks. In the case of parietal cortex, multiplicative gain modulation appears to play a crucial role in the transformation of object locations from retinal to body-centered coordinates. It has proven difficult to uncover single-neuron mechanisms that account for neuronal multiplication. Here we show that multiplicative responses can arise in a network model through population effects. Specifically, neurons in a recurrently connected network with excitatory connections between similarly tuned neurons and inhibitory connections between differently tuned neurons can perform a product operation on additive synaptic inputs. The results suggest that parietal responses may be based on this architecture.},
  number = {21},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  author = {Salinas, E and Abbott, L F},
  month = oct,
  year = {1996},
  pages = {11956-11961},
  file = {/Users/sean10/zotero/storage/IMLDP9AB/Salinas and Abbott - 1996 - A model of multiplicative neural responses in pari.pdf},
  pmid = {8876244},
  pmcid = {PMC38165}
}

@misc{JiYuJuanJiShenJingWangLuoHeTreeLSTMDeWeiBoQingGanFenXiZhongGuoZhiWang,
  title = {{{基于卷积神经网络和Tree}}-{{LSTM的微博情感分析}} - 中国知网},
  howpublished = {http://kns.cnki.net/KCMS/detail/51.1196.TP.20180314.1729.014.html?uid=WEEvREcwSlJHSldRa1Fhb09jSnZqRWhGRjNLcnovTHlnM215QU1kY0d2az0=\$9A4hF\_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!\&v=MjM3MTVscVdNMENMTDdSN3FlYitacUZ5RG1VNzdQSkZvPUx6N1NaTEc0SDluTXJJNUJaT3NJWXc5TXptUm42ajU3VDNm},
  file = {/Users/sean10/zotero/storage/4P4V26FY/51.1196.TP.20180314.1729.014.html}
}

@article{jinzhigangYiChongJieHeShenDuXueXiHeJiChengXueXiDeQingGanFenXiMoXing2018,
  title = {{一种结合深度学习和集成学习的情感分析模型}},
  issn = {0367-6234},
  lccn = {23-1235/T},
  abstract = {随着社交媒体的不断发展,用户评价已成为网络决策的关键因素.为了准确分析社交媒体用户评价的情感倾向性,更好地推进舆情分析、推荐算法等工作,本文通过对Bi-LSTM模型和Bagging算法的改进,提出了一种新的情感分析模型\textemdash{}BiLSTMM-B模型.该模型的特点在于将深度学习模型可提取抽象特征的优势和集成学习多分类器共同决策的思想相结合.一方面在Bi-LSTM模型的基础上引入Maxout神经元,构建Bi-LSTMM模型,解决随机梯度下降算法中存在的梯度弥散问题,更好地优化训练过程.另一方面,模型基于Bagging算法训练多个情感分类器,根据分类器性能优劣利用袋外数据为每个分类器分配指定类别的权重,并提出相应的改进投票策略,增强了模型的泛化能力.实验结果表明:本文提出的Bi-LSTMM-B模型相比于传统的LSTM模型准确率提高12.08\%,其中Maxout神经元的引入对情感分析准确率有8.28\%的相对改善效果,改进后的投票策略对准确率有4.06\%的相对改善效果,并在召回率和F值两项指标上均优于其他对比模型.由此证明,深度学习模型和集成学习思想相结合可提高情感分析的准确率,并具有一定的研究价值.},
  language = {中文},
  number = {11},
  journal = {哈尔滨工业大学学报},
  author = {金志刚 and 韩玥 and 朱琦},
  year = {2018},
  keywords = {深度学习,Bagging算法,Bi-LSTM模型,Maxout神经元,情感倾向性分析,集成学习},
  pages = {1-8},
  file = {/Users/sean10/zotero/storage/H4NDE49L/金志刚 et al_2018_一种结合深度学习和集成学习的情感分析模型.pdf}
}

@inproceedings{ghoshDocumentModelingHierarchical2018,
  address = {New York, NY, USA},
  series = {ICDSP 2018},
  title = {Document {{Modeling}} with {{Hierarchical Deep Learning Approach}} for {{Sentiment Classification}}},
  isbn = {978-1-4503-6402-7},
  doi = {10.1145/3193025.3193046},
  abstract = {Sentiment analysis has recently been considered as most active research field in NLP domain. Deep learning is a growing trend of machine learning due to its automatic learning capability with impressive results across different NLP task. In this paper a model is proposed to analyze the deep sentiment representation based on CNN and LSTM (modified version of RNN) network. We aim to improve the performance of traditional machine learning method by merging them with deep learning techniques to tackle the challenge of sentiment prediction of massive amount of unsupervised product review dataset. We make our model first learn to sentence representation with CNN. Next, the semantics of sentences are encoded with LSTM network for document representation. We conduct experiments on two review datasets based on movie review with evaluation metric 'accuracy'. The result shows that proposed model outperformed traditional machine learning as well as baseline neural network model},
  booktitle = {Proceedings of the {{2Nd International Conference}} on {{Digital Signal Processing}}},
  publisher = {{ACM}},
  author = {Ghosh, Monalisa and Sanyal, Goutam},
  year = {2018},
  keywords = {LSTM,Convolutional neural network (CNN),Deep learning,embedding algorithm etc,recurrent neural networks,Sentiment Analysis,traditional machine learning},
  pages = {181--185},
  file = {/Users/sean10/zotero/storage/PLWX3TJD/Ghosh_Sanyal_2018_Document Modeling with Hierarchical Deep Learning Approach for Sentiment.pdf}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long Short-Term Memory},
  volume = {9},
  issn = {1530-888X(Electronic),0899-7667(Print)},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Introduces a novel, efficient, gradient-based method called long short-term memory (LSTM) in conjunction with an appropriate gradient-based learning algorithm. LSTM can learn to bridge minimal time lags in excess of 1,000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time. Experiments with artificial data involve local distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. An appendix contains a detailed description of the algorithm and explicit error flow formulas. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  number = {8},
  journal = {Neural Computation},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  keywords = {Algorithms,Automated Information Storage,Neural Networks},
  pages = {1735-1780},
  file = {/Users/sean10/zotero/storage/QRPQGFPI/Long Short-Term Memory.pdf;/Users/sean10/zotero/storage/88KWI8BL/1997-43185-002.html}
}

@article{zarembaRecurrentNeuralNetwork2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.2329},
  primaryClass = {cs},
  title = {Recurrent {{Neural Network Regularization}}},
  abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
  journal = {arXiv:1409.2329 [cs]},
  author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  month = sep,
  year = {2014},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/sean10/zotero/storage/BMRBB8RU/Zaremba et al_2014_Recurrent Neural Network Regularization.pdf;/Users/sean10/zotero/storage/MICQ43AQ/1409.html}
}

@article{liptonCriticalReviewRecurrent2015,
  title = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
  language = {en},
  author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
  month = may,
  year = {2015},
  file = {/Users/sean10/zotero/storage/ZHYLX2RR/Lipton et al_2015_A Critical Review of Recurrent Neural Networks for Sequence Learning.pdf;/Users/sean10/zotero/storage/VLLRZB56/1506.html}
}

@article{liujianxingJiYuShenDuShuangXiangChangDuanShiJiYiWangLuoDeWenBenQingGanFenLeiZhongGuoZhiWang,
  title = {基于深度双向长短时记忆网络的文本情感分类 - 中国知网},
  abstract = {针对文本情感分类中浅层统计特征忽略了文本内容的序列顺序的问题,提出了一个基于深度双向长短时记忆循环神经网络(DB-LSTM-RNN)的情感分析预测模型。用词嵌入的方法学习文本的分布式表示,并将这些表示作为预训练的向量,用深度双向长短时记忆网络模型进行序列学习,将该结构中学习到的深层表示输入到机器学习分类器中进行情感分类。实验结果表明,该模型比基于浅层统计特征的方法提高了7.6\%的准确率。},
  author = {刘建兴 and 蔡国永 and 吕光瑞 and 毕梦莹},
  file = {/Users/sean10/zotero/storage/VIDE8YZK/刘建兴 et al_基于深度双向长短时记忆网络的文本情感分类 - 中国知网.pdf;/Users/sean10/zotero/storage/RDLRWHH5/45.1351.TN.20180514.1348.016.html}
}

@phdthesis{zhangyaoJiHuoHanShuDaoXiangDeRNNSuanFaYouHua2017,
  type = {{硕士}},
  title = {{激活函数导向的RNN算法优化}},
  abstract = {循环神经网络(RecurrentNeuralNetworks,RNN)是人工神经网络的一个重要分支。它在隐含层引入了反馈机制,实现对序列数据的有效处理。循环神经网络具有存储和处理上下文信息的强大能力,成为语音识别、自然语言处理、计算机视觉等领域的研究热点之一。一方面,循环神经网络普遍采用S型函数作为激活函数,而S型函数的饱和区限制了RNN训练收敛速度,因此对激活函数的优化研究成为研究热点。另一方面,循环神经网络主要采用软件实现的方式,算法的硬件加速研究具有重要意义。本文针对上述问题和研究背景,在前人的研究基础上做了如下工作:循环神经网络的理论总结研究。长短时记忆单元((Long Short-Term Memory,LSTM)特有的门结构解决了传统循环神经网络时间维度的梯度消失问题,成为RNN结构的重要组成部分。分析了 LSTM型RNN的训练过程,包括前向传播过程和反向传播过程。在反向传播过程中,激活函数及其导数直接影响网络训练的收敛速度。从激活函数方面着手对循环神经网络算法进行优化。传统的S型激活函数存在饱和区导致收敛速度慢,前人提出的修正线性单元避免了饱和区梯度消失问题,但是带来了梯度爆炸问题。利用S型函数系数不同,非饱和区范围不同,进一步分析了不同系数之间的训练收敛速度的大小关系。通过实验证明了扩展非饱和区的优化方法有效地加快了训练收敛速度。从激活函数的硬件实现着手对循环神经网络算法进行优化,激活函数的硬件实现难度较大,具有更重要的研究意义。优化误差的研究,引入了拟合直线误差修正项,在硬件开销不变的前提下,误差变为原来的二分之一;优化分割方法的研究,调整不同子区间的分割段数,在硬件开销不变的前提下,使得误差进一步减小;激活函数硬件实现的可扩展性研究,基于Sigmoid函数的硬件实现,实现了参数化Sigmoid函数和Tanh函数。},
  language = {中文;},
  school = {浙江大学},
  author = {张尧},
  year = {2017},
  keywords = {LSTM,Activation Function,Linear Fitting,Recurrent Neural Network,Unsaturated Region,循环神经网络,激活函数,线性拟合,长短时记忆单元,非饱和区},
  file = {/Users/sean10/zotero/storage/AF7FR4EI/张尧_2017_激活函数导向的RNN算法优化.pdf}
}

@article{lisongruCaiYongXunHuanShenJingWangLuoDeQingGanFenXiZhuYiLiMoXing2018,
  title = {{采用循环神经网络的情感分析注意力模型}},
  issn = {1000-5013},
  lccn = {35-1079/N},
  abstract = {针对目前情感分析中的循环神经网络模型缺乏对情感词的关注的问题,提出一种基于循环神经网络的情感词注意力模型,通过引入注意力机制,在情感分类时着重考虑文本中的情感词的影响.在NLPCC 2014情感分析数据集及IMDB影评数据集上进行试验,结果表明:该模型能够提高情感分析的效果.},
  language = {Chinese},
  number = {02},
  journal = {华侨大学学报(自然科学版)},
  author = {李松如 and 陈锻生},
  year = {2018},
  keywords = {情感分析,sentiment analysis,循环神经网络,attention,long short term memory,recurrent neural network,注意力,长短时记忆},
  pages = {252-255},
  file = {/Users/sean10/zotero/storage/NS2KQCZQ/李松如_陈锻生_2018_采用循环神经网络的情感分析注意力模型.pdf}
}

@phdthesis{malongRenJiDuiHuaLiJieZhongLianHeXueXiJiZhuDeYanJiuYuYingYong2017,
  type = {{硕士}},
  title = {{人机对话理解中联合学习技术的研究与应用}},
  abstract = {随着计算技术与人工智能技术的发展,人机对话系统得到了越来越广泛的研究,尤其是面向特定任务的人机对话系统,可以用于机票预订等客户服务中,帮助企业有效地降低运营成本,具有重要的应用价值。人机对话理解是人机对话系统的重要组成部分,其包含三个关键任务:领域识别,槽填充和意图识别。经典的人机对话理解技术将三个子任务分别建模,串行执行。这种管道方法存在误差累积等缺陷。近年来开始出现了一些将其中若干个子任务联合建模的方法,可以有效缓解误差累积等问题。本文在前人工作的基础上,对联合建模技术开展研究,主要工作内容包括:提出了 一种基于双向长短期记忆模型(bi-LSTM: bi-direction Long Short Term Memory)的联合建模槽填充与意图识别子任务的方法,对意图标志位、历史信息、语料规模等多种因素对模型性能的影响进行了分析,并与基于条件随机场(CRF: Conditional Random Fields)的联合模型进行了比较,实验表明了该方法的有效性。综合运用上述模型实现了一个面向会议室预订任务的人机对话系统,系统可以和用户进行会议室预订的人机交互,具有较好的交互能力。同时,基于该对话系统获取了一定规模的汉语人机对话数据,并对该数据集进行了意图与槽信息的标注。},
  language = {中文;},
  school = {北京邮电大学},
  author = {马龙},
  year = {2017},
  keywords = {Bi-LSTM,joint learning model,multi-task,natural language understanding,spoken dialogue understanding,对话理解,意图识别,槽填充,联合建模}
}

@article{choLearningPhraseRepresentations2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.1078},
  primaryClass = {cs, stat},
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}-{{Decoder}} for {{Statistical Machine Translation}}},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  journal = {arXiv:1406.1078 [cs, stat]},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  month = jun,
  year = {2014},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/sean10/zotero/storage/YSQ86BSB/Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf;/Users/sean10/zotero/storage/7CD7U87H/1406.html}
}

@article{chungEmpiricalEvaluationGated2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.3555},
  primaryClass = {cs},
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  journal = {arXiv:1412.3555 [cs]},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/sean10/zotero/storage/M6WF47KF/Chung et al_2014_Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf;/Users/sean10/zotero/storage/CJ9AIZ9J/1412.html}
}

@article{liIndependentlyRecurrentNeural2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.04831},
  primaryClass = {cs},
  title = {Independently {{Recurrent Neural Network}} ({{IndRNN}}): {{Building A Longer}} and {{Deeper RNN}}},
  shorttitle = {Independently {{Recurrent Neural Network}} ({{IndRNN}})},
  abstract = {Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM. The code is available at https://github.com/Sunnydreamrain/IndRNN\_Theano\_Lasagne.},
  journal = {arXiv:1803.04831 [cs]},
  author = {Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/sean10/zotero/storage/68LN4TDP/Li et al_2018_Independently Recurrent Neural Network (IndRNN).pdf;/Users/sean10/zotero/storage/A4MFXZZZ/1803.html}
}

@inproceedings{maasLearningWordVectors2011,
  address = {Stroudsburg, PA, USA},
  series = {HLT '11},
  title = {Learning {{Word Vectors}} for {{Sentiment Analysis}}},
  isbn = {978-1-932432-87-9},
  abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} - {{Volume}} 1},
  publisher = {{Association for Computational Linguistics}},
  author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
  year = {2011},
  pages = {142--150},
  file = {/Users/sean10/zotero/storage/HMULPSCM/Maas et al_2011_Learning Word Vectors for Sentiment Analysis.pdf}
}

@inproceedings{pangThumbsSentimentClassification2002,
  address = {Stroudsburg, PA, USA},
  series = {EMNLP '02},
  title = {Thumbs {{Up}}?: {{Sentiment Classification Using Machine Learning Techniques}}},
  shorttitle = {Thumbs {{Up}}?},
  doi = {10.3115/1118693.1118704},
  abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
  booktitle = {Proceedings of the {{ACL}}-02 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} - {{Volume}} 10},
  publisher = {{Association for Computational Linguistics}},
  author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
  year = {2002},
  pages = {79--86},
  file = {/Users/sean10/zotero/storage/NSK5UTKF/Pang et al_2002_Thumbs Up.pdf}
}

@inproceedings{liuOpinionObserverAnalyzing2005,
  address = {New York, NY, USA},
  series = {WWW '05},
  title = {Opinion {{Observer}}: {{Analyzing}} and {{Comparing Opinions}} on the {{Web}}},
  isbn = {978-1-59593-046-0},
  shorttitle = {Opinion {{Observer}}},
  doi = {10.1145/1060745.1060797},
  abstract = {The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly.},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{World Wide Web}}},
  publisher = {{ACM}},
  author = {Liu, Bing and Hu, Minqing and Cheng, Junsheng},
  year = {2005},
  keywords = {sentiment analysis,information extraction,opinion analysis,visualization},
  pages = {342--351},
  file = {/Users/sean10/zotero/storage/LEMG95WF/Liu et al_2005_Opinion Observer.pdf}
}

@inproceedings{yiSentimentAnalyzerExtracting2003,
  title = {Sentiment Analyzer: Extracting Sentiments about a given Topic Using Natural Language Processing Techniques},
  shorttitle = {Sentiment Analyzer},
  doi = {10.1109/ICDM.2003.1250949},
  abstract = {We present sentiment analyzer (SA) that extracts sentiment (or opinion) about a subject from online text documents. Instead of classifying the sentiment of an entire document about a subject, SA detects all references to the given subject, and determines sentiment in each of the references using natural language processing (NLP) techniques. Our sentiment analysis consists of 1) a topic specific feature term extraction, 2) sentiment extraction, and 3) (subject, sentiment) association by relationship analysis. SA utilizes two linguistic resources for the analysis: the sentiment lexicon and the sentiment pattern database. The performance of the algorithms was verified on online product review articles ("digital camera" and "music" reviews), and more general documents including general Webpages and news articles.},
  booktitle = {Third {{IEEE International Conference}} on {{Data Mining}}},
  author = {Yi, J. and Nasukawa, T. and Bunescu, R. and Niblack, W.},
  month = nov,
  year = {2003},
  keywords = {sentiment lexicon,computational linguistics,Computer science,Data mining,feature extraction,Feature extraction,feature term extraction,Internet,Marketing management,natural language processing,Natural language processing,natural languages,online text documents,Pattern analysis,Product development,sentiment analyzer,sentiment extraction,sentiment pattern database,Spatial databases,text analysis,Text analysis,Web pages},
  pages = {427-434},
  file = {/Users/sean10/zotero/storage/ILZN3JUR/1250949.html}
}

@incollection{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  publisher = {{Curran Associates, Inc.}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  year = {2013},
  pages = {3111--3119},
  file = {/Users/sean10/zotero/storage/7WDSK36N/Mikolov et al_2013_Distributed Representations of Words and Phrases and their Compositionality.pdf;/Users/sean10/zotero/storage/L2ISFAYC/5021-distributed-representations-of-words-andphrases.html}
}

@inproceedings{irsoyOpinionMiningDeep2014,
  address = {Doha, Qatar},
  title = {Opinion {{Mining}} with {{Deep Recurrent Neural Networks}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  publisher = {{Association for Computational Linguistics}},
  author = {Irsoy, Ozan and Cardie, Claire},
  month = oct,
  year = {2014},
  pages = {720--728},
  file = {/Users/sean10/zotero/storage/CS3CRWDT/Irsoy_Cardie_2014_Opinion Mining with Deep Recurrent Neural Networks.pdf}
}

@article{rushNeuralAttentionModel2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.00685},
  primaryClass = {cs},
  title = {A {{Neural Attention Model}} for {{Abstractive Sentence Summarization}}},
  abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
  journal = {arXiv:1509.00685 [cs]},
  author = {Rush, Alexander M. and Chopra, Sumit and Weston, Jason},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence},
  file = {/Users/sean10/zotero/storage/JRC8SB5Q/Rush et al_2015_A Neural Attention Model for Abstractive Sentence Summarization.pdf;/Users/sean10/zotero/storage/6B57BZ6F/1509.html}
}

@article{bengioNeuralProbabilisticLanguage2003,
  title = {A {{Neural Probabilistic Language Model}}},
  volume = {3},
  issn = {ISSN 1533-7928},
  number = {Feb},
  journal = {Journal of Machine Learning Research},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year = {2003},
  pages = {1137-1155},
  file = {/Users/sean10/zotero/storage/XGH9IW62/Bengio et al_2003_A Neural Probabilistic Language Model.pdf;/Users/sean10/zotero/storage/82F7ZIAR/bengio03a.html}
}

@article{mikolovRecurrentNeuralNetwork,
  title = {Recurrent {{Neural Network Based Language Model}}},
  abstract = {A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50\% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18\% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5\% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity.},
  author = {Mikolov, Tomas and Karafiat, Martin and Burget, Lukas and Cernocky, Jan and Khudanpur, Sanjeev},
  pages = {4},
  file = {/Users/sean10/zotero/storage/6MU3YWMX/Mikolov et al. - Recurrent Neural Network Based Language Model.pdf}
}

@incollection{mikolovDistributedRepresentationsWords2013a,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  publisher = {{Curran Associates, Inc.}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  year = {2013},
  pages = {3111--3119},
  file = {/Users/sean10/zotero/storage/RYLIJ7H7/5021-distributed-representations-of-words-andphrases.html}
}

@article{leDistributedRepresentationsSentences,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, ``powerful,'' ``strong'' and ``Paris'' are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  language = {en},
  author = {Le, Quoc and Mikolov, Tomas},
  pages = {9},
  file = {/Users/sean10/zotero/storage/6V6PEZKS/Le and Mikolov - Distributed Representations of Sentences and Docum.pdf}
}

@article{mikolovEfficientEstimationWord2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3781},
  primaryClass = {cs},
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  journal = {arXiv:1301.3781 [cs]},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/sean10/zotero/storage/TZZKNIH8/Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf;/Users/sean10/zotero/storage/5JCRYDFE/1301.html}
}

@article{yangligongWenBenQingGanFenXiZongShu2013,
  title = {{文本情感分析综述}},
  issn = {1001-9081},
  lccn = {51-1307/TP},
  abstract = {以文本颗粒度为视角,从情感词抽取、语料库和情感词典构建、评价对象与意见持有者分析、篇章级情感分析、实际应用五个方面对文本情感分析文献进行了梳理,并做出必要评述。指出当前情感分析系统的准确率普遍不高,进一步研究的重点在于:自然语言处理的研究成果在文本情感倾向分析中更广泛和贴切的应用;选取文本情感倾向分类的特征和方法;利用现有语言工具和相关资源,规范、快速地构造语言工具和相关资源并应用。},
  language = {中文;},
  number = {06},
  journal = {计算机应用},
  author = {杨立公 and 朱俭 and 汤世平},
  year = {2013},
  keywords = {文本情感分析,情感词典,corpus,opinion holder,sentiment word,sentiment word dictionary,text sentiment analysis,情感词,意见持有者,语料库},
  pages = {1574-1578+1607},
  file = {/Users/sean10/zotero/storage/UPWD3553/杨立公 et al_2013_文本情感分析综述.pdf}
}

@article{kimConvolutionalNeuralNetworks2014a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1408.5882},
  primaryClass = {cs},
  title = {Convolutional {{Neural Networks}} for {{Sentence Classification}}},
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  journal = {arXiv:1408.5882 [cs]},
  author = {Kim, Yoon},
  month = aug,
  year = {2014},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/sean10/zotero/storage/4GZ3FEHJ/Kim_2014_Convolutional Neural Networks for Sentence Classification.pdf;/Users/sean10/zotero/storage/F8EASKKE/1408.html}
}

@inproceedings{socherRecursiveDeepModels2013,
  address = {Seattle, Washington, USA},
  title = {Recursive {{Deep Models}} for {{Semantic Compositionality Over}} a {{Sentiment Treebank}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
  month = oct,
  year = {2013},
  pages = {1631--1642},
  file = {/Users/sean10/zotero/storage/HY467NZJ/Socher et al_2013_Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.pdf}
}

@phdthesis{xulinhongJiYuYuYiZiYuanDeWenBenQingGanJiSuan2007,
  address = {大连},
  type = {{硕士}},
  title = {{基于语义资源的文本情感计算}},
  abstract = {情感计算是人工智能的一个热门研究领域，它的目标是使计算机拥有情感，能够像人一样自然亲切的交流。随着Internet的发展，以文本形式出现的信息越来越多，逐渐成为我们最容易获取也是最为丰富的一种交互资源，然而在国内文本情感分析方面的研究较少。所以本文首先构建情感识别所需的语义资源，情感词汇本体和情感语料库，在此基础上采用了基于语义特征和基于认知-评价理论的两种方法识别文本中每句的情感类别。
在语义资源的建设中，首先根据目前情感分类发展的现状，确定情感分类体系。然后通过手工分类和自动获取相结合的方法填充词汇本体的描述框架。情感语料库构建中讨论了制定标注规范、选择标注集、设计标注工具以及标注过程中的质量监控等问题。目前已经标注完成近四万句，一百万字的语料。在完成这些已标注语料的基础上，进一步给出了语料库的情感分布，情感迁移规律等统计数据，分析了情感语料库的特点及应用。
基于语义特征的情感识别方法是将句子中词汇的情感信息和语义特征加入条件随机域(CRFs)中，生成文章的情感链。而基于认知-评价理论的情感识别是以拉扎勒斯的认知-评价理论和认知语用学中的认知语境为理论背景，从情感的发生机制出发，以多种情感图式为基础的一个文本情感认知模型。最后本文采用单句评估法和多句联合评估法两种不同的方法，分别评测两种识别方法的正确率和情感的连贯性。
实验证明基于语义特征的方法在情感的连贯性上效果较好，而基于认知的方法总体准确率较高。两种方法分别从不同的角度对文本情感识别做了初步探索，具有进一步研究的价值。},
  language = {Chinese},
  school = {大连理工大学},
  author = {徐琳宏},
  year = {2007},
  keywords = {Affecting Computing,Cognitive Context,Condition Random Fields,Ontology,情感计算,本体,条件随机域,认知语境}
}

@article{socherSemanticCompositionalityRecursive,
  title = {Semantic {{Compositionality}} through {{Recursive Matrix}}-{{Vector Spaces}}},
  abstract = {Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.},
  author = {Socher, Richard and Huval, Brody and Manning, Christopher D and Ng, Andrew Y},
  pages = {11},
  file = {/Users/sean10/zotero/storage/D2GY6EQ5/Socher et al. - Semantic Compositionality through Recursive Matrix.pdf}
}

@phdthesis{zhushaojieJiYuShenDuXueXiDeWenBenQingGanFenLeiYanJiu2014,
  address = {哈尔滨},
  type = {{硕士}},
  title = {{基于深度学习的文本情感分类研究}},
  abstract = {文本情感分类在情感分析研究中占有举足轻重的地位，在信息爆炸的21世纪，海量数据的情感分类研究吸引了很多的研究者，如何深入学习文本的语义信息，准确表达语义特征，提高情感分类的准确性是研究的目标。 	鉴于传统的机器学习方法无法学习文本语义信息的缺陷，本文在浅层学习特征的基础上提出了融合深度学习特征的方案研究文本情感分类问题，提高特征对文本语义信息的表达，增加模型对语义的学习和理解能力；研究中发现融合中采用的深层特征由于不是基于多隐层的深度学习方法抽取的，导致学习到的深层特征向量不能真正的理解文本中的具体语义，针对这些问题，文章引入了基于深度学习的半监督RAE方法研究文本的情感分类问题，RAE方法是多隐层的神经网络结构，可以逐层分析，优化每一层学习得到的特征向量表示，因此它抽取的文本特征向量可以更准确的表达语义信息，提高分类结果。 	本文首先采用传统的SVM方法进行情感分类问题的研究，实验中选择词、词性和词典的特征组合方式，得到最好的分类正确率结果是81.88\%；融合深度学习特征的方案中首先通过实验得到最佳深度特征向量长度值为150，在此条件下得到的最优分类正确率是81.98\%，比传统的SVM方法提高了0.1\%；基于深度学习的半监督RAE方法中经过实验得到深度特征向量的最佳长度值是50，其结果得到了显著的提高，正确率为85.10\%，比传统的SVM方法提高了3.2\%，将样本容量增加到原来的2倍时，情感分类的正确率可提高2.5\%，同时模型的学习时间增长到原来的3倍。},
  language = {Chinese},
  school = {哈尔滨工业大学},
  author = {朱少杰},
  year = {2014},
  keywords = {情感分类,深度学习,deep learning,feature fusion,semi-,sentiment classification,supervised RAE method,半监督RAE方法,特征融合}
}

@article{liuyanmeiShenDuXueXiJiZhuXiaDeZhongWenWeiBoQingGanDeFenXiYuYanJiu2016,
  title = {{深度学习技术下的中文微博情感的分析与研究}},
  issn = {1003-6970},
  lccn = {12-1151/TP},
  abstract = {微博情感分析主要在于发现用户对某种热点事件的观点和态度,已有的研究,诸如SVM、CRF等传统算法,付出了昂贵的手工标注的代价。本文在研究情感分析、深度学习等技术的基础上,提出了一种新的技术方案:即通过网络爬虫技术从微博上抓取部分数据,经过词料预处理后,作为卷积神经网络的输入样本,并基于SVM/RNN构建分类器。最后在给定的测试集中判断每个句子的情感倾向性,实验结果良好。},
  language = {中文;},
  number = {05},
  journal = {软件},
  author = {刘艳梅},
  year = {2016},
  keywords = {深度学习,Deep learning,卷积神经网络,Classifier,CNN(Convolutional Neural Network),Micro-blog sentiment analysis,分类器,微博情感分析},
  pages = {22-24},
  file = {/Users/sean10/zotero/storage/ELWZIB9U/刘艳梅_2016_深度学习技术下的中文微博情感的分析与研究.pdf}
}

@article{liangjunJiYuJiXingZhuanYiHeLSTMDiGuiWangLuoDeQingGanFenXi2015,
  title = {{基于极性转移和LSTM递归网络的情感分析}},
  issn = {1003-0077},
  lccn = {11-2325/N},
  abstract = {长短时记忆(long short term memory,LSTM)是一种有效的链式循环神经网络(recurrent neural network,R2 NN1),被广泛用于语言模型、机器翻译、语音识别等领域。但由于该网络结构是一种链式结构,不能有效表征语言的结构层次信息,该文将LSTM扩展到基于树结构的递归神经网络(Recursive Neural Network,RNN)上,用于捕获文本更深层次的语义语法信息,并根据句子前后词语间的关联性引入情感极性转移模型。实验证明本文提出的模型优于LSTM、递归神经网络等。},
  language = {中文;},
  number = {05},
  journal = {中文信息学报},
  author = {梁军 and 柴玉梅 and 原慧斌 and 高明磊 and 昝红英},
  year = {2015},
  keywords = {情感分析,sentiment analysis,LSTM,recursive neural network,递归神经网络},
  pages = {152-159},
  file = {/Users/sean10/zotero/storage/TSBUCT8T/梁军 et al_2015_基于极性转移和LSTM递归网络的情感分析.pdf}
}

@phdthesis{lisongruJiYuXunHuanShenJingWangLuoDeWangLuoYuQingWenBenQingGanFenXiJiZhuYanJiu2017,
  address = {厦门},
  type = {{硕士}},
  title = {{基于循环神经网络的网络舆情文本情感分析技术研究}},
  abstract = {文本情感分析作为自然语言处理领域的研究热点之一,近年来受到越来越多的关注,其目的在于自动地从文本中提取和归纳主观情感信息。现有的文本情感分析方法主要可以分为基于情感词典的方法和基于机器学习的方法。基于情感词典的方法严重依赖于情感词典的质量,而构建一个覆盖范围广且精确的情感词典是非常困难的。基于机器学习的文本情感分析方法依赖于构建和抽取的特征,但传统的特征表示方法都不能很好的保留文本的语义信息。随着深度学习技术在计算机视觉领域的广泛应用,深度学习模型被证明在特征提取方面有着很大优势,其中循环神经网络由于其特殊的循环链式结构,非常适合处理文本等序列数据,已被广泛应用到自然语言处理领域。本文主要研究基于循环神经网络的文本情感分析方法。本文的工作内容可以分为如下两块:1)循环神经网络在传统的情感分析任务上的应用。考虑到文本的情感极性很大程度上取决于句子中带有情感倾向性的词语,对文本中的情感词加以关注将有助于提高情感分类的效果。针对目前情感分析中的循环神经网络模型缺乏对情感词的关注,本文提出了一种基于循环神经网络的情感分析注意力模型(RNN-Attention),通过引入注意力机制,在情感分类时着重考虑文本中的情感词的影响。在NLPCC 2014情感分析数据集以及IMDB影评数据集上进行的实验结果表明,该模型能够提高情感分析的效果。2)针对目标依赖情感分析任务的循环神经网络模型。目标依赖情感分析针对给定的一个目标对象,判断句子关于该对象的情感极性。目前,大多数的情感分析方法都是针对传统的情感分析任务,即对给定的一段文本直接分析它所表达的情感倾向。使用传统情感分析的方法进行目标依赖的情感分析时,由于没有考虑目标对象的信息,会导致作出错误的判断。为了解决这个问题,本文在RNN-Attention模型的基础上进行改进,提出RNN-Attention-T模型,该模型在对文本进行建模的同时引入目标对象的信息。此外,考虑到目标对象的上文和下文对句子情感倾向的影响程度往往是不同的,本文接着提出了一种对目标对象的上文和下文分别建模的RNN-Attention-C模型。实验结果表明,与现有的目标依赖情感分析方法相比,本文所提出的改进模型能够在不使用句法解析器和外部情感词典的情况下获得较好的分类效果。},
  language = {Chinese},
  school = {华侨大学},
  author = {李松如},
  year = {2017},
  keywords = {文本情感分析,循环神经网络,词向量,Attention model,Recurrent neural network,Target-dependent,Text sentiment analysis,Word embedding,注意力模型,目标依赖}
}

@inproceedings{tangLearningSentimentSpecificWord2014,
  address = {Baltimore, Maryland},
  title = {Learning {{Sentiment}}-{{Specific Word Embedding}} for {{Twitter Sentiment Classification}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  author = {Tang, Duyu and Wei, Furu and Yang, Nan and Zhou, Ming and Liu, Ting and Qin, Bing},
  month = jun,
  year = {2014},
  pages = {1555--1565},
  file = {/Users/sean10/zotero/storage/7PFRNDIJ/Tang et al_2014_Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification.pdf}
}

@inproceedings{tangDocumentModelingGated2015,
  address = {Lisbon, Portugal},
  title = {Document {{Modeling}} with {{Gated Recurrent Neural Network}} for {{Sentiment Classification}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Tang, Duyu and Qin, Bing and Liu, Ting},
  month = sep,
  year = {2015},
  pages = {1422--1432},
  file = {/Users/sean10/zotero/storage/FVPT7XGK/Tang et al_2015_Document Modeling with Gated Recurrent Neural Network for Sentiment.pdf}
}

@inproceedings{narayananFastAccurateSentiment2013,
  series = {Lecture Notes in Computer Science},
  title = {Fast and {{Accurate Sentiment Classification Using}} an {{Enhanced Naive Bayes Model}}},
  isbn = {978-3-642-41277-6 978-3-642-41278-3},
  doi = {10.1007/978-3-642-41278-3_24},
  abstract = {We have explored different methods of improving the accuracy of a Naive Bayes classifier for sentiment analysis. We observed that a combination of methods like effective negation handling, word n-grams and feature selection by mutual information results in a significant improvement in accuracy. This implies that a highly accurate and fast sentiment classifier can be built using a simple Naive Bayes model that has linear training and testing time complexities. We achieved an accuracy of 88.80\% on the popular IMDB movie reviews dataset. The proposed method can be generalized to a number of text categorization problems for improving speed and accuracy.},
  booktitle = {Intelligent {{Data Engineering}} and {{Automated Learning}} \textendash{} {{IDEAL}} 2013},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Narayanan, Vivek and Arora, Ishan and Bhatia, Arjun},
  month = oct,
  year = {2013},
  pages = {194-201},
  file = {/Users/sean10/zotero/storage/2ISWQUQP/Narayanan et al_2013_Fast and Accurate Sentiment Classification Using an Enhanced Naive Bayes Model.pdf;/Users/sean10/zotero/storage/I93IYP54/978-3-642-41278-3_24.html}
}

@inproceedings{dongAdaptiveRecursiveNeural2014,
  address = {Baltimore, Maryland},
  title = {Adaptive {{Recursive Neural Network}} for {{Target}}-Dependent {{Twitter Sentiment Classification}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  publisher = {{Association for Computational Linguistics}},
  author = {Dong, Li and Wei, Furu and Tan, Chuanqi and Tang, Duyu and Zhou, Ming and Xu, Ke},
  month = jun,
  year = {2014},
  pages = {49--54},
  file = {/Users/sean10/zotero/storage/NKMVWCF6/Dong et al_2014_Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment.pdf}
}

@inproceedings{dossantosDeepConvolutionalNeural2014,
  address = {Dublin, Ireland},
  title = {Deep {{Convolutional Neural Networks}} for {{Sentiment Analysis}} of {{Short Texts}}},
  booktitle = {Proceedings of {{COLING}} 2014, the 25th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  publisher = {{Dublin City University and Association for Computational Linguistics}},
  author = {{dos Santos}, Cicero and Gatti, Maira},
  month = aug,
  year = {2014},
  pages = {69--78},
  file = {/Users/sean10/zotero/storage/KPHTS6WV/dos Santos_Gatti_2014_Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts.pdf}
}

@article{taiImprovedSemanticRepresentations2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.00075},
  primaryClass = {cs},
  title = {Improved {{Semantic Representations From Tree}}-{{Structured Long Short}}-{{Term Memory Networks}}},
  abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
  journal = {arXiv:1503.00075 [cs]},
  author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Artificial Intelligence},
  file = {/Users/sean10/zotero/storage/JKLBGY3J/Tai et al_2015_Improved Semantic Representations From Tree-Structured Long Short-Term Memory.pdf;/Users/sean10/zotero/storage/SAIWQSEN/1503.html}
}

@incollection{irsoyDeepRecursiveNeural2014,
  title = {Deep {{Recursive Neural Networks}} for {{Compositionality}} in {{Language}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  author = {Irsoy, Ozan and Cardie, Claire},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {2096--2104},
  file = {/Users/sean10/zotero/storage/C45NU6II/Irsoy_Cardie_2014_Deep Recursive Neural Networks for Compositionality in Language.pdf;/Users/sean10/zotero/storage/ATE62XE8/5551-deep-recursive-neural-networks-for-compositionality-in-language.html}
}

@article{leeSequentialShortTextClassification2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.03827},
  primaryClass = {cs, stat},
  title = {Sequential {{Short}}-{{Text Classification}} with {{Recurrent}} and {{Convolutional Neural Networks}}},
  abstract = {Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.},
  journal = {arXiv:1603.03827 [cs, stat]},
  author = {Lee, Ji Young and Dernoncourt, Franck},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence},
  file = {/Users/sean10/zotero/storage/QD3F52TW/Lee_Dernoncourt_2016_Sequential Short-Text Classification with Recurrent and Convolutional Neural.pdf;/Users/sean10/zotero/storage/FW8EBHXN/1603.html}
}

@article{chengluJiYuZhuYiLiJiZhiDeShuangXiangLSTMMoXingZaiZhongWenShangPinPingLunQingGanFenLeiZhongDeYanJiu2017,
  title = {{基于注意力机制的双向LSTM模型在中文商品评论情感分类中的研究}},
  issn = {2096-1472},
  lccn = {21-1603/TP},
  abstract = {国内电商网站的快速发展促使产生大量的中文商品评论信息。对这些评论进行情感分类有利于获取其中的有用信息,具有重要的应用意义。目前,情感分类的研究主要基于情感词典或者传统机器学习。这些方法通常需要人工选取特征,费事费力,分类效果不好。针对这些不足,本文提出一种基于注意力机制的双向LSTM模型,对中文商品评论进行情感分类。实验结果表明,该模型在中文商品评论二分类任务和三分类任务中均获得了较好的准确率、召回率、F1值。},
  language = {中文;},
  number = {11},
  journal = {软件工程},
  author = {成璐},
  year = {2017},
  keywords = {情感分类,sentiment classification,attention mechanism,bidirectional LSTM,Chinese product reviews,中文商品评论,双向LSTM,注意力机制},
  pages = {4-6+3},
  file = {/Users/sean10/zotero/storage/DTUX3E2U/成璐_2017_基于注意力机制的双向LSTM模型在中文商品评论情感分类中的研究.pdf}
}

@phdthesis{alotaibiSentimentAnalysisArabic2015,
  address = {United States -- Colorado},
  type = {Ph.{{D}}.},
  title = {Sentiment Analysis in the {{Arabic}} Language Using Machine Learning},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  abstract = {Sentiment analysis has recently become one of the growing areas of research related to natural language processing and machine learning. Much opinion and sentiment about specific topics are available online, which allows several parties such as customers, companies and even governments, to explore these opinions. The first task is to classify the text in terms of whether or not it expresses opinion or factual information. Polarity classification is the second task, which distinguishes between polarities (positive, negative or neutral) that sentences may carry. The analysis of natural language text for the identification of subjectivity and sentiment has been well studied in terms of the English language. Conversely, the work that has been carried out in terms of Arabic remains in its infancy; thus, more cooperation is required between research communities in order for them to offer a mature sentiment analysis system for Arabic. There are recognized challenges in this field; some of which are inherited from the nature of the Arabic language itself, while others are derived from the scarcity of tools and sources.
This dissertation provides the rationale behind the current work and proposed methods to enhance the performance of sentiment analysis in the Arabic language. The first step is to increase the resources that help in the analysis process; the most important part of this task is to have annotated sentiment corpora. Several free corpora are available for the English language, but these resources are still limited in other languages, such as Arabic. This dissertation describes the work undertaken by the author to enrich sentiment analysis in Arabic by building a new Arabic Sentiment Corpus. The data is labeled not only with two polarities (positive and negative), but the neutral sentiment is also used during the annotation process.
The second step includes the proposal of features that may capture sentiment orientation in the Arabic language, as well as using different machine learning classifiers that may be able to work better and capture the non-linearity with a richly morphological and highly inflectional language, such as Arabic. Different types of features are proposed. These proposed features try to capture different aspects and characteristics of Arabic. Morphological, Semantic, Stylistic features are proposed and investigated. In regard with the classifier, the performance of using linear and nonlinear machine learning approaches was compared. The results are promising for the continued use of nonlinear ML classifiers for this task. Learning knowledge from a particular dataset domain and applying it to a different domain is one useful method in the case of limited resources, such as with the Arabic language. This dissertation shows and discussed the possibility of applying cross-domain in the field of Arabic sentiment analysis. It also indicates the feasibility of using different mechanisms of the cross-domain method.
Other work in this dissertation includes the exploration of the effect of negation in Arabic subjectivity and polarity classification. The negation word lists were devised to help in this and other natural language processing tasks. These words include both types of Arabic, Modern Standard and some of Dialects. Two methods of dealing with the negation in sentiment analysis in Arabic were proposed. The first method is based on a static approach that assumes that each sentence containing negation words is considered a negated sentence. When determining the effect of negation, different techniques were proposed, using different word window sizes, or using base phrase chunk. The second approach depends on a dynamic method that needs an annotated negation dataset in order to build a model that can determine whether or not the sentence is negated by the negation words and to establish the effect of the negation on the sentence. The results achieved by adding negation to Arabic sentiment analysis were promising and indicate that the negation has an effect on this task. Finally, the experiments and evaluations that were conducted in this dissertation encourage the researchers to continue in this direction of research.},
  language = {English},
  school = {Colorado State University},
  author = {Alotaibi, Saud Saleh},
  year = {2015},
  keywords = {Sentiment analysis,Applied sciences,Arabic sentiment,Machine learning,Polarity classificaion,Subjectivity classification},
  file = {/Users/sean10/zotero/storage/XSDH7985/Alotaibi_2015_Sentiment analysis in the Arabic language using machine learning.pdf}
}

@article{pangOpinionMiningSentiment2008,
  title = {Opinion {{Mining}} and {{Sentiment Analysis}}},
  volume = {2},
  issn = {1554-0669, 1554-0677},
  doi = {10.1561/1500000011},
  abstract = {Opinion Mining and Sentiment Analysis},
  language = {English},
  number = {1\textendash{}2},
  journal = {Foundations and Trends\textregistered{} in Information Retrieval},
  author = {Pang, Bo and Lee, Lillian},
  month = jul,
  year = {2008},
  pages = {1-135},
  file = {/Users/sean10/zotero/storage/LDGRPD5D/Pang_Lee_2008_Opinion Mining and Sentiment Analysis.pdf;/Users/sean10/zotero/storage/Z7SE85J7/INR-011.html}
}

@article{guoBackPropagationTime,
  title = {{{BackPropagation Through Time}}},
  abstract = {This report provides detailed description and necessary derivations for the BackPropagation Through Time (BPTT) algorithm. BPTT is often used to learn recurrent neural networks (RNN). Contrary to feed-forward neural networks, the RNN is characterized by the ability of encoding longer past information, thus very suitable for sequential models. The BPTT extends the ordinary BP algorithm to suit the recurrent neural architecture.},
  language = {en},
  author = {Guo, Jiang},
  pages = {6},
  file = {/Users/sean10/zotero/storage/D78P9KMQ/Guo - BackPropagation Through Time.pdf}
}

@phdthesis{zhangchongJiYuAttentionBasedLSTMMoXingDeWenBenFenLeiJiZhuDeYanJiu2016,
  address = {南京},
  type = {{硕士}},
  title = {{基于Attention-Based LSTM模型的文本分类技术的研究}},
  abstract = {文本分类是自然语言处理领域的一个经典的研究方向,传统的研究涉及到文本的预处理、文本特征的提取、机器学习分类器训练等方面。随着深度学习技术在图像识别,机器翻译等领域取得了很大的进展,深度学习模型被证明在数据预处理和特征提取方面有着很大优势。本文在研究分析和总结文本向量表示技术和深度学习模型LSTM原理的基础上,对运用深度学习模型解决文本分类问题做了深入的研究。本文的主要研究工作如下：(1)针对文本分类中数据表示的高维度难以训练和向量表示特征无关的问题,采用了Word Embedding机制,将文本数据映射到一个低维度的实数向量,避免了高维度的输入导致LSTM模型产生维度灾难的问题。同时Word Embedding机制训练出的词向量具有同义词向量相似的特征,作为LSTM模型的输入,提高了分类器的性能。(2)针对文本分类的特征选择问题,本文设计了Attention-Based LSTM模型用于提取特征,其中LSTM模型解决了传统RNN的梯度消失的问题,通过3种``门''的控制,解决了RNN模型训练中的长期依赖问题。同时本文通过Attention-Based的方法,得到含有输入序列节点注意力概率分布的语义编码,并将其作为分类器的输入,减少了特征向量提取过程中的信息丢失和信息冗余。(3)针对LSTM模型的前向依赖问题,本文设计了组合正逆序Attention-Based LSTM模型,组合正逆序向量作为特征向量,将Bi-LSTM模型作为对比模型,探究文本上下文对文本分类的影响。},
  language = {Chinese},
  school = {南京大学},
  author = {张冲},
  year = {2016},
  keywords = {深度学习,文本分类,Deep learning,Feature extraction,Attention probability distribution,LSTM model,LSTM模型,Text classification,注意力概率分布,特征提取},
  file = {/Users/sean10/zotero/storage/64X2MPDE/张冲_2016_基于Attention-Based LSTM模型的文本分类技术的研究.pdf}
}

@inproceedings{wangAttentionbasedLSTMAspectlevel2016,
  address = {Austin, Texas},
  title = {Attention-Based {{LSTM}} for {{Aspect}}-Level {{Sentiment Classification}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Wang, Yequan and Huang, Minlie and {zhu}, xiaoyan and Zhao, Li},
  month = nov,
  year = {2016},
  pages = {606--615},
  file = {/Users/sean10/zotero/storage/H6M9XMD2/Wang et al_2016_Attention-based LSTM for Aspect-level Sentiment Classification.pdf}
}

@article{goldbergPrimerNeuralNetwork2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.00726},
  primaryClass = {cs},
  title = {A {{Primer}} on {{Neural Network Models}} for {{Natural Language Processing}}},
  abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
  journal = {arXiv:1510.00726 [cs]},
  author = {Goldberg, Yoav},
  month = oct,
  year = {2015},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/sean10/zotero/storage/XBLEMUPR/Goldberg_2015_A Primer on Neural Network Models for Natural Language Processing.pdf;/Users/sean10/zotero/storage/WXECQ6ZB/1510.html}
}

@article{leeSentimentClassificationWord2017a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.09885},
  primaryClass = {cs},
  title = {Sentiment {{Classification}} with {{Word Attention}} Based on {{Weakly Supervised Learning}} with a {{Convolutional Neural Network}}},
  abstract = {In order to maximize the applicability of sentiment analysis results, it is necessary to not only classify the overall sentiment (positive/negative) of a given document but also to identify the main words that contribute to the classification. However, most datasets for sentiment analysis only have the sentiment label for each document or sentence. In other words, there is no information about which words play an important role in sentiment classification. In this paper, we propose a method for identifying key words discriminating positive and negative sentences by using a weakly supervised learning method based on a convolutional neural network (CNN). In our model, each word is represented as a continuous-valued vector and each sentence is represented as a matrix whose rows correspond to the word vector used in the sentence. Then, the CNN model is trained using these sentence matrices as inputs and the sentiment labels as the output. Once the CNN model is trained, we implement the word attention mechanism that identifies high-contributing words to classification results with a class activation map, using the weights from the fully connected layer at the end of the learned CNN model. In order to verify the proposed methodology, we evaluated the classification accuracy and inclusion rate of polarity words using two movie review datasets. Experimental result show that the proposed model can not only correctly classify the sentence polarity but also successfully identify the corresponding words with high polarity scores.},
  journal = {arXiv:1709.09885 [cs]},
  author = {Lee, Gichang and Jeong, Jaeyun and Seo, Seungwan and Kim, CzangYeob and Kang, Pilsung},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/sean10/zotero/storage/BIFRF8SI/Lee et al_2017_Sentiment Classification with Word Attention based on Weakly Supervised.pdf;/Users/sean10/zotero/storage/MNGWIC2I/1709.html}
}

@inproceedings{yangHierarchicalAttentionNetworks2016,
  address = {San Diego, California},
  title = {Hierarchical {{Attention Networks}} for {{Document Classification}}},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Yang, Zichao and Yang, Diyi and {Chris Dyer} and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  month = jun,
  year = {2016},
  pages = {1480--1489},
  file = {/Users/sean10/zotero/storage/TDZJTZ78/Yang et al_2016_Hierarchical Attention Networks for Document Classification.pdf}
}

@article{socherRecursiveDeepModels,
  title = {Recursive {{Deep Models}} for {{Semantic Compositionality Over}} a {{Sentiment Treebank}}},
  abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80\% up to 85.4\%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7\%, an improvement of 9.7\% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
  language = {en},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jean Y and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  pages = {12},
  file = {/Users/sean10/zotero/storage/PD6U8IFS/Socher et al. - Recursive Deep Models for Semantic Compositionalit.pdf}
}

@article{shangNeuralRespondingMachine2015,
  title = {Neural {{Responding Machine}} for {{Short}}-{{Text Conversation}}},
  volume = {1503},
  abstract = {We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75\% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.},
  journal = {ArXiv e-prints},
  author = {Shang, Lifeng and Lu, Zhengdong and Li, Hang},
  month = mar,
  year = {2015},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence},
  pages = {arXiv:1503.02364},
  file = {/Users/sean10/zotero/storage/RJQT6D3D/Shang et al_2015_Neural Responding Machine for Short-Text Conversation.pdf}
}

@article{vaswaniAttentionAllYou2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03762},
  primaryClass = {cs},
  title = {Attention {{Is All You Need}}},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  journal = {arXiv:1706.03762 [cs]},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning},
  file = {/Users/sean10/zotero/storage/AJZ62EGV/Vaswani et al_2017_Attention Is All You Need.pdf;/Users/sean10/zotero/storage/EXABNNUQ/1706.html}
}


